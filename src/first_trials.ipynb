{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "2\n",
      "NVIDIA GeForce RTX 3080\n",
      "NVIDIA GeForce RTX 3060\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.cuda.get_device_name(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total_examples': 2889, 'examples_with_answer_in_context': 721, 'examples_without_answer_in_context': 2168, 'correct_with_context': 539, 'correct_without_context': 87, 'average_correct_with_context': 0.7475728155339806, 'average_correct_without_context': 0.04012915129151291, 'overall_accuracy': 0.2166839736933195}\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "\n",
    "def analyze_json_responses(my_path):\n",
    "    data = read_json(my_path)  # Legge il file JSON\n",
    "\n",
    "    total_examples = len(data)  # Numero totale di esempi\n",
    "    correct_with_context = 0  # Risposte corrette con risposta nel contesto\n",
    "    correct_without_context = 0  # Risposte corrette senza risposta nel contesto\n",
    "    has_answer_in_context = 0  # Esempi con risposta nel contesto\n",
    "    no_answer_in_context = 0  # Esempi senza risposta nel contesto\n",
    "    total_correct = 0  # Totale delle risposte corrette\n",
    "\n",
    "    for entry in data:\n",
    "        ans_in_documents = entry.get(\"ans_in_documents\", False)\n",
    "        ans_match_after_norm = entry.get(\"ans_match_after_norm\", False)\n",
    "\n",
    "        if ans_in_documents:\n",
    "            has_answer_in_context += 1\n",
    "            if ans_match_after_norm:\n",
    "                correct_with_context += 1\n",
    "        else:\n",
    "            no_answer_in_context += 1\n",
    "            if ans_match_after_norm:\n",
    "                correct_without_context += 1\n",
    "\n",
    "        # Conta ogni risposta corretta\n",
    "        if ans_match_after_norm:\n",
    "            total_correct += 1\n",
    "\n",
    "    # Calcola le medie\n",
    "    avg_correct_with_context = correct_with_context / has_answer_in_context if has_answer_in_context > 0 else 0\n",
    "    avg_correct_without_context = correct_without_context / no_answer_in_context if no_answer_in_context > 0 else 0\n",
    "    overall_accuracy = total_correct / total_examples if total_examples > 0 else 0\n",
    "\n",
    "    return {\n",
    "        \"total_examples\": total_examples,\n",
    "        \"examples_with_answer_in_context\": has_answer_in_context,\n",
    "        \"examples_without_answer_in_context\": no_answer_in_context,\n",
    "        \"correct_with_context\": correct_with_context,\n",
    "        \"correct_without_context\": correct_without_context,\n",
    "        \"average_correct_with_context\": avg_correct_with_context,\n",
    "        \"average_correct_without_context\": avg_correct_without_context,\n",
    "        \"overall_accuracy\": overall_accuracy\n",
    "    }\n",
    "\n",
    "\n",
    "# Esempio di utilizzo\n",
    "path = r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\gen_res_example_llm\\nq\\gemma-2-2b-it\\test\\retrieved\\contriever\\1_doc\\numdoc1_retr1_template_info_all_extended.json'\n",
    "result = analyze_json_responses(path)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LLM...\n",
      "LLM loaded\n",
      "Loading corpus and search results...\n",
      "Corpus and search results loaded\n",
      "Loading prompt dataset...\n",
      "Prompt dataset loaded\n",
      "INFO:\n",
      "DATA: C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\10k_train_dataset.json\n",
      "USE TEST: False\n",
      "MODEL: google/flan-t5-large\n",
      "MODEL MAX LENGTH: 4096\n",
      "MAX NEW TOKENS: 50\n",
      "USE MODEL CHAT TEMPLATE: False\n",
      "TASK WITH PROOF: False\n",
      "GOLD POSITION: None\n",
      "NUM DOCUMENTS IN CONTEXT: 3\n",
      "BATCH SIZE: None\n",
      "SAVE EVERY: 250\n",
      "Saving DataLoader contents to JSON...\n",
      "DataLoader contents saved to C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\dataloader_contents.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from typing import Tuple, Dict, Optional\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "from utils import *\n",
    "from bgm import BGM\n",
    "from default_prompts import *\n",
    "from prompt_dataset import PromptDataset\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "device = torch.device(f\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "warnings.filterwarnings('ignore')\n",
    "SEED=10\n",
    "\n",
    "info = {\n",
    "    \"nq_bgm\": {\n",
    "        \"train\": {\n",
    "            \"data_path\": r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\10k_train_dataset.json',\n",
    "            \"contriever_search_results_path\": r\"C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\processed\\contriever_search_results_at150.pkl\",\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "def save_dataloader_to_json(dataloader, output_file, num_examples=15):\n",
    "    all_batches = []\n",
    "\n",
    "    print(\"Saving DataLoader contents to JSON...\")\n",
    "    for idx, batch in enumerate(dataloader):\n",
    "        if idx >= num_examples:  # Stop after saving the specified number of examples\n",
    "            break\n",
    "\n",
    "        batch_dict = {}\n",
    "        for key, value in batch.items():\n",
    "            # Convert tensors to lists for JSON serialization\n",
    "            if isinstance(value, torch.Tensor):\n",
    "                batch_dict[key] = value.tolist()\n",
    "            else:\n",
    "                batch_dict[key] = value\n",
    "        all_batches.append(batch_dict)\n",
    "    \n",
    "    # Save the entire list of dictionaries to a JSON file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_batches, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"DataLoader contents saved to {output_file}\")\n",
    "\n",
    "class DotDict:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "def parse_arguments(custom_args=None):\n",
    "    \"\"\"\n",
    "    Mimics argparse to parse arguments for LLM generation. Accepts custom arguments as a dictionary for notebooks.\n",
    "    \"\"\"\n",
    "    # Define default values\n",
    "    default_args = {\n",
    "        'output_dir': r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\gen_id_document_bgm',\n",
    "        'llm_id': 'google/flan-t5-large',\n",
    "        'dataset': 'nq_bgm',\n",
    "        'model_max_length': 4096,\n",
    "        'quantization_bits': 4,\n",
    "        'use_model_chat_template': False, \n",
    "        'gold_position': None,\n",
    "        'num_retrieved_documents': 5,\n",
    "        'use_test': False,\n",
    "        'max_new_tokens': 50,\n",
    "        'use_task_with_proof': False,\n",
    "        'batch_size': None,\n",
    "        'save_every': 250,\n",
    "    }\n",
    "\n",
    "    # If custom_args is provided, update defaults\n",
    "    if custom_args:\n",
    "        default_args.update(custom_args)\n",
    "\n",
    "    # Perform validation\n",
    "    if default_args['num_retrieved_documents'] is None:\n",
    "        raise ValueError(\"'num_retrieved_documents' must be specified.\")\n",
    "    if default_args['num_retrieved_documents'] <= 0:\n",
    "        raise ValueError(\"'num_retrieved_documents' must be a positive integer.\")\n",
    "    if default_args['gold_position'] is not None:\n",
    "        if (default_args['gold_position'] < 0 or \n",
    "            default_args['gold_position'] >= default_args['num_retrieved_documents']):\n",
    "            raise ValueError(\"'gold_position' must be within the range of 'num_retrieved_documents'.\")\n",
    "\n",
    "    return DotDict(**default_args)\n",
    "\n",
    "\n",
    "def load_corpus(\n",
    "    args: argparse.Namespace\n",
    ") -> Tuple[List[Dict], Optional[Dict[int, int]]]:\n",
    "    \n",
    "    # Corpus with documents from Contriever\n",
    "    corpus, full_to_subset_idx_map = read_corpus_with_contriever()\n",
    "\n",
    "    return corpus, full_to_subset_idx_map\n",
    "\n",
    "def load_search_results(args: argparse.Namespace) -> List[Tuple[List[int], List[float]]]:\n",
    "\n",
    "    search_results_path = info[args.dataset][args.split]['contriever_search_results_path']\n",
    "    retriever_search_results = read_pickle(search_results_path)\n",
    "\n",
    "    return retriever_search_results\n",
    "\n",
    "\n",
    "def get_prompt_template(args: argparse.Namespace):\n",
    "    prompt_configuration = args.dataset\n",
    "    if args.use_model_chat_template:\n",
    "        chat_task_template_str = chat_task_templates[args.llm_id]['template']\n",
    "        \n",
    "        task_instruction = task_instructions[prompt_configuration]\n",
    "\n",
    "        prompt_template = apply_chat_task_template(chat_task_template_str, task_instruction)\n",
    "    else:\n",
    "        task_template = task_templates[prompt_configuration]\n",
    "\n",
    "        prompt_template = task_template.create_prompt_template()\n",
    "\n",
    "    return prompt_template\n",
    "\n",
    "\n",
    "def initialize_dataset_and_loader(\n",
    "    args: argparse.Namespace, \n",
    "    corpus: List[Dict], \n",
    "    full_to_subset_idx_map: Optional[Dict[int, int]], \n",
    "    retriever_search_results: List[Tuple[List[int], List[float]]], \n",
    "    tokenizer: PreTrainedTokenizer\n",
    ") -> DataLoader:\n",
    "    \n",
    "    prompt_template = get_prompt_template(args)\n",
    "    \n",
    "    prompt_ds = PromptDataset(\n",
    "        corpus=corpus, data_path=info[args.dataset][args.split]['data_path'], \n",
    "        tokenizer=tokenizer, \n",
    "        max_tokenized_length=args.model_max_length - 2, \n",
    "        search_results=retriever_search_results,\n",
    "        prompt_template=prompt_template,\n",
    "        full_to_subset_idx_map=full_to_subset_idx_map,\n",
    "        do_normalize_query=True, \n",
    "        num_documents_in_context=args.num_retrieved_documents,\n",
    "        gold_position=args.gold_position, # None in these experiments\n",
    "    )\n",
    "        \n",
    "    prompt_dataloader = DataLoader(\n",
    "        prompt_ds,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=8,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    return prompt_dataloader\n",
    "\n",
    "\n",
    "def print_info(args: argparse.Namespace):\n",
    "    print(\"INFO:\")    \n",
    "    print(f\"DATA: {info[args.dataset][args.split]['data_path']}\")\n",
    "    print(f\"USE TEST: {args.use_test}\")\n",
    "    print(f\"MODEL: {args.llm_id}\")\n",
    "    print(f\"MODEL MAX LENGTH: {args.model_max_length}\")\n",
    "    print(f'MAX NEW TOKENS: {args.max_new_tokens}')\n",
    "    print(f\"USE MODEL CHAT TEMPLATE: {args.use_model_chat_template}\")\n",
    "    print(f\"TASK WITH PROOF:\", args.use_task_with_proof)\n",
    "    print(f\"GOLD POSITION: {args.gold_position}\")\n",
    "    print(f\"NUM DOCUMENTS IN CONTEXT: {args.num_retrieved_documents}\")\n",
    "    print(f\"BATCH SIZE: {args.batch_size}\")\n",
    "    print(f\"SAVE EVERY: {args.save_every}\")\n",
    "\n",
    "\n",
    "def extract_generate_answers(\n",
    "    args: argparse.Namespace, \n",
    "    generated_output: List[str]\n",
    ") -> List[str]:\n",
    "    answer_prefix = \"Answer:\"\n",
    "    if args.use_model_chat_template:\n",
    "        answer_prefix = re.escape(chat_task_templates[args.llm_id]['answer_prefix'])\n",
    "\n",
    "    generated_answers = []\n",
    "    for output in generated_output:\n",
    "        matches = list(re.finditer(answer_prefix, output))\n",
    "        match_idx = 0\n",
    "\n",
    "        # When using the proof there is a one-shot example that already \n",
    "        # contains the string \"Answer:\". Thus, we should get the second (match_idx=1) match.\n",
    "        if args.use_model_chat_template and answer_prefix != \"Answer:\":\n",
    "            match_idx = 0\n",
    " \n",
    "        answer_end = matches[match_idx].end()\n",
    "        response = output[answer_end:].strip()\n",
    "        generated_answers.append(response)\n",
    "    \n",
    "    return generated_answers\n",
    "\n",
    "\n",
    "def BGMTraining(\n",
    "    args: argparse.Namespace, \n",
    "    prompt_ds: PromptDataset,\n",
    "    llm: BGM, \n",
    "    prompt_dataloader: DataLoader\n",
    "):\n",
    "    # Info from arguments\n",
    "    llm_id = args.llm_id\n",
    "    num_doc = args.num_retrieved_documents\n",
    "    save_every = args.save_every\n",
    "    retriever_str = \"contriever\" \n",
    "    chat_template_str = \"_template\" if args.use_model_chat_template else \"\"\n",
    "    prompt_type = \"retrieved_proof\" if args.use_task_with_proof else \"retrieved\"\n",
    "\n",
    "    # Create the saving directory\n",
    "    llm_folder = llm_id.split(\"/\")[1] if '/' in llm_id else llm_id\n",
    "    saving_dir = f\"{args.output_dir}/{args.dataset}/{llm_folder}/{args.split}/{prompt_type}/{retriever_str}/{num_doc}_doc\"\n",
    "    os.makedirs(saving_dir, exist_ok=True)\n",
    "\n",
    "    all_info = []  \n",
    "    for idx, prompt_batch in enumerate(tqdm(prompt_dataloader)):\n",
    "        prompts = prompt_batch['prompt']\n",
    "        example_id = prompt_batch['example_id']\n",
    "        prompts = prompt_batch['prompt']\n",
    "        query = prompt_batch['query']\n",
    "        document_indices=prompt_batch['document_indices']\n",
    "        \n",
    "        for doc_idx in document_indices:\n",
    "            \n",
    "            candidate_docs += doc_idx\n",
    "\n",
    "            formatted_docs, _ = prompt_ds._get_documents_from_indices(candidate_docs)\n",
    "\n",
    "            if '\\nAnswer:' not in candidate_prompt:\n",
    "                candidate_prompt += '\\nAnswer:'\n",
    "\n",
    "            \n",
    "\n",
    "        generated_output = llm.generate(\n",
    "            prompts, \n",
    "            max_new_tokens=args.max_new_tokens\n",
    "        )\n",
    "\n",
    "        generated_answers = extract_generate_answers(args, generated_output)\n",
    "        prompt_batch['generated_answer'] = generated_answers\n",
    "        \n",
    "        all_info.append(prompt_batch)\n",
    "        '''\n",
    "        if (idx + 1) % save_every == 0 or (idx + 1) == len(prompt_dataloader):\n",
    "            print(f\"Saving at {idx + 1}...\")\n",
    "            file_name = f\"{saving_dir}/numdoc{num_doc}_retr{args.num_retrieved_documents}{chat_template_str}_info_{idx+1}.pkl\"\n",
    "            write_pickle(all_info, file_name)\n",
    "            all_info = []\n",
    "        '''\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = parse_arguments()\n",
    "\n",
    "    args.split = \"test\" if args.use_test else \"train\"\n",
    "\n",
    "    print(\"Loading LLM...\")\n",
    "    llm_id = args.llm_id\n",
    "    bgm = BGM(\n",
    "        llm_id, device,  \n",
    "        model_max_length=args.model_max_length\n",
    "    )\n",
    "    tokenizer = bgm.tokenizer\n",
    "    print(\"LLM loaded\")\n",
    "\n",
    "\n",
    "    print(\"Loading corpus and search results...\")\n",
    "    corpus, full_to_subset_idx_map = load_corpus(args)\n",
    "    retriever_search_results = load_search_results(args)\n",
    "    print(\"Corpus and search results loaded\")\n",
    "\n",
    "\n",
    "    print(\"Loading prompt dataset...\")\n",
    "    prompt_ds, prompt_dataloader = initialize_dataset_and_loader(\n",
    "        args, corpus, full_to_subset_idx_map, \n",
    "        retriever_search_results, tokenizer\n",
    "    )\n",
    "    print(\"Prompt dataset loaded\")\n",
    "\n",
    "    print_info(args)\n",
    "\n",
    "    #output_json_path = r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\dataloader_contents.json'\n",
    "    #save_dataloader_to_json(prompt_dataloader, output_json_path, num_examples=15)\n",
    "        \n",
    "    BGMTraining(args, prompt_ds, bgm, prompt_dataloader)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    seed_everything(SEED)\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File aggiornato salvato in: C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\gen_ids_document_training_set_bgm\\nq_training\\gemma-2-2b-it\\train\\retrieved\\contriever\\5_doc\\numdoc5_retr5_template_info_all_extended_updated.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def match_example_ids(file1_path, file2_path, output_path):\n",
    "    \"\"\"\n",
    "    Modifica il file1 aggiungendo l'example_id da file2 quando query e question corrispondono.\n",
    "\n",
    "    Args:\n",
    "        file1_path (str): Percorso al file JSON di input 1.\n",
    "        file2_path (str): Percorso al file JSON di input 2.\n",
    "        output_path (str): Percorso al file JSON di output aggiornato.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Caricamento dei file JSON\n",
    "        with open(file1_path, 'r') as f1:\n",
    "            file1 = json.load(f1)\n",
    "\n",
    "        with open(file2_path, 'r') as f2:\n",
    "            file2 = json.load(f2)\n",
    "\n",
    "        # Creazione di un dizionario per mappare le domande agli example_id\n",
    "        question_to_example_id = {item['question']: item['example_id'] for item in file2}\n",
    "\n",
    "        # Modifica del primo file\n",
    "        for entry in file1:\n",
    "            query = entry.get('query')\n",
    "            if query in question_to_example_id:\n",
    "                entry['example_id'] = question_to_example_id[query]\n",
    "\n",
    "        # Salvataggio del file aggiornato\n",
    "        with open(output_path, 'w') as f1_updated:\n",
    "            json.dump(file1, f1_updated, indent=4)\n",
    "\n",
    "        print(f\"File aggiornato salvato in: {output_path}\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Errore: {e}\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Errore nel parsing del file JSON: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Errore imprevisto: {e}\")\n",
    "\n",
    "def update_queries_with_document_indices(file1_path, file2_path, output_path):\n",
    "    # Carica i dati dai file JSON\n",
    "    with open(file1_path, 'r', encoding='utf-8') as f1, open(file2_path, 'r', encoding='utf-8') as f2:\n",
    "        file1_data = json.load(f1)\n",
    "        file2_data = json.load(f2)\n",
    "\n",
    "    # Crea un dizionario per mappare le query ai document_indices di File 2\n",
    "    query_to_indices = {\n",
    "        entry['query']: entry.get('document_indices', [])\n",
    "        for entry in file2_data\n",
    "    }\n",
    "\n",
    "    # Aggiorna File 1 aggiungendo i document_indices associati alle query\n",
    "    for entry in file1_data:\n",
    "        query = entry['query']\n",
    "        if query in query_to_indices:\n",
    "            entry['document_indices'] = query_to_indices[query]\n",
    "\n",
    "    # Salva il risultato in un nuovo file JSON\n",
    "    with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "        json.dump(file1_data, output_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "\n",
    "path_output=r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\gen_ids_document_training_set_bgm\\nq_training\\gemma-2-2b-it\\train\\retrieved\\contriever\\5_doc\\numdoc5_retr5_template_info_all_extended_updated.json'\n",
    "file_da_modificare = r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\gen_ids_document_training_set_bgm\\nq_training\\gemma-2-2b-it\\train\\retrieved\\contriever\\5_doc\\numdoc5_retr5_template_info_all_extended.json'\n",
    "file_di_confronto = r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\10k_train_dataset.json'\n",
    "\n",
    "match_example_ids(file_da_modificare, file_di_confronto, path_output)\n",
    "\n",
    "update_queries_with_document_indices(r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\gen_ids_document_training_set_bgm\\nq_training\\gemma-2-2b-it\\train\\retrieved\\contriever\\5_doc\\numdoc5_retr5_template_info_all_extended_updated.json', r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\gen_ids_document_training_set_bgm\\nq_training\\gemma-2-2b-it\\train\\retrieved\\contriever\\5_doc\\numdoc5_retr5_template_info_all_extended_updated.json', r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\gen_ids_document_training_set_bgm\\nq_training\\gemma-2-2b-it\\train\\retrieved\\contriever\\5_doc\\numdoc5_retr5_template_info_all_extended_updated_last.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "# Percentuali per ogni caso\n",
    "percentages = {\n",
    "    \"case_1_single_doc\": 0.1,\n",
    "    \"case_2_multiple_docs\": 0.2,\n",
    "    \"case_3_no_docs\": 0.1,\n",
    "    \"case_4_less_docs\": 0.4,\n",
    "    \"case_5_reranking\": 0.2,\n",
    "}\n",
    "\n",
    "# Task instruction da aggiungere a ogni query\n",
    "task_instruction = \"Output only the document IDs relevant to the query. Use this format: [ID1, ID2, ...].\"\n",
    "\n",
    "def process_data(input_file, output_file):\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        examples = json.load(f)\n",
    "\n",
    "    dataset = []\n",
    "    \n",
    "    # Shuffle examples to ensure random sampling\n",
    "    random.shuffle(examples)\n",
    "\n",
    "    # Total examples to be processed for each case\n",
    "    total_examples = len(examples)\n",
    "    case_limits = {case: int(total_examples * perc) for case, perc in percentages.items()}\n",
    "    case_counters = {case: 0 for case in percentages}\n",
    "\n",
    "    for example in examples:\n",
    "        if all(count >= case_limits[case] for case, count in case_counters.items()):\n",
    "            break  # Stop if all case limits are met\n",
    "        \n",
    "        query = f\"Task Instruction: {task_instruction}\\nQuestion:{example['query']}\"  # Aggiunge la task instruction\n",
    "        retrieved_docs = example[\"document_indices\"]\n",
    "        selected_docs = example[\"selected_documents\"]\n",
    "        are_answer = example[\"are_answer\"]\n",
    "\n",
    "        # Case 1: Single document correct answer\n",
    "        if are_answer and len(selected_docs) == 1 and case_counters[\"case_1_single_doc\"] < case_limits[\"case_1_single_doc\"]:\n",
    "            dataset.append({\n",
    "                \"input\": {\n",
    "                    \"query\": query,\n",
    "                    \"retrieved_docs\": retrieved_docs,\n",
    "                },\n",
    "                \"output\": selected_docs,\n",
    "            })\n",
    "            case_counters[\"case_1_single_doc\"] += 1\n",
    "\n",
    "        # Case 2: Multiple documents correct answer\n",
    "        elif are_answer and len(selected_docs) > 1 and case_counters[\"case_2_multiple_docs\"] < case_limits[\"case_2_multiple_docs\"]:\n",
    "            dataset.append({\n",
    "                \"input\": {\n",
    "                    \"query\": query,\n",
    "                    \"retrieved_docs\": retrieved_docs,\n",
    "                },\n",
    "                \"output\": selected_docs,\n",
    "            })\n",
    "            case_counters[\"case_2_multiple_docs\"] += 1\n",
    "\n",
    "        # Case 3: No documents correct answer\n",
    "        elif are_answer and len(selected_docs) == 0 and case_counters[\"case_3_no_docs\"] < case_limits[\"case_3_no_docs\"]:\n",
    "            dataset.append({\n",
    "                \"input\": {\n",
    "                    \"query\": query,\n",
    "                    \"retrieved_docs\": [],\n",
    "                },\n",
    "                \"output\": [],\n",
    "            })\n",
    "            case_counters[\"case_3_no_docs\"] += 1\n",
    "\n",
    "        # Case 4: Input and output unchanged\n",
    "        elif are_answer and len(selected_docs) > 2 and case_counters[\"case_4_less_docs\"] < case_limits[\"case_4_less_docs\"]:\n",
    "            dataset.append({\n",
    "                \"input\": {\n",
    "                    \"query\": query,\n",
    "                    \"retrieved_docs\": selected_docs,\n",
    "                },\n",
    "                \"output\": selected_docs,\n",
    "            })\n",
    "            case_counters[\"case_4_less_docs\"] += 1\n",
    "\n",
    "        # Case 5: Reranking\n",
    "        elif are_answer and len(selected_docs) > 2 and case_counters[\"case_5_reranking\"] < case_limits[\"case_5_reranking\"]:\n",
    "            reranked_docs = random.sample(selected_docs, len(selected_docs))  # Randomize order\n",
    "            dataset.append({\n",
    "                \"input\": {\n",
    "                    \"query\": query,\n",
    "                    \"retrieved_docs\": reranked_docs,\n",
    "                },\n",
    "                \"output\": selected_docs,\n",
    "            })\n",
    "            case_counters[\"case_5_reranking\"] += 1\n",
    "\n",
    "    # Save the dataset to a file\n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(dataset, f, indent=4)\n",
    "\n",
    "# Path to input and output files\n",
    "input_file = r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\gen_ids_document_training_set_bgm\\nq_training\\gemma-2-2b-it\\train\\retrieved\\contriever\\5_doc\\numdoc5_retr5_info_all_extended_training_set.json'\n",
    "output_file = r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\training_dataset.json'\n",
    "\n",
    "process_data(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Totale esempi nel file di input: 3000\n",
      "Esempi con 'are_answer=True': 1233\n",
      "Esempi con 'selected_documents == 0': 366\n",
      "Esempi con 'selected_documents == 1': 736\n",
      "Esempi con 'selected_documents > 1': 131\n",
      "Distribuzione pianificata degli esempi nel dataset creato:\n",
      "case_1_single_doc: 51\n",
      "case_2_multiple_docs: 52\n",
      "case_3_no_docs: 36\n",
      "case_4_multi_doc_unchanged: 45\n",
      "case_5_reranking: 65\n",
      "case_6_single_doc_unchanged: 36\n",
      "Esempi effettivamente inclusi nel dataset creato:\n",
      "case_1_single_doc: 51\n",
      "case_2_multiple_docs: 52\n",
      "case_3_no_docs: 36\n",
      "case_4_multi_doc_unchanged: 45\n",
      "case_5_reranking: 34\n",
      "case_6_single_doc_unchanged: 36\n",
      "Totale degli Esempi inclusi nel training dataset creato: 254\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "# Percentuali per ogni caso\n",
    "percentages = {\n",
    "    \"case_1_single_doc\": 0.07,\n",
    "    \"case_2_multiple_docs\": 0.4,\n",
    "    \"case_3_no_docs\": 0.1,\n",
    "    \"case_4_multi_doc_unchanged\": 0.35,\n",
    "    \"case_5_reranking\": 0.5,\n",
    "    \"case_6_single_doc_unchanged\": 0.05,\n",
    "}\n",
    "\n",
    "# Task instruction da aggiungere a ogni query\n",
    "task_instruction = \"Output only the document IDs relevant to the query. Use this format: [ID1, ID2, ...].\"\n",
    "\n",
    "def process_data(input_file, output_file):\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        examples = json.load(f)\n",
    "\n",
    "    # Filtra gli esempi con are_answer = true\n",
    "    valid_examples = [ex for ex in examples if ex[\"are_answer\"] is True]\n",
    "\n",
    "    print(f\"Totale esempi nel file di input: {len(examples)}\")\n",
    "    print(f\"Esempi con 'are_answer=True': {len(valid_examples)}\")\n",
    "\n",
    "    # Raggruppa per numero di selected_documents\n",
    "    grouped_examples = {\n",
    "        \"len_0\": [ex for ex in valid_examples if len(ex[\"selected_documents\"]) == 0],\n",
    "        \"len_1\": [ex for ex in valid_examples if len(ex[\"selected_documents\"]) == 1],\n",
    "        \"len_gt_1\": [ex for ex in valid_examples if len(ex[\"selected_documents\"]) > 1],\n",
    "    }\n",
    "\n",
    "    print(f\"Esempi con 'selected_documents == 0': {len(grouped_examples['len_0'])}\")\n",
    "    print(f\"Esempi con 'selected_documents == 1': {len(grouped_examples['len_1'])}\")\n",
    "    print(f\"Esempi con 'selected_documents > 1': {len(grouped_examples['len_gt_1'])}\")\n",
    "\n",
    "    # Calcola le suddivisioni per ogni gruppo\n",
    "    group_case_limits = {\n",
    "        \"case_1_single_doc\": int(len(grouped_examples[\"len_1\"]) * percentages[\"case_1_single_doc\"]),\n",
    "        \"case_2_multiple_docs\": int(len(grouped_examples[\"len_gt_1\"]) * percentages[\"case_2_multiple_docs\"]),\n",
    "        \"case_3_no_docs\": int(len(grouped_examples[\"len_0\"]) * percentages[\"case_3_no_docs\"]),\n",
    "        \"case_4_multi_doc_unchanged\": int(len(grouped_examples[\"len_gt_1\"]) * percentages[\"case_4_multi_doc_unchanged\"]),\n",
    "        \"case_5_reranking\": int(len(grouped_examples[\"len_gt_1\"]) * percentages[\"case_5_reranking\"]),\n",
    "        \"case_6_single_doc_unchanged\": int(len(grouped_examples[\"len_1\"]) * percentages[\"case_6_single_doc_unchanged\"]),\n",
    "    }\n",
    "\n",
    "    print(\"Distribuzione pianificata degli esempi nel dataset creato:\")\n",
    "    for case, limit in group_case_limits.items():\n",
    "        print(f\"{case}: {limit}\")\n",
    "\n",
    "    dataset = []\n",
    "    case_counters = {case: 0 for case in group_case_limits}\n",
    "\n",
    "    # Processa gli esempi\n",
    "    for example in valid_examples:\n",
    "        query = f\"Task Instruction: {task_instruction}\\nQuestion:{example['query']}\"\n",
    "        retrieved_docs = example[\"document_indices\"]\n",
    "        selected_docs = example[\"selected_documents\"]\n",
    "\n",
    "        # Case 1: Single document correct answer\n",
    "        if len(selected_docs) == 1 and case_counters[\"case_1_single_doc\"] < group_case_limits[\"case_1_single_doc\"]:\n",
    "            dataset.append({\n",
    "                \"input\": {\n",
    "                    \"query\": query,\n",
    "                    \"retrieved_docs\": retrieved_docs,\n",
    "                },\n",
    "                \"output\": selected_docs,\n",
    "            })\n",
    "            case_counters[\"case_1_single_doc\"] += 1\n",
    "\n",
    "        # Case 2: Multiple documents correct answer\n",
    "        elif len(selected_docs) > 1 and case_counters[\"case_2_multiple_docs\"] < group_case_limits[\"case_2_multiple_docs\"]:\n",
    "            dataset.append({\n",
    "                \"input\": {\n",
    "                    \"query\": query,\n",
    "                    \"retrieved_docs\": retrieved_docs,\n",
    "                },\n",
    "                \"output\": selected_docs,\n",
    "            })\n",
    "            case_counters[\"case_2_multiple_docs\"] += 1\n",
    "\n",
    "        # Case 3: No documents correct answer\n",
    "        elif len(selected_docs) == 0 and case_counters[\"case_3_no_docs\"] < group_case_limits[\"case_3_no_docs\"]:\n",
    "            dataset.append({\n",
    "                \"input\": {\n",
    "                    \"query\": query,\n",
    "                    \"retrieved_docs\": [],\n",
    "                },\n",
    "                \"output\": [],\n",
    "            })\n",
    "            case_counters[\"case_3_no_docs\"] += 1\n",
    "\n",
    "        # Case 4: Input and output unchanged for multiple docs\n",
    "        elif len(selected_docs) > 1 and case_counters[\"case_4_multi_doc_unchanged\"] < group_case_limits[\"case_4_multi_doc_unchanged\"]:\n",
    "            dataset.append({\n",
    "                \"input\": {\n",
    "                    \"query\": query,\n",
    "                    \"retrieved_docs\": selected_docs,\n",
    "                },\n",
    "                \"output\": selected_docs,\n",
    "            })\n",
    "            case_counters[\"case_4_multi_doc_unchanged\"] += 1\n",
    "\n",
    "        # Case 6: Input and output unchanged for single doc\n",
    "        elif len(selected_docs) == 1 and case_counters[\"case_6_single_doc_unchanged\"] < group_case_limits[\"case_6_single_doc_unchanged\"]:\n",
    "            dataset.append({\n",
    "                \"input\": {\n",
    "                    \"query\": query,\n",
    "                    \"retrieved_docs\": selected_docs,\n",
    "                },\n",
    "                \"output\": selected_docs,\n",
    "            })\n",
    "            case_counters[\"case_6_single_doc_unchanged\"] += 1\n",
    "\n",
    "        # Case 5: Reranking\n",
    "        elif len(selected_docs) > 1 and case_counters[\"case_5_reranking\"] < group_case_limits[\"case_5_reranking\"]:\n",
    "            reranked_docs = selected_docs[:]\n",
    "            while reranked_docs == selected_docs:  # Garantisce che l'ordine sia diverso\n",
    "                reranked_docs = random.sample(selected_docs, len(selected_docs))\n",
    "            dataset.append({\n",
    "                \"input\": {\n",
    "                    \"query\": query,\n",
    "                    \"retrieved_docs\": reranked_docs,\n",
    "                },\n",
    "                \"output\": selected_docs,\n",
    "            })\n",
    "            case_counters[\"case_5_reranking\"] += 1\n",
    "\n",
    "    print(\"Esempi effettivamente inclusi nel dataset creato:\")\n",
    "    tot=0\n",
    "    for case, count in case_counters.items():\n",
    "        tot += count\n",
    "        print(f\"{case}: {count}\")\n",
    "\n",
    "    print(f\"Totale degli Esempi inclusi nel training dataset creato: {tot}\")\n",
    "    \n",
    "\n",
    "    # Save the dataset to a file\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(dataset, f, indent=4)\n",
    "\n",
    "# Path to input and output files\n",
    "input_file = r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\gen_ids_document_training_set_bgm\\nq_training\\gemma-2-2b-it\\train\\retrieved\\contriever\\5_doc\\numdoc5_retr5_info_all_extended_training_set.json'\n",
    "output_file = r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\training_dataset.json'\n",
    "\n",
    "process_data(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LLM...\n",
      "LoRA weights loaded from: C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\lora_training_bgm\\lora-checkpoint\\epochs\\epoch_25\n",
      "LLM loaded\n",
      "Loading corpus and search results...\n",
      "Corpus and search results loaded\n",
      "Loading prompt dataset...\n",
      "Prompt dataset loaded\n",
      "INFO:\n",
      "DATA: C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\test_dataset.json\n",
      "USE TEST: True\n",
      "MODEL: google-t5/t5-base\n",
      "MODEL MAX LENGTH: 4096\n",
      "MAX NEW TOKENS: 15\n",
      "USE MODEL CHAT TEMPLATE: False\n",
      "TASK WITH PROOF: False\n",
      "GOLD POSITION: None\n",
      "NUM DOCUMENTS IN CONTEXT: 5\n",
      "BATCH SIZE: None\n",
      "SAVE EVERY: 250\n",
      "{'example_id': '-3290814144789249484', 'query': 'who got the first nobel prize in physics', 'prompt': 'You are given a question and you must respond based on the provided documents. You must always provide an answer.\\nQuestion: who got the first nobel prize in physics\\nDocuments:\\nDocument [628506](Title: Nobel Prize in Physics) receive a diploma, a medal and a document confirming the prize amount. Nobel Prize in Physics The Nobel Prize in Physics () is a yearly award given by the Royal Swedish Academy of Sciences for those who have made the most outstanding contributions for mankind in the field of physics. It is one of the five Nobel Prizes established by the will of Alfred Nobel in 1895 and awarded since 1901; the others being the Nobel Prize in Chemistry, Nobel Prize in Literature, Nobel Peace Prize, and Nobel Prize in Physiology or Medicine. The first Nobel Prize in Physics was\\nDocument [3546609](Title: E. C. George Sudarshan) had developed the breakthrough. In 2007, Sudarshan told the \"Hindustan Times\", \"The 2005 Nobel prize for Physics was awarded for my work, but I wasn\\'t the one to get it. Each one of the discoveries that this Nobel was given for work based on my research.\" Sudarshan also commented on not being selected for the 1979 Nobel, \"Steven Weinberg, Sheldon Glashow and Abdus Salam built on work I had done as a 26-year-old student. If you give a prize for a building, shouldn’t the fellow who built the first floor be given the prize before those who built the second\\nDocument [439756](Title: University of Chicago) Medal, which is rewarded annually to the best economist under the age of 40, has also been awarded to 4 current members of the university faculty. Notable faculty in physics have included the speed of light calculator A. A. Michelson, elementary charge calculator Robert A. Millikan, discoverer of the Compton Effect Arthur H. Compton, the creator of the first nuclear reactor Enrico Fermi, \"the father of the hydrogen bomb\" Edward Teller, \"one of the most brilliant and productive experimental physicists of the twentieth century\" Luis Walter Alvarez, Murray Gell-Mann who introduced the quark, second female Nobel laureate Maria Goeppert-Mayer, the\\nDocument [1860765](Title: Tsung-Dao Lee) 1956, with her so-called Wu experiment. Lee was the youngest Nobel laureate after World War II until Malala Yousafzai was awarded the Nobel Peace Prize in 2014. He is the fourth youngest Nobel laureate in history after William L. Bragg (who won the prize at 25 with his father William H. Bragg in 1915), Werner Heisenberg (who won in 1932 also at 30) and Malala Yousafzai (awarded at just 17). Lee and Yang were the first Chinese laureates. Since he became a naturalized American citizen in 1962, Lee is also the youngest American ever to have won a Nobel Prize.\\nDocument [2043329](Title: Universology) become the chief proponent of universology today. \"Everything in this universe is part of an uninterrupted sequence of events\" Mohri has said. In 1872 Andrews published \"The Basic Outline of Universology\" which was subtitled \"An introduction to the newly discovered science of the universe, its elementary principles, and the first stages of their development in the special sciences.\" Ilya Romanovich Prigogine (born on January 25, 1917) was a Belgian and American physicist and chemist who was born in Russia and became a Nobel Prize laureate in chemistry. In the book \"Order Out of Chaos: Man\\'s New Dialogue With Nature\", which\\nAnswer:', 'document_indices': [628506, 3546609, 439756, 1860765, 2043329], 'gold_document_idx': '20994698', 'prompt_tokens_len': 844}\n",
      "{'example_id': '8851020722386421469', 'query': 'when is the next deadpool movie being released', 'prompt': 'You are given a question and you must respond based on the provided documents. You must always provide an answer.\\nQuestion: when is the next deadpool movie being released\\nDocuments:\\nDocument [3201523](Title: Cable & Deadpool) the symbiote. However, Deadpool stabs himself in the head with the psimitar, and is released from the grip of the symbiote. Deadpool awakes to find out from the Avengers that the dinosaurs have been returned to the jungle thanks to Weasel. It all ends with Deadpool back at his apartment watching TV. Deadpool had been blamed for the incident by the media on TV. Weasel, Bob, Irene Merryweather, Agent X, Sandi, and Outlaw all come in to hang out with Deadpool. The series ends with Deadpool asking his friends, \"So... whaddyou guys want to watch?\" Cable & Deadpool Cable &\\nDocument [3246420](Title: X-Force) gray area. It\\'ll be raunchier, it\\'ll be rated R, I\\'m sure. We\\'ll get to see an ensemble movie that\\'s pushed, hopefully, as far as the Deadpool individual movie was pushed.\" The team (consisting of Domino, Fantomex and Archangel) appear in Deadpool\\'s ending in \"Ultimate Marvel vs. Capcom 3\". They are shown celebrating Deadpool\\'s victory over Galactus alongside Cable, Bob, Agent of HYDRA and several Capcom characters. In April 2010, the popular Web comic Homestar Runner featured a parody titled \"Xeriouxly Forxe\", in which all of the site\\'s main characters except Homsar were re-designed to look like X-Men. X-Force X-Force is\\nDocument [10435180](Title: X-Men (film series)) marked both Hugh Jackman and Patrick Stewart\\'s return as Wolverine and Xavier, respectively. After a personal tragedy, Deadpool creates the X-Force to save a young mutant from the time-traveling soldier Cable. In September 2015, Kinberg said that a sequel for \"Deadpool\" was in development. By the release of \"Deadpool\", 20th Century Fox greenlit the film, with Rheese and Wernick returning to write, and Miller being looked at to return as director, as he was working on the script at the time. However in October 2016, Miller left the film due to creative differences with Reynolds and was replaced by David\\nDocument [3201509](Title: Cable & Deadpool) Deadpool, with Weasel, goes to the Hydra Headquarters where Agent X is being held, in Pakistan. Using his small size to his advantage, Deadpool infiltrates the Headquarters and captures Bob, Agent of HYDRA, to help him get to Agent X. It is then found that the actuator has been used to give Agent X morbid obesity by scientists experimenting with the actuator, and that it can not be removed by the actuator. Deadpool then forces the scientists to turn him back to his normal size (\"They had some Pym Particles lying around. Figured they must. Who doesn\\'t, right?\") and gets\\nDocument [3180056](Title: Rob Liefeld) on the graphic novel \"Deadpool: Bad Blood\", which is set for release later that year. In 2017, it was reported by Deadline that Liefeld is working with Akiva Goldsman and Graham King on a seven-figure movie deal for his Extreme Universe. Liefeld\\'s name has become something of a lightning rod in the industry. In an interview, Brian Michael Bendis described the polarization of opinion on Liefeld: \"There is a great dichotomy...There\\'s either some great and generous story about [Liefeld] or you will hear some unbelievable thing like, \\'How is he not in jail if he did that?\\' There is no\\nAnswer:', 'document_indices': [3201523, 3246420, 10435180, 3201509, 3180056], 'gold_document_idx': '21032933', 'prompt_tokens_len': 931}\n",
      "{'example_id': '955374967862684316', 'query': 'the south west wind blows across nigeria between', 'prompt': \"You are given a question and you must respond based on the provided documents. You must always provide an answer.\\nQuestion: the south west wind blows across nigeria between\\nDocuments:\\nDocument [283714](Title: Geography of Nigeria) finally retreats from most part of Nigeria, and the West African atmosphere around April to May, leaving an empty atmosphere over Nigeria. The sun's rays enters into the atmosphere of Nigeria more intense than it does during the presence of the Tropical continental airmass, which contained dust (in form of haze) that reduced the intensity of the sun. The overheating of the west Africa land mass and Nigeria in particular creates a low pressure region over west Africa and Nigeria. This low pressure zone attracts the Tropical Maritime Airmass (MT) from the south Atlantic Ocean since areas of low pressures\\nDocument [283712](Title: Geography of Nigeria) of low pressure to develop over west Africa and Nigeria (between March to May). The Tropical continental airmass (CT) from the Sahara Desert in the northern part of West Africa, is weakened due to the overheating of the land surface in west Africa and Nigeria at this time. The Tropical continental airmass (CT) begins to retreat northwards to the Sahara Desert due to massive heating of the land which transfers heat in the form of convection into the Tropical continental airmass (CT) which constitutes the main layer of air above the land. This transfer of heat in the Tropical continental\\nDocument [283711](Title: Geography of Nigeria) West Africa comes directly under the sun at this time. The sun is overhead throughout west Africa and over Nigeria during this period of the sun's northward migration to the tropic of cancer in the northern hemisphere. The whole of West Africa is heated intensely as result of the increased insolation received from the sun being overhead over west Africa. Temperatures can climb as high as over west Africa during this time. Temperatures in the northern part of Nigeria can go as high as in cities like Maiduguri. The high temperatures coupled with an increase in insolation causes a region\\nDocument [283717](Title: Geography of Nigeria) northern end is south of the 15 degrees line at about 14 degrees. Nigeria's location in the wetter part of the easterly waves south of the 15 degree line creates wetter climatic conditions for Nigeria especially during the monsoons. The Tropical Continental Airmass (CT) locally known as the harmattan, is a wind originating from North Africa which crosses the Sahara Desert into west Africa to Nigeria. This airmass dominates Nigeria's climate during the dry season from December to March. The Tropical continental airmass is dusty and creates a haze within the atmosphere of west Africa and Nigeria when it predominates.\\nDocument [283713](Title: Geography of Nigeria) airmass (CT) in turn, causes the wind to expand and become lighter as this is the normal behaviour for winds moving above intensely heated grounds. The Tropical continental airmass (CT) loses its strength as a major airmass in the region of west Africa and over Nigeria at this time (around February in the southern part of Nigeria to June in northern Nigeria) and begins to retreat coupled with the rising of air in form of convection within this airmass (Tropical continental airmass (CT)), further weakening the dominance of the wind over west Africa and Nigeria. The Tropical continental airmass (CT)\\nAnswer:\", 'document_indices': [283714, 283712, 283711, 283717, 283713], 'gold_document_idx': '21032934', 'prompt_tokens_len': 785}\n",
      "{'example_id': '-4340755100872459608', 'query': 'what does hp mean in war and order', 'prompt': 'You are given a question and you must respond based on the provided documents. You must always provide an answer.\\nQuestion: what does hp mean in war and order\\nDocuments:\\nDocument [2979957](Title: Revolution in Military Affairs) transformed warfare, and, as a result, the question is not one of \"Does an RMA exist?\" but, rather, \"When did it begin, and what are its implications?\" Tied to this are surprisingly persistent questions about the use and value of air power, now more accurately seen as aerospace power. If nothing else, given the record of precision air power application, aerospace power advocates should not still have to spend as much time as they do arguing the merits of three-dimensional war and precision attack\\'s value to it. Modern joint service aerospace forces offer the most responsive, flexible, lethal, and devastating\\nDocument [3526617](Title: Navy Marine Corps Intranet) perspective, produced results that were far from optimal.” NMCI consolidated roughly 6,000 networks—some of which could not e-mail, let alone collaborate with each other—into a single integrated and secure IT environment. HP updated more than 100,000 desktop and laptop PCs in 2007. The program also consolidated an ad hoc network of more than 8,000 applications to 500 in four years and 15,003 logistics and readiness systems to 2,759 over a two-year period. Sub-contractors to HP include: HP also provides the security services once provided by Raytheon. HP also has worked with more than 400 small businesses, with 5 percent for\\nDocument [12643438](Title: A World At War) dropping. Kamikazes no longer defend against enemy aircraft; they are no longer doubled to attack ships, but instead attack with +4 DRM. Kamikazes pick targets at random (carriers, then battleships, in order of size and slowness, then light ships as a group, damaged ships first in every category) in the same way as submarines; they combine their attack if more than one selects the same target. Jets - only available after a high research result - fight at triple strength and have a +1 modifier in combat. For activities at sea each Army Air Factor splits into search, cover and\\nDocument [11673357](Title: HP 35s) and make decisions. All programs are stored in one continuous program space, and may be created and edited by switching to programming mode, using the key. Within the program space, up to twenty-six alphabetic labels may be defined in the form , and each label may be followed by up to 998 other steps, so that any step may be targeted by a (\"go to\") or (\"execute\") instruction in the form (or just for the label step itself, A001). Any steps before the first label are numbered with four digits, but these steps cannot be targeted. Subsequent insertion or deletion\\nDocument [12638363](Title: War and Peace (game)) Spain and Russia where it was harder for armies to live off the land; all units in their home country get a -1 DRM whereas unsupplied units get a +1 DRM). If more than one strength point is to be lost then one of them must be cavalry if there is one in the hex. Neutral countries are immune from attrition, enabling a defeated major power to rebuild their forces until they rejoin the war. Diplomacy takes place in the alliance phase. This system is used to allow nations to ally with one another. Alliances essentially allow you to acquire\\nAnswer:', 'document_indices': [2979957, 3526617, 12643438, 11673357, 12638363], 'gold_document_idx': '21032935', 'prompt_tokens_len': 802}\n",
      "{'example_id': '-4752044886865067782', 'query': 'who wrote the first declaration of human rights', 'prompt': 'You are given a question and you must respond based on the provided documents. You must always provide an answer.\\nQuestion: who wrote the first declaration of human rights\\nDocuments:\\nDocument [3932834](Title: Human Rights Day) and societies should \"strive by progressive measures, national and international, to secure their universal and effective recognition and observance\". The measure was received by both advocates and critics alike as \"being more declarative than legislative, more suggestive than binding.\" Although the Declaration with its broad range of political, civil, economic, social and cultural rights is not a binding document, it inspired more than 60 human rights instruments which together constitute an international standard of human rights. Today the general consent of all United Nations Member States on the basic Human Rights laid down in the Declaration makes it even stronger\\nDocument [182208](Title: Human rights) and peace in the world.\" The declaration was the first international legal effort to limit the behaviour of states and press upon them duties to their citizens. The UDHR was framed by members of the Human Rights Commission, with former First Lady Eleanor Roosevelt as Chair, who began to discuss an \"International Bill of Rights\" in 1947. The members of the Commission did not immediately agree on the form of such a bill of rights, and whether, or how, it should be enforced. The Commission proceeded to frame the UDHR and accompanying treaties, but the UDHR quickly became the priority.\\nDocument [13616094](Title: Human rights in the United States) prisoners at Guantanamo Bay and black sites, and extrajudicial targeted killings (Disposition Matrix). Some observers give the U.S. high to fair marks on human rights, while others charge it with a persistent pattern of human rights violations. The first human rights organization in the Thirteen Colonies of British America, dedicated to the abolition of slavery, was formed by Anthony Benezet in 1775. A year later, the Declaration of Independence announced that the Thirteen Colonies regarded themselves as independent states, and no longer a part of the British Empire. The Declaration stated \"that all men are created equal, that they are\\nDocument [781100](Title: Bartolomé de las Casas) In the Catholic Church, the Dominicans introduced his cause for canonization in 1976. In 2002 the Church began the process for his beatification. He has also come to be seen as an early advocate for a concept of universal human rights. He was among the first to develop a view of unity among humankind, stating that \"All people of the world are humans,\" and that they had a natural right to liberty – a combination of Thomist rights philosophy with Augustinian political theology. In this capacity, an ecumenical human rights institute located in San Cristóbal de las Casas, the Centro\\nDocument [434619](Title: Universal Declaration of Human Rights) the Rights of the Child, the United Nations Convention Against Torture, and many more. The Declaration continues to be widely cited by governments, academics, advocates, and constitutional courts, and by individuals who appeal to its principles for the protection of their recognised human rights. The Universal Declaration has received praise from a number of notable people. The Lebanese philosopher and diplomat Charles Malik called it \"an international document of the first order of importance\", while Eleanor Roosevelt—first chairwoman of the Commission on Human Rights (CHR) that drafted the Declaration—stated that it \"may well become the international Magna Carta of all\\nAnswer:', 'document_indices': [3932834, 182208, 13616094, 781100, 434619], 'gold_document_idx': '21032936', 'prompt_tokens_len': 786}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 500/2889 [02:11<10:29,  3.79it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Risultati salvati in: C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\gen_id_res_example_bgm/nq/t5-base/test/retrieved/contriever/5_doc\\generated_results_weights_epoch_25.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re \n",
    "import argparse\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from typing import Tuple, Dict, Optional\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "from utils import *\n",
    "from bgm import BGM\n",
    "from default_prompts import *\n",
    "from prompt_dataset import PromptDataset\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "device = torch.device(f\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "warnings.filterwarnings('ignore')\n",
    "SEED=10\n",
    "\n",
    "info = {\n",
    "    \"nq\": {\n",
    "        \"test\": {\n",
    "            \"data_path\": r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\test_dataset.json',\n",
    "            \"contriever_search_results_path\": r\"C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\processed\\contriever_test_search_results_at150.pkl\",\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "class DotDict:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "def parse_arguments(custom_args=None):\n",
    "    \"\"\"\n",
    "    Mimics argparse to parse arguments for LLM generation. Accepts custom arguments as a dictionary for notebooks.\n",
    "    \"\"\"\n",
    "    # Define default values\n",
    "    default_args = {\n",
    "        'output_dir': r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\gen_id_res_example_bgm',\n",
    "        'llm_id': 'google-t5/t5-base',\n",
    "        'dataset': 'nq',\n",
    "        'model_max_length': 4096,\n",
    "        'quantization_bits': 4,\n",
    "        'gold_position': None,\n",
    "        'use_model_chat_template': False, \n",
    "        'num_retrieved_documents': 5,\n",
    "        'use_test': True,\n",
    "        'padding_strategy': 'longest',\n",
    "        'max_new_tokens': 15,\n",
    "        'use_task_with_proof': False,\n",
    "        'batch_size': None,\n",
    "        'save_every': 250,\n",
    "    }\n",
    "\n",
    "    # If custom_args is provided, update defaults\n",
    "    if custom_args:\n",
    "        default_args.update(custom_args)\n",
    "\n",
    "    # Perform validation\n",
    "    if default_args['num_retrieved_documents'] is None:\n",
    "        raise ValueError(\"'num_retrieved_documents' must be specified.\")\n",
    "    if default_args['num_retrieved_documents'] <= 0:\n",
    "        raise ValueError(\"'num_retrieved_documents' must be a positive integer.\")\n",
    "    if default_args['gold_position'] is not None:\n",
    "        if (default_args['gold_position'] < 0 or \n",
    "            default_args['gold_position'] >= default_args['num_retrieved_documents']):\n",
    "            raise ValueError(\"'gold_position' must be within the range of 'num_retrieved_documents'.\")\n",
    "\n",
    "    return DotDict(**default_args)\n",
    "\n",
    "\n",
    "def load_corpus(\n",
    "    args: argparse.Namespace\n",
    ") -> Tuple[List[Dict], Optional[Dict[int, int]]]:\n",
    "    \n",
    "    # Corpus with documents from Contriever\n",
    "    corpus, full_to_subset_idx_map = read_test_corpus_with_random_and_contriever()\n",
    "\n",
    "    return corpus, full_to_subset_idx_map\n",
    "\n",
    "def load_search_results(args: argparse.Namespace) -> List[Tuple[List[int], List[float]]]:\n",
    "\n",
    "    search_results_path = info[args.dataset][args.split]['contriever_search_results_path']\n",
    "    retriever_search_results = read_pickle(search_results_path)\n",
    "\n",
    "    return retriever_search_results\n",
    "\n",
    "\n",
    "def get_prompt_template(args: argparse.Namespace):\n",
    "    prompt_configuration = args.dataset\n",
    "    if args.use_model_chat_template:\n",
    "        chat_task_template_str = chat_task_templates[args.llm_id]['template']\n",
    "        \n",
    "        task_instruction = task_instructions[prompt_configuration]\n",
    "\n",
    "        prompt_template = apply_chat_task_template(chat_task_template_str, task_instruction)\n",
    "    else:\n",
    "        task_template = task_templates[prompt_configuration]\n",
    "\n",
    "        prompt_template = task_template.create_prompt_template()\n",
    "\n",
    "    return prompt_template\n",
    "\n",
    "\n",
    "def initialize_dataset_and_loader(\n",
    "    args: argparse.Namespace, \n",
    "    corpus: List[Dict], \n",
    "    full_to_subset_idx_map: Optional[Dict[int, int]], \n",
    "    retriever_search_results: List[Tuple[List[int], List[float]]], \n",
    "    tokenizer: PreTrainedTokenizer\n",
    ") -> DataLoader:\n",
    "    \n",
    "    prompt_template = get_prompt_template(args)\n",
    "    \n",
    "    prompt_ds = PromptDataset(\n",
    "        corpus=corpus, data_path=info[args.dataset][args.split]['data_path'], \n",
    "        tokenizer=tokenizer, \n",
    "        max_tokenized_length=args.model_max_length - 2, \n",
    "        search_results=retriever_search_results,\n",
    "        prompt_template=prompt_template,\n",
    "        full_to_subset_idx_map=full_to_subset_idx_map,\n",
    "        do_normalize_query=True, \n",
    "        num_documents_in_context=args.num_retrieved_documents,\n",
    "        gold_position=args.gold_position, # None in these experiments\n",
    "    )\n",
    "        \n",
    "    prompt_dataloader = DataLoader(\n",
    "        prompt_ds,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=8,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    return prompt_dataloader\n",
    "\n",
    "\n",
    "def print_info(args: argparse.Namespace):\n",
    "    print(\"INFO:\")    \n",
    "    print(f\"DATA: {info[args.dataset]['test']['data_path']}\")\n",
    "    print(f\"USE TEST: {args.use_test}\")\n",
    "    print(f\"MODEL: {args.llm_id}\")\n",
    "    print(f\"MODEL MAX LENGTH: {args.model_max_length}\")\n",
    "    print(f'MAX NEW TOKENS: {args.max_new_tokens}')\n",
    "    print(f\"USE MODEL CHAT TEMPLATE: {args.use_model_chat_template}\")\n",
    "    print(f\"TASK WITH PROOF:\", args.use_task_with_proof)\n",
    "    print(f\"GOLD POSITION: {args.gold_position}\")\n",
    "    print(f\"NUM DOCUMENTS IN CONTEXT: {args.num_retrieved_documents}\")\n",
    "    print(f\"BATCH SIZE: {args.batch_size}\")\n",
    "    print(f\"SAVE EVERY: {args.save_every}\")\n",
    "\n",
    "\n",
    "def extract_generate_answers(\n",
    "    args: argparse.Namespace, \n",
    "    generated_output: List[str]\n",
    ") -> List[str]:\n",
    "    answer_prefix = \"Answer:\"\n",
    "    if args.use_model_chat_template:\n",
    "        answer_prefix = re.escape(chat_task_templates[args.llm_id]['answer_prefix'])\n",
    "\n",
    "    generated_answers = []\n",
    "    for output in generated_output:\n",
    "        matches = list(re.finditer(answer_prefix, output))\n",
    "        match_idx = 0\n",
    "\n",
    "        # When using the proof there is a one-shot example that already \n",
    "        # contains the string \"Answer:\". Thus, we should get the second (match_idx=1) match.\n",
    "        if args.use_task_with_proof:\n",
    "            match_idx = 1\n",
    "            if args.use_model_chat_template and answer_prefix != \"Answer:\":\n",
    "                match_idx = 0\n",
    " \n",
    "        answer_end = matches[match_idx].end()\n",
    "        response = output[answer_end:].strip()\n",
    "        generated_answers.append(response)\n",
    "    \n",
    "    return generated_answers\n",
    "\n",
    "\n",
    "def generate_and_save(\n",
    "    args: argparse.Namespace, \n",
    "    bgm: BGM, \n",
    "    prompt_dataloader: DataLoader\n",
    "):\n",
    "    # Info from arguments\n",
    "    llm_id = args.llm_id\n",
    "    num_doc = args.num_retrieved_documents\n",
    "    save_every = args.save_every\n",
    "    retriever_str = \"contriever\"\n",
    "    padding_str = f\"_{args.padding_strategy}{args.model_max_length}\" if args.padding_strategy != \"longest\" else \"\" \n",
    "    chat_template_str = \"_template\" if args.use_model_chat_template else \"\"\n",
    "    prompt_type = \"retrieved_proof\" if args.use_task_with_proof else \"retrieved\"\n",
    "\n",
    "    # Create the saving directory\n",
    "    llm_folder = llm_id.split(\"/\")[1] if '/' in llm_id else llm_id\n",
    "    saving_dir = f\"{args.output_dir}/{args.dataset}/{llm_folder}/{args.split}/{prompt_type}/{retriever_str}/{num_doc}_doc\"\n",
    "    os.makedirs(saving_dir, exist_ok=True)\n",
    "\n",
    "    # Path del file .json\n",
    "    json_file_path = os.path.join(saving_dir, \"generated_results_weights_epoch_25.json\")\n",
    "\n",
    "    all_info = []  \n",
    "    for idx, prompt_batch in enumerate(tqdm(prompt_dataloader)):\n",
    "        if idx == 500:\n",
    "            break\n",
    "\n",
    "        prompts = prompt_batch['prompt']\n",
    "\n",
    "        # Usa una regex per estrarre tutto a partire da \"Question\"\n",
    "        match = re.search(r\"Question:.*\", prompts, re.DOTALL)\n",
    "\n",
    "        # Controlla se c'è una corrispondenza e prendi il risultato\n",
    "        if match:\n",
    "            prompts = match.group()\n",
    "        else:\n",
    "            print(\"Nessuna corrispondenza trovata.\")\n",
    "\n",
    "        generated_output = bgm.generate(\n",
    "            prompts, \n",
    "            padding_strategy=args.padding_strategy,\n",
    "            max_new_tokens=args.max_new_tokens\n",
    "        )\n",
    "\n",
    "        # Salva i risultati in un dizionario\n",
    "        result = {\n",
    "            \"prompt\": prompts,\n",
    "            \"generated_indices\": generated_output,  \n",
    "            \"target_indices\": prompt_batch['document_indices']\n",
    "        }\n",
    "        all_info.append(result)\n",
    "        \n",
    "        #print(f\"Esempio {idx+1}\\n\")\n",
    "        #print(f\"I migliori indici secondo il modello: {generated_output}\\n\")\n",
    "        #print(f\"Gli indici target sono: {prompt_batch['document_indices']}\")\n",
    "\n",
    "    # Scrivi i risultati nel file JSON\n",
    "    with open(json_file_path, \"w\", encoding=\"utf-8\") as json_file:\n",
    "        json.dump(all_info, json_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "    print(f\"Risultati salvati in: {json_file_path}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = parse_arguments()\n",
    "\n",
    "    args.split = \"test\" if args.use_test else \"train\"\n",
    "\n",
    "    print(\"Loading LLM...\")\n",
    "    llm_id = args.llm_id\n",
    "\n",
    "    saved_model_path = r\"C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\lora_training_bgm\\lora-checkpoint\\epochs\\epoch_25\"\n",
    "    \n",
    "    bgm = BGM(\n",
    "        llm_id, device, \n",
    "        quantization_bits=args.quantization_bits, \n",
    "        model_max_length=args.model_max_length,\n",
    "        lora_weights_path=saved_model_path\n",
    "    )\n",
    "    tokenizer = bgm.tokenizer\n",
    "    print(\"LLM loaded\")\n",
    "\n",
    "\n",
    "    print(\"Loading corpus and search results...\")\n",
    "    corpus, full_to_subset_idx_map = load_corpus(args)\n",
    "    retriever_search_results = load_search_results(args)\n",
    "    print(\"Corpus and search results loaded\")\n",
    "\n",
    "\n",
    "    print(\"Loading prompt dataset...\")\n",
    "    prompt_dataloader = initialize_dataset_and_loader(\n",
    "        args, corpus, full_to_subset_idx_map, \n",
    "        retriever_search_results, tokenizer\n",
    "    )\n",
    "    print(\"Prompt dataset loaded\")\n",
    "\n",
    "    print_info(args)\n",
    "\n",
    "    for i in range(5):\n",
    "        entry = prompt_dataloader.dataset[i]\n",
    "        print(f\"{entry}\")\n",
    "\n",
    "    results=generate_and_save(args, bgm, prompt_dataloader)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    seed_everything(SEED)\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
