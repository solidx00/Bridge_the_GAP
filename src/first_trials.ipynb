{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "2\n",
      "NVIDIA GeForce RTX 3080\n",
      "NVIDIA GeForce RTX 3060\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.cuda.get_device_name(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total_examples': 2889, 'examples_with_answer_in_context': 721, 'examples_without_answer_in_context': 2168, 'correct_with_context': 539, 'correct_without_context': 87, 'average_correct_with_context': 0.7475728155339806, 'average_correct_without_context': 0.04012915129151291, 'overall_accuracy': 0.2166839736933195}\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "\n",
    "def analyze_json_responses(my_path):\n",
    "    data = read_json(my_path)  # Legge il file JSON\n",
    "\n",
    "    total_examples = len(data)  # Numero totale di esempi\n",
    "    correct_with_context = 0  # Risposte corrette con risposta nel contesto\n",
    "    correct_without_context = 0  # Risposte corrette senza risposta nel contesto\n",
    "    has_answer_in_context = 0  # Esempi con risposta nel contesto\n",
    "    no_answer_in_context = 0  # Esempi senza risposta nel contesto\n",
    "    total_correct = 0  # Totale delle risposte corrette\n",
    "\n",
    "    for entry in data:\n",
    "        ans_in_documents = entry.get(\"ans_in_documents\", False)\n",
    "        ans_match_after_norm = entry.get(\"ans_match_after_norm\", False)\n",
    "\n",
    "        if ans_in_documents:\n",
    "            has_answer_in_context += 1\n",
    "            if ans_match_after_norm:\n",
    "                correct_with_context += 1\n",
    "        else:\n",
    "            no_answer_in_context += 1\n",
    "            if ans_match_after_norm:\n",
    "                correct_without_context += 1\n",
    "\n",
    "        # Conta ogni risposta corretta\n",
    "        if ans_match_after_norm:\n",
    "            total_correct += 1\n",
    "\n",
    "    # Calcola le medie\n",
    "    avg_correct_with_context = correct_with_context / has_answer_in_context if has_answer_in_context > 0 else 0\n",
    "    avg_correct_without_context = correct_without_context / no_answer_in_context if no_answer_in_context > 0 else 0\n",
    "    overall_accuracy = total_correct / total_examples if total_examples > 0 else 0\n",
    "\n",
    "    return {\n",
    "        \"total_examples\": total_examples,\n",
    "        \"examples_with_answer_in_context\": has_answer_in_context,\n",
    "        \"examples_without_answer_in_context\": no_answer_in_context,\n",
    "        \"correct_with_context\": correct_with_context,\n",
    "        \"correct_without_context\": correct_without_context,\n",
    "        \"average_correct_with_context\": avg_correct_with_context,\n",
    "        \"average_correct_without_context\": avg_correct_without_context,\n",
    "        \"overall_accuracy\": overall_accuracy\n",
    "    }\n",
    "\n",
    "\n",
    "# Esempio di utilizzo\n",
    "path = r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\gen_res_example_llm\\nq\\gemma-2-2b-it\\test\\retrieved\\contriever\\1_doc\\numdoc1_retr1_template_info_all_extended.json'\n",
    "result = analyze_json_responses(path)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LLM...\n",
      "LLM loaded\n",
      "Loading corpus and search results...\n",
      "Corpus and search results loaded\n",
      "Loading prompt dataset...\n",
      "Prompt dataset loaded\n",
      "INFO:\n",
      "DATA: C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\10k_train_dataset.json\n",
      "USE TEST: False\n",
      "MODEL: google/flan-t5-large\n",
      "MODEL MAX LENGTH: 4096\n",
      "MAX NEW TOKENS: 50\n",
      "USE MODEL CHAT TEMPLATE: False\n",
      "TASK WITH PROOF: False\n",
      "GOLD POSITION: None\n",
      "NUM DOCUMENTS IN CONTEXT: 3\n",
      "BATCH SIZE: None\n",
      "SAVE EVERY: 250\n",
      "Saving DataLoader contents to JSON...\n",
      "DataLoader contents saved to C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\dataloader_contents.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from typing import Tuple, Dict, Optional\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "from utils import *\n",
    "from bgm import BGM\n",
    "from default_prompts import *\n",
    "from prompt_dataset import PromptDataset\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "device = torch.device(f\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "warnings.filterwarnings('ignore')\n",
    "SEED=10\n",
    "\n",
    "info = {\n",
    "    \"nq_bgm\": {\n",
    "        \"train\": {\n",
    "            \"data_path\": r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\10k_train_dataset.json',\n",
    "            \"contriever_search_results_path\": r\"C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\processed\\contriever_search_results_at150.pkl\",\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "def save_dataloader_to_json(dataloader, output_file, num_examples=15):\n",
    "    all_batches = []\n",
    "\n",
    "    print(\"Saving DataLoader contents to JSON...\")\n",
    "    for idx, batch in enumerate(dataloader):\n",
    "        if idx >= num_examples:  # Stop after saving the specified number of examples\n",
    "            break\n",
    "\n",
    "        batch_dict = {}\n",
    "        for key, value in batch.items():\n",
    "            # Convert tensors to lists for JSON serialization\n",
    "            if isinstance(value, torch.Tensor):\n",
    "                batch_dict[key] = value.tolist()\n",
    "            else:\n",
    "                batch_dict[key] = value\n",
    "        all_batches.append(batch_dict)\n",
    "    \n",
    "    # Save the entire list of dictionaries to a JSON file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_batches, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"DataLoader contents saved to {output_file}\")\n",
    "\n",
    "class DotDict:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "def parse_arguments(custom_args=None):\n",
    "    \"\"\"\n",
    "    Mimics argparse to parse arguments for LLM generation. Accepts custom arguments as a dictionary for notebooks.\n",
    "    \"\"\"\n",
    "    # Define default values\n",
    "    default_args = {\n",
    "        'output_dir': r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\gen_id_document_bgm',\n",
    "        'llm_id': 'google/flan-t5-large',\n",
    "        'dataset': 'nq_bgm',\n",
    "        'model_max_length': 4096,\n",
    "        'quantization_bits': 4,\n",
    "        'use_model_chat_template': False, \n",
    "        'gold_position': None,\n",
    "        'num_retrieved_documents': 5,\n",
    "        'use_test': False,\n",
    "        'max_new_tokens': 50,\n",
    "        'use_task_with_proof': False,\n",
    "        'batch_size': None,\n",
    "        'save_every': 250,\n",
    "    }\n",
    "\n",
    "    # If custom_args is provided, update defaults\n",
    "    if custom_args:\n",
    "        default_args.update(custom_args)\n",
    "\n",
    "    # Perform validation\n",
    "    if default_args['num_retrieved_documents'] is None:\n",
    "        raise ValueError(\"'num_retrieved_documents' must be specified.\")\n",
    "    if default_args['num_retrieved_documents'] <= 0:\n",
    "        raise ValueError(\"'num_retrieved_documents' must be a positive integer.\")\n",
    "    if default_args['gold_position'] is not None:\n",
    "        if (default_args['gold_position'] < 0 or \n",
    "            default_args['gold_position'] >= default_args['num_retrieved_documents']):\n",
    "            raise ValueError(\"'gold_position' must be within the range of 'num_retrieved_documents'.\")\n",
    "\n",
    "    return DotDict(**default_args)\n",
    "\n",
    "\n",
    "def load_corpus(\n",
    "    args: argparse.Namespace\n",
    ") -> Tuple[List[Dict], Optional[Dict[int, int]]]:\n",
    "    \n",
    "    # Corpus with documents from Contriever\n",
    "    corpus, full_to_subset_idx_map = read_corpus_with_contriever()\n",
    "\n",
    "    return corpus, full_to_subset_idx_map\n",
    "\n",
    "def load_search_results(args: argparse.Namespace) -> List[Tuple[List[int], List[float]]]:\n",
    "\n",
    "    search_results_path = info[args.dataset][args.split]['contriever_search_results_path']\n",
    "    retriever_search_results = read_pickle(search_results_path)\n",
    "\n",
    "    return retriever_search_results\n",
    "\n",
    "\n",
    "def get_prompt_template(args: argparse.Namespace):\n",
    "    prompt_configuration = args.dataset\n",
    "    if args.use_model_chat_template:\n",
    "        chat_task_template_str = chat_task_templates[args.llm_id]['template']\n",
    "        \n",
    "        task_instruction = task_instructions[prompt_configuration]\n",
    "\n",
    "        prompt_template = apply_chat_task_template(chat_task_template_str, task_instruction)\n",
    "    else:\n",
    "        task_template = task_templates[prompt_configuration]\n",
    "\n",
    "        prompt_template = task_template.create_prompt_template()\n",
    "\n",
    "    return prompt_template\n",
    "\n",
    "\n",
    "def initialize_dataset_and_loader(\n",
    "    args: argparse.Namespace, \n",
    "    corpus: List[Dict], \n",
    "    full_to_subset_idx_map: Optional[Dict[int, int]], \n",
    "    retriever_search_results: List[Tuple[List[int], List[float]]], \n",
    "    tokenizer: PreTrainedTokenizer\n",
    ") -> DataLoader:\n",
    "    \n",
    "    prompt_template = get_prompt_template(args)\n",
    "    \n",
    "    prompt_ds = PromptDataset(\n",
    "        corpus=corpus, data_path=info[args.dataset][args.split]['data_path'], \n",
    "        tokenizer=tokenizer, \n",
    "        max_tokenized_length=args.model_max_length - 2, \n",
    "        search_results=retriever_search_results,\n",
    "        prompt_template=prompt_template,\n",
    "        full_to_subset_idx_map=full_to_subset_idx_map,\n",
    "        do_normalize_query=True, \n",
    "        num_documents_in_context=args.num_retrieved_documents,\n",
    "        gold_position=args.gold_position, # None in these experiments\n",
    "    )\n",
    "        \n",
    "    prompt_dataloader = DataLoader(\n",
    "        prompt_ds,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=8,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    return prompt_dataloader\n",
    "\n",
    "\n",
    "def print_info(args: argparse.Namespace):\n",
    "    print(\"INFO:\")    \n",
    "    print(f\"DATA: {info[args.dataset][args.split]['data_path']}\")\n",
    "    print(f\"USE TEST: {args.use_test}\")\n",
    "    print(f\"MODEL: {args.llm_id}\")\n",
    "    print(f\"MODEL MAX LENGTH: {args.model_max_length}\")\n",
    "    print(f'MAX NEW TOKENS: {args.max_new_tokens}')\n",
    "    print(f\"USE MODEL CHAT TEMPLATE: {args.use_model_chat_template}\")\n",
    "    print(f\"TASK WITH PROOF:\", args.use_task_with_proof)\n",
    "    print(f\"GOLD POSITION: {args.gold_position}\")\n",
    "    print(f\"NUM DOCUMENTS IN CONTEXT: {args.num_retrieved_documents}\")\n",
    "    print(f\"BATCH SIZE: {args.batch_size}\")\n",
    "    print(f\"SAVE EVERY: {args.save_every}\")\n",
    "\n",
    "\n",
    "def extract_generate_answers(\n",
    "    args: argparse.Namespace, \n",
    "    generated_output: List[str]\n",
    ") -> List[str]:\n",
    "    answer_prefix = \"Answer:\"\n",
    "    if args.use_model_chat_template:\n",
    "        answer_prefix = re.escape(chat_task_templates[args.llm_id]['answer_prefix'])\n",
    "\n",
    "    generated_answers = []\n",
    "    for output in generated_output:\n",
    "        matches = list(re.finditer(answer_prefix, output))\n",
    "        match_idx = 0\n",
    "\n",
    "        # When using the proof there is a one-shot example that already \n",
    "        # contains the string \"Answer:\". Thus, we should get the second (match_idx=1) match.\n",
    "        if args.use_model_chat_template and answer_prefix != \"Answer:\":\n",
    "            match_idx = 0\n",
    " \n",
    "        answer_end = matches[match_idx].end()\n",
    "        response = output[answer_end:].strip()\n",
    "        generated_answers.append(response)\n",
    "    \n",
    "    return generated_answers\n",
    "\n",
    "\n",
    "def BGMTraining(\n",
    "    args: argparse.Namespace, \n",
    "    prompt_ds: PromptDataset,\n",
    "    llm: BGM, \n",
    "    prompt_dataloader: DataLoader\n",
    "):\n",
    "    # Info from arguments\n",
    "    llm_id = args.llm_id\n",
    "    num_doc = args.num_retrieved_documents\n",
    "    save_every = args.save_every\n",
    "    retriever_str = \"contriever\" \n",
    "    chat_template_str = \"_template\" if args.use_model_chat_template else \"\"\n",
    "    prompt_type = \"retrieved_proof\" if args.use_task_with_proof else \"retrieved\"\n",
    "\n",
    "    # Create the saving directory\n",
    "    llm_folder = llm_id.split(\"/\")[1] if '/' in llm_id else llm_id\n",
    "    saving_dir = f\"{args.output_dir}/{args.dataset}/{llm_folder}/{args.split}/{prompt_type}/{retriever_str}/{num_doc}_doc\"\n",
    "    os.makedirs(saving_dir, exist_ok=True)\n",
    "\n",
    "    all_info = []  \n",
    "    for idx, prompt_batch in enumerate(tqdm(prompt_dataloader)):\n",
    "        prompts = prompt_batch['prompt']\n",
    "        example_id = prompt_batch['example_id']\n",
    "        prompts = prompt_batch['prompt']\n",
    "        query = prompt_batch['query']\n",
    "        document_indices=prompt_batch['document_indices']\n",
    "        \n",
    "        for doc_idx in document_indices:\n",
    "            \n",
    "            candidate_docs += doc_idx\n",
    "\n",
    "            formatted_docs, _ = prompt_ds._get_documents_from_indices(candidate_docs)\n",
    "\n",
    "            if '\\nAnswer:' not in candidate_prompt:\n",
    "                candidate_prompt += '\\nAnswer:'\n",
    "\n",
    "            \n",
    "\n",
    "        generated_output = llm.generate(\n",
    "            prompts, \n",
    "            max_new_tokens=args.max_new_tokens\n",
    "        )\n",
    "\n",
    "        generated_answers = extract_generate_answers(args, generated_output)\n",
    "        prompt_batch['generated_answer'] = generated_answers\n",
    "        \n",
    "        all_info.append(prompt_batch)\n",
    "        '''\n",
    "        if (idx + 1) % save_every == 0 or (idx + 1) == len(prompt_dataloader):\n",
    "            print(f\"Saving at {idx + 1}...\")\n",
    "            file_name = f\"{saving_dir}/numdoc{num_doc}_retr{args.num_retrieved_documents}{chat_template_str}_info_{idx+1}.pkl\"\n",
    "            write_pickle(all_info, file_name)\n",
    "            all_info = []\n",
    "        '''\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = parse_arguments()\n",
    "\n",
    "    args.split = \"test\" if args.use_test else \"train\"\n",
    "\n",
    "    print(\"Loading LLM...\")\n",
    "    llm_id = args.llm_id\n",
    "    bgm = BGM(\n",
    "        llm_id, device,  \n",
    "        model_max_length=args.model_max_length\n",
    "    )\n",
    "    tokenizer = bgm.tokenizer\n",
    "    print(\"LLM loaded\")\n",
    "\n",
    "\n",
    "    print(\"Loading corpus and search results...\")\n",
    "    corpus, full_to_subset_idx_map = load_corpus(args)\n",
    "    retriever_search_results = load_search_results(args)\n",
    "    print(\"Corpus and search results loaded\")\n",
    "\n",
    "\n",
    "    print(\"Loading prompt dataset...\")\n",
    "    prompt_ds, prompt_dataloader = initialize_dataset_and_loader(\n",
    "        args, corpus, full_to_subset_idx_map, \n",
    "        retriever_search_results, tokenizer\n",
    "    )\n",
    "    print(\"Prompt dataset loaded\")\n",
    "\n",
    "    print_info(args)\n",
    "\n",
    "    #output_json_path = r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\dataloader_contents.json'\n",
    "    #save_dataloader_to_json(prompt_dataloader, output_json_path, num_examples=15)\n",
    "        \n",
    "    BGMTraining(args, prompt_ds, bgm, prompt_dataloader)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    seed_everything(SEED)\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File aggiornato salvato in: C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\gen_ids_document_training_set_bgm\\nq_training\\gemma-2-2b-it\\train\\retrieved\\contriever\\5_doc\\numdoc5_retr5_template_info_all_extended_updated.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def match_example_ids(file1_path, file2_path, output_path):\n",
    "    \"\"\"\n",
    "    Modifica il file1 aggiungendo l'example_id da file2 quando query e question corrispondono.\n",
    "\n",
    "    Args:\n",
    "        file1_path (str): Percorso al file JSON di input 1.\n",
    "        file2_path (str): Percorso al file JSON di input 2.\n",
    "        output_path (str): Percorso al file JSON di output aggiornato.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Caricamento dei file JSON\n",
    "        with open(file1_path, 'r') as f1:\n",
    "            file1 = json.load(f1)\n",
    "\n",
    "        with open(file2_path, 'r') as f2:\n",
    "            file2 = json.load(f2)\n",
    "\n",
    "        # Creazione di un dizionario per mappare le domande agli example_id\n",
    "        question_to_example_id = {item['question']: item['example_id'] for item in file2}\n",
    "\n",
    "        # Modifica del primo file\n",
    "        for entry in file1:\n",
    "            query = entry.get('query')\n",
    "            if query in question_to_example_id:\n",
    "                entry['example_id'] = question_to_example_id[query]\n",
    "\n",
    "        # Salvataggio del file aggiornato\n",
    "        with open(output_path, 'w') as f1_updated:\n",
    "            json.dump(file1, f1_updated, indent=4)\n",
    "\n",
    "        print(f\"File aggiornato salvato in: {output_path}\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Errore: {e}\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Errore nel parsing del file JSON: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Errore imprevisto: {e}\")\n",
    "\n",
    "def update_queries_with_document_indices(file1_path, file2_path, output_path):\n",
    "    # Carica i dati dai file JSON\n",
    "    with open(file1_path, 'r', encoding='utf-8') as f1, open(file2_path, 'r', encoding='utf-8') as f2:\n",
    "        file1_data = json.load(f1)\n",
    "        file2_data = json.load(f2)\n",
    "\n",
    "    # Crea un dizionario per mappare le query ai document_indices di File 2\n",
    "    query_to_indices = {\n",
    "        entry['query']: entry.get('document_indices', [])\n",
    "        for entry in file2_data\n",
    "    }\n",
    "\n",
    "    # Aggiorna File 1 aggiungendo i document_indices associati alle query\n",
    "    for entry in file1_data:\n",
    "        query = entry['query']\n",
    "        if query in query_to_indices:\n",
    "            entry['document_indices'] = query_to_indices[query]\n",
    "\n",
    "    # Salva il risultato in un nuovo file JSON\n",
    "    with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "        json.dump(file1_data, output_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "\n",
    "path_output=r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\gen_ids_document_training_set_bgm\\nq_training\\gemma-2-2b-it\\train\\retrieved\\contriever\\5_doc\\numdoc5_retr5_template_info_all_extended_updated.json'\n",
    "file_da_modificare = r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\gen_ids_document_training_set_bgm\\nq_training\\gemma-2-2b-it\\train\\retrieved\\contriever\\5_doc\\numdoc5_retr5_template_info_all_extended.json'\n",
    "file_di_confronto = r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\10k_train_dataset.json'\n",
    "\n",
    "match_example_ids(file_da_modificare, file_di_confronto, path_output)\n",
    "\n",
    "update_queries_with_document_indices(r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\gen_ids_document_training_set_bgm\\nq_training\\gemma-2-2b-it\\train\\retrieved\\contriever\\5_doc\\numdoc5_retr5_template_info_all_extended_updated.json', r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\gen_ids_document_training_set_bgm\\nq_training\\gemma-2-2b-it\\train\\retrieved\\contriever\\5_doc\\numdoc5_retr5_template_info_all_extended_updated.json', r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\gen_ids_document_training_set_bgm\\nq_training\\gemma-2-2b-it\\train\\retrieved\\contriever\\5_doc\\numdoc5_retr5_template_info_all_extended_updated_last.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "# Percentuali per ogni caso\n",
    "percentages = {\n",
    "    \"case_1_single_doc\": 0.1,\n",
    "    \"case_2_multiple_docs\": 0.2,\n",
    "    \"case_3_no_docs\": 0.1,\n",
    "    \"case_4_less_docs\": 0.4,\n",
    "    \"case_5_reranking\": 0.2,\n",
    "}\n",
    "\n",
    "# Task instruction da aggiungere a ogni query\n",
    "task_instruction = \"Output only the document IDs relevant to the query. Use this format: [ID1, ID2, ...].\"\n",
    "\n",
    "def process_data(input_file, output_file):\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        examples = json.load(f)\n",
    "\n",
    "    dataset = []\n",
    "    \n",
    "    # Shuffle examples to ensure random sampling\n",
    "    random.shuffle(examples)\n",
    "\n",
    "    # Total examples to be processed for each case\n",
    "    total_examples = len(examples)\n",
    "    case_limits = {case: int(total_examples * perc) for case, perc in percentages.items()}\n",
    "    case_counters = {case: 0 for case in percentages}\n",
    "\n",
    "    for example in examples:\n",
    "        if all(count >= case_limits[case] for case, count in case_counters.items()):\n",
    "            break  # Stop if all case limits are met\n",
    "        \n",
    "        query = f\"Task Instruction: {task_instruction}\\nQuestion:{example['query']}\"  # Aggiunge la task instruction\n",
    "        retrieved_docs = example[\"document_indices\"]\n",
    "        selected_docs = example[\"selected_documents\"]\n",
    "        are_answer = example[\"are_answer\"]\n",
    "\n",
    "        # Case 1: Single document correct answer\n",
    "        if are_answer and len(selected_docs) == 1 and case_counters[\"case_1_single_doc\"] < case_limits[\"case_1_single_doc\"]:\n",
    "            dataset.append({\n",
    "                \"input\": {\n",
    "                    \"query\": query,\n",
    "                    \"retrieved_docs\": retrieved_docs,\n",
    "                },\n",
    "                \"output\": selected_docs,\n",
    "            })\n",
    "            case_counters[\"case_1_single_doc\"] += 1\n",
    "\n",
    "        # Case 2: Multiple documents correct answer\n",
    "        elif are_answer and len(selected_docs) > 1 and case_counters[\"case_2_multiple_docs\"] < case_limits[\"case_2_multiple_docs\"]:\n",
    "            dataset.append({\n",
    "                \"input\": {\n",
    "                    \"query\": query,\n",
    "                    \"retrieved_docs\": retrieved_docs,\n",
    "                },\n",
    "                \"output\": selected_docs,\n",
    "            })\n",
    "            case_counters[\"case_2_multiple_docs\"] += 1\n",
    "\n",
    "        # Case 3: No documents correct answer\n",
    "        elif are_answer and len(selected_docs) == 0 and case_counters[\"case_3_no_docs\"] < case_limits[\"case_3_no_docs\"]:\n",
    "            dataset.append({\n",
    "                \"input\": {\n",
    "                    \"query\": query,\n",
    "                    \"retrieved_docs\": [],\n",
    "                },\n",
    "                \"output\": [],\n",
    "            })\n",
    "            case_counters[\"case_3_no_docs\"] += 1\n",
    "\n",
    "        # Case 4: Input and output unchanged\n",
    "        elif are_answer and len(selected_docs) > 2 and case_counters[\"case_4_less_docs\"] < case_limits[\"case_4_less_docs\"]:\n",
    "            dataset.append({\n",
    "                \"input\": {\n",
    "                    \"query\": query,\n",
    "                    \"retrieved_docs\": selected_docs,\n",
    "                },\n",
    "                \"output\": selected_docs,\n",
    "            })\n",
    "            case_counters[\"case_4_less_docs\"] += 1\n",
    "\n",
    "        # Case 5: Reranking\n",
    "        elif are_answer and len(selected_docs) > 2 and case_counters[\"case_5_reranking\"] < case_limits[\"case_5_reranking\"]:\n",
    "            reranked_docs = random.sample(selected_docs, len(selected_docs))  # Randomize order\n",
    "            dataset.append({\n",
    "                \"input\": {\n",
    "                    \"query\": query,\n",
    "                    \"retrieved_docs\": reranked_docs,\n",
    "                },\n",
    "                \"output\": selected_docs,\n",
    "            })\n",
    "            case_counters[\"case_5_reranking\"] += 1\n",
    "\n",
    "    # Save the dataset to a file\n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(dataset, f, indent=4)\n",
    "\n",
    "# Path to input and output files\n",
    "input_file = r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\gen_ids_document_training_set_bgm\\nq_training\\gemma-2-2b-it\\train\\retrieved\\contriever\\5_doc\\numdoc5_retr5_info_all_extended_training_set.json'\n",
    "output_file = r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\training_dataset.json'\n",
    "\n",
    "process_data(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Totale esempi nel file di input: 3000\n",
      "Esempi con 'are_answer=True': 1233\n",
      "Esempi con 'selected_documents == 0': 366\n",
      "Esempi con 'selected_documents == 1': 736\n",
      "Esempi con 'selected_documents > 1': 131\n",
      "Distribuzione pianificata degli esempi nel dataset creato:\n",
      "case_1_single_doc: 51\n",
      "case_2_multiple_docs: 52\n",
      "case_3_no_docs: 36\n",
      "case_4_multi_doc_unchanged: 45\n",
      "case_5_reranking: 65\n",
      "case_6_single_doc_unchanged: 36\n",
      "Esempi effettivamente inclusi nel dataset creato:\n",
      "case_1_single_doc: 51\n",
      "case_2_multiple_docs: 52\n",
      "case_3_no_docs: 36\n",
      "case_4_multi_doc_unchanged: 45\n",
      "case_5_reranking: 34\n",
      "case_6_single_doc_unchanged: 36\n",
      "Totale degli Esempi inclusi nel training dataset creato: 254\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "# Percentuali per ogni caso\n",
    "percentages = {\n",
    "    \"case_1_single_doc\": 0.07,\n",
    "    \"case_2_multiple_docs\": 0.4,\n",
    "    \"case_3_no_docs\": 0.1,\n",
    "    \"case_4_multi_doc_unchanged\": 0.35,\n",
    "    \"case_5_reranking\": 0.5,\n",
    "    \"case_6_single_doc_unchanged\": 0.05,\n",
    "}\n",
    "\n",
    "# Task instruction da aggiungere a ogni query\n",
    "task_instruction = \"Output only the document IDs relevant to the query. Use this format: [ID1, ID2, ...].\"\n",
    "\n",
    "def process_data(input_file, output_file):\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        examples = json.load(f)\n",
    "\n",
    "    # Filtra gli esempi con are_answer = true\n",
    "    valid_examples = [ex for ex in examples if ex[\"are_answer\"] is True]\n",
    "\n",
    "    print(f\"Totale esempi nel file di input: {len(examples)}\")\n",
    "    print(f\"Esempi con 'are_answer=True': {len(valid_examples)}\")\n",
    "\n",
    "    # Raggruppa per numero di selected_documents\n",
    "    grouped_examples = {\n",
    "        \"len_0\": [ex for ex in valid_examples if len(ex[\"selected_documents\"]) == 0],\n",
    "        \"len_1\": [ex for ex in valid_examples if len(ex[\"selected_documents\"]) == 1],\n",
    "        \"len_gt_1\": [ex for ex in valid_examples if len(ex[\"selected_documents\"]) > 1],\n",
    "    }\n",
    "\n",
    "    print(f\"Esempi con 'selected_documents == 0': {len(grouped_examples['len_0'])}\")\n",
    "    print(f\"Esempi con 'selected_documents == 1': {len(grouped_examples['len_1'])}\")\n",
    "    print(f\"Esempi con 'selected_documents > 1': {len(grouped_examples['len_gt_1'])}\")\n",
    "\n",
    "    # Calcola le suddivisioni per ogni gruppo\n",
    "    group_case_limits = {\n",
    "        \"case_1_single_doc\": int(len(grouped_examples[\"len_1\"]) * percentages[\"case_1_single_doc\"]),\n",
    "        \"case_2_multiple_docs\": int(len(grouped_examples[\"len_gt_1\"]) * percentages[\"case_2_multiple_docs\"]),\n",
    "        \"case_3_no_docs\": int(len(grouped_examples[\"len_0\"]) * percentages[\"case_3_no_docs\"]),\n",
    "        \"case_4_multi_doc_unchanged\": int(len(grouped_examples[\"len_gt_1\"]) * percentages[\"case_4_multi_doc_unchanged\"]),\n",
    "        \"case_5_reranking\": int(len(grouped_examples[\"len_gt_1\"]) * percentages[\"case_5_reranking\"]),\n",
    "        \"case_6_single_doc_unchanged\": int(len(grouped_examples[\"len_1\"]) * percentages[\"case_6_single_doc_unchanged\"]),\n",
    "    }\n",
    "\n",
    "    print(\"Distribuzione pianificata degli esempi nel dataset creato:\")\n",
    "    for case, limit in group_case_limits.items():\n",
    "        print(f\"{case}: {limit}\")\n",
    "\n",
    "    dataset = []\n",
    "    case_counters = {case: 0 for case in group_case_limits}\n",
    "\n",
    "    # Processa gli esempi\n",
    "    for example in valid_examples:\n",
    "        query = f\"Task Instruction: {task_instruction}\\nQuestion:{example['query']}\"\n",
    "        retrieved_docs = example[\"document_indices\"]\n",
    "        selected_docs = example[\"selected_documents\"]\n",
    "\n",
    "        # Case 1: Single document correct answer\n",
    "        if len(selected_docs) == 1 and case_counters[\"case_1_single_doc\"] < group_case_limits[\"case_1_single_doc\"]:\n",
    "            dataset.append({\n",
    "                \"input\": {\n",
    "                    \"query\": query,\n",
    "                    \"retrieved_docs\": retrieved_docs,\n",
    "                },\n",
    "                \"output\": selected_docs,\n",
    "            })\n",
    "            case_counters[\"case_1_single_doc\"] += 1\n",
    "\n",
    "        # Case 2: Multiple documents correct answer\n",
    "        elif len(selected_docs) > 1 and case_counters[\"case_2_multiple_docs\"] < group_case_limits[\"case_2_multiple_docs\"]:\n",
    "            dataset.append({\n",
    "                \"input\": {\n",
    "                    \"query\": query,\n",
    "                    \"retrieved_docs\": retrieved_docs,\n",
    "                },\n",
    "                \"output\": selected_docs,\n",
    "            })\n",
    "            case_counters[\"case_2_multiple_docs\"] += 1\n",
    "\n",
    "        # Case 3: No documents correct answer\n",
    "        elif len(selected_docs) == 0 and case_counters[\"case_3_no_docs\"] < group_case_limits[\"case_3_no_docs\"]:\n",
    "            dataset.append({\n",
    "                \"input\": {\n",
    "                    \"query\": query,\n",
    "                    \"retrieved_docs\": [],\n",
    "                },\n",
    "                \"output\": [],\n",
    "            })\n",
    "            case_counters[\"case_3_no_docs\"] += 1\n",
    "\n",
    "        # Case 4: Input and output unchanged for multiple docs\n",
    "        elif len(selected_docs) > 1 and case_counters[\"case_4_multi_doc_unchanged\"] < group_case_limits[\"case_4_multi_doc_unchanged\"]:\n",
    "            dataset.append({\n",
    "                \"input\": {\n",
    "                    \"query\": query,\n",
    "                    \"retrieved_docs\": selected_docs,\n",
    "                },\n",
    "                \"output\": selected_docs,\n",
    "            })\n",
    "            case_counters[\"case_4_multi_doc_unchanged\"] += 1\n",
    "\n",
    "        # Case 6: Input and output unchanged for single doc\n",
    "        elif len(selected_docs) == 1 and case_counters[\"case_6_single_doc_unchanged\"] < group_case_limits[\"case_6_single_doc_unchanged\"]:\n",
    "            dataset.append({\n",
    "                \"input\": {\n",
    "                    \"query\": query,\n",
    "                    \"retrieved_docs\": selected_docs,\n",
    "                },\n",
    "                \"output\": selected_docs,\n",
    "            })\n",
    "            case_counters[\"case_6_single_doc_unchanged\"] += 1\n",
    "\n",
    "        # Case 5: Reranking\n",
    "        elif len(selected_docs) > 1 and case_counters[\"case_5_reranking\"] < group_case_limits[\"case_5_reranking\"]:\n",
    "            reranked_docs = selected_docs[:]\n",
    "            while reranked_docs == selected_docs:  # Garantisce che l'ordine sia diverso\n",
    "                reranked_docs = random.sample(selected_docs, len(selected_docs))\n",
    "            dataset.append({\n",
    "                \"input\": {\n",
    "                    \"query\": query,\n",
    "                    \"retrieved_docs\": reranked_docs,\n",
    "                },\n",
    "                \"output\": selected_docs,\n",
    "            })\n",
    "            case_counters[\"case_5_reranking\"] += 1\n",
    "\n",
    "    print(\"Esempi effettivamente inclusi nel dataset creato:\")\n",
    "    tot=0\n",
    "    for case, count in case_counters.items():\n",
    "        tot += count\n",
    "        print(f\"{case}: {count}\")\n",
    "\n",
    "    print(f\"Totale degli Esempi inclusi nel training dataset creato: {tot}\")\n",
    "    \n",
    "\n",
    "    # Save the dataset to a file\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(dataset, f, indent=4)\n",
    "\n",
    "# Path to input and output files\n",
    "input_file = r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\gen_ids_document_training_set_bgm\\nq_training\\gemma-2-2b-it\\train\\retrieved\\contriever\\5_doc\\numdoc5_retr5_info_all_extended_training_set.json'\n",
    "output_file = r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\training_dataset.json'\n",
    "\n",
    "process_data(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LLM...\n",
      "LoRA weights loaded from: C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\lora_training_bgm\\lora-checkpoint\\epochs\\epoch_25\n",
      "LLM loaded\n",
      "Loading corpus and search results...\n",
      "Corpus and search results loaded\n",
      "Loading prompt dataset...\n",
      "Prompt dataset loaded\n",
      "INFO:\n",
      "DATA: C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\test_dataset.json\n",
      "USE TEST: True\n",
      "MODEL: google-t5/t5-base\n",
      "MODEL MAX LENGTH: 4096\n",
      "MAX NEW TOKENS: 15\n",
      "USE MODEL CHAT TEMPLATE: False\n",
      "TASK WITH PROOF: False\n",
      "GOLD POSITION: None\n",
      "NUM DOCUMENTS IN CONTEXT: 5\n",
      "BATCH SIZE: None\n",
      "SAVE EVERY: 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/2889 [00:54<2:49:06,  3.52s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'ID generato è vuoto o contiene solo spazi. Controllo tutti i documenti...\n",
      "Nessuna risposta trovata in nessun documento.\n",
      "L'ID generato 'Id_5' non è valido.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 8/2889 [00:54<1:38:11,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'ID generato 'Version 9' non è valido.\n",
      "L'ID generato è vuoto o contiene solo spazi. Controllo tutti i documenti...\n",
      "Risposta trovata nel documento ID titles. In 2014 the Little Princess Ballet Academy (LPBA) performed the entire Swan Lake in Second Life. The adaption follows the original, but some parts like the pas de deux were not possible to perform in Second Life and has been changed. All parts are played by individual avatars. Audio Video Swan Lake Swan Lake ( \"\"), Op. 20, is a ballet composed by Pyotr Ilyich Tchaikovsky in 1875–76. Despite its initial failure, it is now one of the most popular of all ballets. The scenario, initially in two acts, was fashioned from Russian and/or German folk tales and tells.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 12/2889 [00:54<42:09,  1.14it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'ID generato è vuoto o contiene solo spazi. Controllo tutti i documenti...\n",
      "Nessuna risposta trovata in nessun documento.\n",
      "L'ID generato 'Barry Parker' non è valido.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 13/2889 [00:54<35:23,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'ID generato 'the PAX6' non è valido.\n",
      "L'ID generato è vuoto o contiene solo spazi. Controllo tutti i documenti...\n",
      "Risposta trovata nel documento ID additional land grants, this time to former soldiers, which included parts of Oak Island. It wasn't until July 6, 1818 that the original lot owners' names were mapped for the Nova Scotia Crown Lands office. Over the next 140 years or so, the island was owned by various treasure hunters who sought a legendary treasure buried somewhere on Oak Island (See section below). The hunt for treasure got so extensive that in 1965 a causeway was built from the western end of the island to Crandall's Point on the mainland, two hundred metres away in order to bring heavy machinery.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 16/2889 [00:55<20:52,  2.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'ID generato 'Id_3' non è valido.\n",
      "L'ID generato 'olivine' non è valido.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 17/2889 [00:55<17:59,  2.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'ID generato 'Id_3' non è valido.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 20/2889 [00:55<11:44,  4.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'ID generato 'the posterior' non è valido.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 22/2889 [00:55<07:42,  6.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'ID generato è vuoto o contiene solo spazi. Controllo tutti i documenti...\n",
      "Risposta trovata nel documento ID as cardiothoracic surgeon Erica Hahn, whose storylines include the rivalry with Preston Burke, her arrival to perform surgery of George O'Malley's dying father, and Richard Webber's decision to hire her in the hospital. Chyler Leigh portrayed Meredith's half-sister, Lexie Grey, who is accepted into the hospital's internship program after her mother's sudden death. Kate Burton appeared as Meredith Grey's mother, Ellis Grey, a renowned surgeon suffering from Alzheimer's disease, who ultimately dies following a heart attack. Veterinary physician Finn Dandrige was portrayed by Chris O'Donnell and appeared in the first four episodes of the season to resume the storyline of.\n",
      "L'ID generato è vuoto o contiene solo spazi. Controllo tutti i documenti...\n",
      "Risposta trovata nel documento ID As a result, in 1603, King James I approved an Act of Parliament banning the practice by which \"the Subjects of this Realm have been of late years abused &c. to the Value of £20,000 yearly, besides the Danger of their Healths\". Hop cultivation was begun in the present-day United States in 1629 by English and Dutch farmers. Before prohibition, cultivation was mainly centred around New York, California, Oregon, and Washington state. Problems with powdery mildew and downy mildew devastated New York's production by the 1920s, and California only produces hops on a small scale. Hop bars were used before.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 26/2889 [00:56<06:55,  6.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'ID generato è vuoto o contiene solo spazi. Controllo tutti i documenti...\n",
      "Risposta trovata nel documento ID had the highest quality pearls. The boundaries of South Asia vary based on how the region is defined. South Asia's northern, eastern, and western boundaries vary based on definitions used, while the Indian Ocean is the southern periphery. Most of this region rests on the Indian Plate and is isolated from the rest of Asia by mountain barriers. Much of the region consists of a peninsula in south-central Asia, rather resembling a diamond which is delineated by the Himalayas on the north, the Hindu Kush in the west, and the Arakanese in the east, and which extends southward into the\n",
      "Answer:.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 28/2889 [00:56<07:19,  6.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'ID generato 'City of Manchester' non è valido.\n",
      "L'ID generato 'Id_1' non è valido.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 31/2889 [00:57<07:04,  6.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'ID generato 'One' non è valido.\n",
      "L'ID generato 'Id_2' non è valido.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 33/2889 [00:57<06:56,  6.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'ID generato 'Id_2' non è valido.\n",
      "L'ID generato '28%' non è valido.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 34/2889 [00:57<09:49,  4.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'ID generato 'I am not in a box of any description' non è valido.\n",
      "L'ID generato è vuoto o contiene solo spazi. Controllo tutti i documenti...\n",
      "Risposta trovata nel documento ID Louis Rams. That year, the tease was followed by the show open produced by Los Angeles-based The Syndicate called \"Transformation\". It features computer-generated imagery showing a city being transformed into a football stadium and passers-by on the street turning into players, coaches, fans and officials set to an updated orchestral treatment of the \"Heavy Action\" theme song. The sequence began every week with a different celebrity walking down the street, picking up a glowing football helmet with the ESPN logo on the side and saying, \"I'm ready for some football! Are you?\", thus beginning the transformation process. Celebrities for 2006.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 39/2889 [00:58<07:22,  6.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'ID generato 'ITV' non è valido.\n",
      "L'ID generato è vuoto o contiene solo spazi. Controllo tutti i documenti...\n",
      "Nessuna risposta trovata in nessun documento.\n",
      "L'ID generato 'Ivan' non è valido.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 43/2889 [00:59<06:50,  6.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'ID generato 'the Parliament' non è valido.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 45/2889 [00:59<05:52,  8.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'ID generato è vuoto o contiene solo spazi. Controllo tutti i documenti...\n",
      "Risposta trovata nel documento ID layer, known as the fibrous tunic, is composed of the cornea and sclera. The middle layer, known as the vascular tunic or uvea, consists of the choroid, ciliary body, pigmented epithelium and iris. The innermost is the retina, which gets its oxygenation from the blood vessels of the choroid (posteriorly) as well as the retinal vessels (anteriorly). The spaces of the eye are filled with the aqueous humour anteriorly, between the cornea and lens, and the vitreous body, a jelly-like substance, behind the lens, filling the entire posterior cavity. The aqueous humour is a clear watery fluid that is contained\n",
      "Answer:.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 47/2889 [00:59<07:11,  6.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'ID generato 'the \"Four Year War\"' non è valido.\n",
      "L'ID generato 'Bernard Tomic' non è valido.\n",
      "L'ID generato è vuoto o contiene solo spazi. Controllo tutti i documenti...\n",
      "Risposta trovata nel documento ID 2017, Brazil returned to the No. 1 spot for the first time since just prior to the 2010 World Cup, but Germany regained the top spot in July after winning the Confederations Cup. In August 2018, France became the leader in the FIFA rankings again after nearly 16 years, having won the 2018 FIFA World Cup, and this is also the first time FIFA adopted the Elo rating system to the ranking system. One month later, for the first time two teams were tied at the top spot as Belgium returned to the number one spot with the same ranking\n",
      "Answer:.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 49/2889 [00:59<06:05,  7.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'ID generato 'the 16th' non è valido.\n",
      "L'ID generato è vuoto o contiene solo spazi. Controllo tutti i documenti...\n",
      "Risposta trovata nel documento ID The Glory of Love (song) \"The Glory of Love\" is a song written by Billy Hill, recorded by Benny Goodman in 1936, whose version was a number one pop hit. Subsequently, the song has been recorded by a vast number of artists, ranging from Dean Martin to Jimmy Durante to Paul McCartney. Bette Midler included the song in her film \"Beaches\" (1988) and it appears in the soundtrack recording. In 1951, R&B vocal group, The Five Keys, had their biggest R&B hit with their version of the song, hitting number one on the R&B chart for four non-consecutive weeks. Although\n",
      "Answer:.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 51/2889 [01:00<05:46,  8.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'ID generato 'Buffon' non è valido.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 53/2889 [01:00<06:50,  6.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'ID generato 'Eiichi Ohtaki' non è valido.\n",
      "L'ID generato 'IOC' non è valido.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 55/2889 [01:00<07:11,  6.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'ID generato 'January 8, 2018' non è valido.\n",
      "L'ID generato 'Id_5' non è valido.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 57/2889 [01:01<06:37,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'ID generato 'George Berkeley' non è valido.\n",
      "L'ID generato 'Uralic' non è valido.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 59/2889 [01:01<05:53,  8.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'ID generato 'larger' non è valido.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 63/2889 [01:01<05:19,  8.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'ID generato 'Phelps' non è valido.\n",
      "L'ID generato è vuoto o contiene solo spazi. Controllo tutti i documenti...\n",
      "Risposta trovata nel documento ID \"OED\" mentions of its meaning \"a liquid for drinking\" occurred in the 14th century. Its use as a term for \"an intoxicating alcoholic drink\" appeared in the 16th century. The term \"spirit\" in reference to alcohol stems from Middle Eastern alchemy. These alchemists were more concerned with medical elixirs than with transmuting lead into gold. The vapor given off and collected during an alchemical process (as with distillation of alcohol) was called a spirit of the original material. Early evidence of distillation comes from Akkadian tablets dated \"circa\" 1200 BC describing perfumery operations, providing textual evidence that an early primitive\n",
      "Answer:.\n",
      "L'ID generato 'John Brown' non è valido.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 65/2889 [01:02<06:11,  7.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'ID generato 'April 22, 1998' non è valido.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 68/2889 [01:02<05:22,  8.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'ID generato è vuoto o contiene solo spazi. Controllo tutti i documenti...\n",
      "Risposta trovata nel documento ID of the same row also becomes calcified, and transverse bars of calcified substance stretch across from one calcareous column to another. Thus there are longitudinal groups of the cartilage cells enclosed in oblong cavities, the walls of which are formed of calcified matrix which cuts off all nutrition from the cells; the cells, in consequence, atrophy, leaving spaces called the primary areolæ. There are two types of ossification centers – primary and secondary. A primary ossification center is the first area of a bone to start ossifying. It usually appears during prenatal development in the central part of each developing\n",
      "Answer:.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 70/2889 [01:02<06:05,  7.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'ID generato 'March 8, 2018' non è valido.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 71/2889 [01:02<06:36,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'ID generato 'Id_2' non è valido.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 72/2889 [01:03<07:43,  6.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'ID generato 'Hasse Olsson' non è valido.\n",
      "L'ID generato è vuoto o contiene solo spazi. Controllo tutti i documenti...\n",
      "Risposta trovata nel documento ID edge has two vertices (which may coincide) as endpoints. That is, we allow multiple edges (edges with the same pair of endpoints) and loops (edges whose two endpoints are the same vertex). A subgraph of a graph is the graph formed by any subsets of its vertices and edges such that each edge in the edge subset has both endpoints in the vertex subset. A connected component of an undirected graph is the subgraph consisting of the vertices and edges that can be reached by following edges from a single given starting vertex. A graph is connected if every vertex.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 75/2889 [01:03<06:46,  6.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'ID generato 'EPISODE 108' non è valido.\n",
      "L'ID generato 'IGN' non è valido.\n",
      "L'ID generato è vuoto o contiene solo spazi. Controllo tutti i documenti...\n",
      "Nessuna risposta trovata in nessun documento.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 77/2889 [01:03<05:03,  9.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'ID generato è vuoto o contiene solo spazi. Controllo tutti i documenti...\n",
      "Risposta trovata nel documento ID Supporting Actress (Beatrice Straight), and Best Screenplay (Paddy Chayefsky). Thanks to a stellar cast, experienced director, and a poignant story, \"Network\" became one of the largest critical successes of 1976. Another film, \"Rocky\", about a clubhouse boxer (played by Sylvester Stallone) who is granted a world championship title fight won the Best Picture Academy Award that year. The film also became a major commercial success and spawned four sequels through the rest of the 1970s and 1980s. Throughout the 1970s, the horror film developed into a lucrative genre of film. It began in 1973 with the terrifying \"The Exorcist\", directed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 82/2889 [01:04<07:59,  5.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'ID generato 'XXXIX' non è valido.\n",
      "L'ID generato 'the Lord's sign' non è valido.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 86/2889 [01:05<08:07,  5.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'ID generato '\"Sacrifice\"' non è valido.\n",
      "L'ID generato '16th' non è valido.\n",
      "L'ID generato è vuoto o contiene solo spazi. Controllo tutti i documenti...\n",
      "Nessuna risposta trovata in nessun documento.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 89/2889 [01:05<06:30,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'ID generato 'IOC' non è valido.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 92/2889 [01:06<10:06,  4.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'ID generato 'I Didn't Leave the Democrats. They Left me' non è valido.\n",
      "L'ID generato 'Dexter Wansel' non è valido.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 93/2889 [01:06<10:12,  4.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'ID generato 'Id_2' non è valido.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 96/2889 [01:07<08:39,  5.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'ID generato 'March 20, 1852' non è valido.\n",
      "L'ID generato 'Armstrong' non è valido.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 98/2889 [01:07<06:13,  7.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'ID generato è vuoto o contiene solo spazi. Controllo tutti i documenti...\n",
      "Risposta trovata nel documento ID chain as a source of energy. The overall process of creating energy in this fashion is termed oxidative phosphorylation. The same process takes place in the mitochondria, where ATP synthase is located in the inner mitochondrial membrane and the F-part projects into mitochondrial matrix. The consumption of ATP by ATP-synthase pumps proton cations into the matrix. The evolution of ATP synthase is thought to have been modular whereby two functionally independent subunits became associated and gained new functionality. This association appears to have occurred early in evolutionary history, because essentially the same structure and activity of ATP synthase enzymes are.\n",
      "L'ID generato è vuoto o contiene solo spazi. Controllo tutti i documenti...\n",
      "Risposta trovata nel documento ID What a Friend We Have in Jesus \"What a Friend We Have in Jesus\" is a Christian hymn originally written by preacher Joseph M. Scriven as a poem in 1855 to comfort his mother who was living in Ireland while he was in Canada. Scriven originally published the poem anonymously, and only received full credit for it in the 1880s. The tune to the hymn was composed by Charles Crozat Converse in 1868. William Bolcom composed a setting of the hymn. The hymn also has many versions with different lyrics in multiple languages. The \"Handbook to the Lutheran Hymnal\" notes,.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 100/2889 [01:09<32:10,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Risultati salvati in: C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\gen_id_res_example_bgm/nq/t5-base/test/retrieved/contriever/5_doc\\generated_results_weights_epoch_25.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re \n",
    "import argparse\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from typing import Tuple, Dict, Optional\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "from utils import *\n",
    "from bgm import BGM\n",
    "from default_prompts import *\n",
    "from prompt_dataset import PromptDataset\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "device = torch.device(f\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "warnings.filterwarnings('ignore')\n",
    "SEED=10\n",
    "\n",
    "info = {\n",
    "    \"nq\": {\n",
    "        \"test\": {\n",
    "            \"data_path\": r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\test_dataset.json',\n",
    "            \"contriever_search_results_path\": r\"C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\processed\\contriever_test_search_results_at150.pkl\",\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "class DotDict:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "def parse_arguments(custom_args=None):\n",
    "    \"\"\"\n",
    "    Mimics argparse to parse arguments for LLM generation. Accepts custom arguments as a dictionary for notebooks.\n",
    "    \"\"\"\n",
    "    # Define default values\n",
    "    default_args = {\n",
    "        'output_dir': r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\gen_id_res_example_bgm',\n",
    "        'llm_id': 'google-t5/t5-base',\n",
    "        'dataset': 'nq',\n",
    "        'model_max_length': 4096,\n",
    "        'quantization_bits': 4,\n",
    "        'gold_position': None,\n",
    "        'use_model_chat_template': False, \n",
    "        'num_retrieved_documents': 5,\n",
    "        'use_test': True,\n",
    "        'padding_strategy': 'longest',\n",
    "        'max_new_tokens': 15,\n",
    "        'use_task_with_proof': False,\n",
    "        'batch_size': None,\n",
    "        'save_every': 250,\n",
    "    }\n",
    "\n",
    "    # If custom_args is provided, update defaults\n",
    "    if custom_args:\n",
    "        default_args.update(custom_args)\n",
    "\n",
    "    # Perform validation\n",
    "    if default_args['num_retrieved_documents'] is None:\n",
    "        raise ValueError(\"'num_retrieved_documents' must be specified.\")\n",
    "    if default_args['num_retrieved_documents'] <= 0:\n",
    "        raise ValueError(\"'num_retrieved_documents' must be a positive integer.\")\n",
    "    if default_args['gold_position'] is not None:\n",
    "        if (default_args['gold_position'] < 0 or \n",
    "            default_args['gold_position'] >= default_args['num_retrieved_documents']):\n",
    "            raise ValueError(\"'gold_position' must be within the range of 'num_retrieved_documents'.\")\n",
    "\n",
    "    return DotDict(**default_args)\n",
    "\n",
    "\n",
    "def load_corpus(\n",
    "    args: argparse.Namespace\n",
    ") -> Tuple[List[Dict], Optional[Dict[int, int]]]:\n",
    "    \n",
    "    # Corpus with documents from Contriever\n",
    "    corpus, full_to_subset_idx_map = read_test_corpus_with_random_and_contriever()\n",
    "\n",
    "    return corpus, full_to_subset_idx_map\n",
    "\n",
    "def load_search_results(args: argparse.Namespace) -> List[Tuple[List[int], List[float]]]:\n",
    "\n",
    "    search_results_path = info[args.dataset][args.split]['contriever_search_results_path']\n",
    "    retriever_search_results = read_pickle(search_results_path)\n",
    "\n",
    "    return retriever_search_results\n",
    "\n",
    "\n",
    "def get_prompt_template(args: argparse.Namespace):\n",
    "    prompt_configuration = args.dataset\n",
    "    if args.use_model_chat_template:\n",
    "        chat_task_template_str = chat_task_templates[args.llm_id]['template']\n",
    "        \n",
    "        task_instruction = task_instructions[prompt_configuration]\n",
    "\n",
    "        prompt_template = apply_chat_task_template(chat_task_template_str, task_instruction)\n",
    "    else:\n",
    "        task_template = task_templates[prompt_configuration]\n",
    "\n",
    "        prompt_template = task_template.create_prompt_template()\n",
    "\n",
    "    return prompt_template\n",
    "\n",
    "\n",
    "def initialize_dataset_and_loader(\n",
    "    args: argparse.Namespace, \n",
    "    corpus: List[Dict], \n",
    "    full_to_subset_idx_map: Optional[Dict[int, int]], \n",
    "    retriever_search_results: List[Tuple[List[int], List[float]]], \n",
    "    tokenizer: PreTrainedTokenizer\n",
    ") -> DataLoader:\n",
    "    \n",
    "    prompt_template = get_prompt_template(args)\n",
    "    \n",
    "    prompt_ds = PromptDataset(\n",
    "        corpus=corpus, data_path=info[args.dataset][args.split]['data_path'], \n",
    "        tokenizer=tokenizer, \n",
    "        max_tokenized_length=args.model_max_length - 2, \n",
    "        search_results=retriever_search_results,\n",
    "        prompt_template=prompt_template,\n",
    "        full_to_subset_idx_map=full_to_subset_idx_map,\n",
    "        do_normalize_query=True, \n",
    "        num_documents_in_context=args.num_retrieved_documents,\n",
    "        gold_position=args.gold_position, # None in these experiments\n",
    "    )\n",
    "        \n",
    "    prompt_dataloader = DataLoader(\n",
    "        prompt_ds,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=8,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    return prompt_dataloader\n",
    "\n",
    "\n",
    "from typing import List, Tuple, Optional, Union\n",
    "\n",
    "from typing import List, Tuple, Optional, Union\n",
    "import re\n",
    "\n",
    "def check_document_has_answer(\n",
    "    generated_id: Union[str, List[Union[str, int]]],\n",
    "    answers: List[str],\n",
    "    prompt: str\n",
    ") -> Tuple[Union[bool, str], Optional[str], Optional[str]]:\n",
    "    \"\"\"\n",
    "    Verifica se il documento corrispondente all'ID generato o tutti i documenti nel prompt contengono una risposta,\n",
    "    gestendo casi in cui l'ID generato può essere una stringa, una lista o nullo.\n",
    "\n",
    "    Args:\n",
    "        generated_id (Union[str, List[Union[str, int]]]): ID generato (stringa, lista di interi/stringhe, o nullo).\n",
    "        answers (List[str]): Lista delle risposte da confrontare.\n",
    "        prompt (str): Testo completo del prompt con i documenti.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Union[bool, str], Optional[str], Optional[str]]: Una tupla con:\n",
    "            - Un booleano (True) se è stata trovata una risposta,\n",
    "              o una stringa descrittiva se nessuna risposta è stata trovata.\n",
    "            - La risposta trovata, se presente (altrimenti None).\n",
    "            - Il testo del documento corrispondente, se esiste (altrimenti None).\n",
    "    \"\"\"\n",
    "    # Usa regex per estrarre i documenti dal prompt\n",
    "    documents = re.findall(r\"Document \\[(\\d+)\\]\\(.*?\\):?\\s(.*?)(?=Document \\[\\d+\\]|$)\", prompt, re.DOTALL)\n",
    "    documents_dict = {int(doc_id): text.strip() for doc_id, text in documents}\n",
    "\n",
    "    # Funzione per controllare un singolo ID\n",
    "    def check_single_id(id_doc):\n",
    "        if id_doc in documents_dict:\n",
    "            document_text = documents_dict[id_doc]\n",
    "            for answer in answers:\n",
    "                if answer.lower() in document_text.lower():\n",
    "                    return True, answer, document_text\n",
    "            return \"Nessuna risposta trovata nel documento specificato.\", None, document_text\n",
    "        return \"Documento con l'ID specificato non trovato.\", None, None\n",
    "\n",
    "    # Caso 1: ID generato è una stringa nulla o contiene solo spazi\n",
    "    if isinstance(generated_id, str) and not generated_id.strip():\n",
    "\n",
    "        for doc_id, document_text in documents_dict.items():\n",
    "            for answer in answers:\n",
    "                if answer.lower() in document_text.lower():\n",
    "                    return True, answer, document_text\n",
    "\n",
    "        return \"Nessun documento contiene le risposte.\", None, None\n",
    "\n",
    "    # Caso 2: ID generato è una lista\n",
    "    if isinstance(generated_id, list):\n",
    "        for id_item in generated_id:\n",
    "            id_doc = int(id_item)\n",
    "            result = check_single_id(id_doc)\n",
    "            if result[0] == True:\n",
    "                return result\n",
    "        return \"Nessun documento contiene le risposte per gli ID nella lista.\", None, None\n",
    "\n",
    "    # Caso 3: ID generato è una stringa o un intero singolo\n",
    "    try:\n",
    "        id_doc = int(generated_id)\n",
    "        return check_single_id(id_doc)\n",
    "    except ValueError:\n",
    "        return \"L'ID generato non è valido.\", None, None\n",
    "    \n",
    "\n",
    "def print_info(args: argparse.Namespace):\n",
    "    print(\"INFO:\")    \n",
    "    print(f\"DATA: {info[args.dataset]['test']['data_path']}\")\n",
    "    print(f\"USE TEST: {args.use_test}\")\n",
    "    print(f\"MODEL: {args.llm_id}\")\n",
    "    print(f\"MODEL MAX LENGTH: {args.model_max_length}\")\n",
    "    print(f'MAX NEW TOKENS: {args.max_new_tokens}')\n",
    "    print(f\"USE MODEL CHAT TEMPLATE: {args.use_model_chat_template}\")\n",
    "    print(f\"TASK WITH PROOF:\", args.use_task_with_proof)\n",
    "    print(f\"GOLD POSITION: {args.gold_position}\")\n",
    "    print(f\"NUM DOCUMENTS IN CONTEXT: {args.num_retrieved_documents}\")\n",
    "    print(f\"BATCH SIZE: {args.batch_size}\")\n",
    "    print(f\"SAVE EVERY: {args.save_every}\")\n",
    "\n",
    "\n",
    "def generate_and_save(\n",
    "    args: argparse.Namespace, \n",
    "    bgm: BGM, \n",
    "    prompt_dataloader: DataLoader\n",
    "):\n",
    "    # Info from arguments\n",
    "    llm_id = args.llm_id\n",
    "    num_doc = args.num_retrieved_documents\n",
    "    save_every = args.save_every\n",
    "    retriever_str = \"contriever\"\n",
    "    padding_str = f\"_{args.padding_strategy}{args.model_max_length}\" if args.padding_strategy != \"longest\" else \"\" \n",
    "    chat_template_str = \"_template\" if args.use_model_chat_template else \"\"\n",
    "    prompt_type = \"retrieved_proof\" if args.use_task_with_proof else \"retrieved\"\n",
    "\n",
    "    # Create the saving directory\n",
    "    llm_folder = llm_id.split(\"/\")[1] if '/' in llm_id else llm_id\n",
    "    saving_dir = f\"{args.output_dir}/{args.dataset}/{llm_folder}/{args.split}/{prompt_type}/{retriever_str}/{num_doc}_doc\"\n",
    "    os.makedirs(saving_dir, exist_ok=True)\n",
    "\n",
    "    # Path del file .json\n",
    "    json_file_path = os.path.join(saving_dir, \"generated_results_weights_epoch_25.json\")\n",
    "\n",
    "    all_info = []  \n",
    "    for idx, prompt_batch in enumerate(tqdm(prompt_dataloader)):\n",
    "        if idx == 100:\n",
    "            break\n",
    "\n",
    "        prompts = prompt_batch['prompt']\n",
    "        answers = prompt_batch['answers']\n",
    "        document_indices=prompt_batch['document_indices']\n",
    "\n",
    "        # Usa una regex per estrarre tutto a partire da \"Question\"\n",
    "        match = re.search(r\"Question:.*\", prompts, re.DOTALL)\n",
    "\n",
    "        # Controlla se c'è una corrispondenza e prendi il risultato\n",
    "        if match:\n",
    "            prompts = match.group()\n",
    "        else:\n",
    "            print(\"Nessuna corrispondenza trovata.\")\n",
    "\n",
    "        generated_output = bgm.generate(\n",
    "            prompts, \n",
    "            padding_strategy=args.padding_strategy,\n",
    "            max_new_tokens=args.max_new_tokens\n",
    "        )\n",
    "\n",
    "        has_answer, answer_found, document_found = check_document_has_answer(generated_output, answers, prompts)\n",
    "\n",
    "\n",
    "        # Salva i risultati in un dizionario\n",
    "        result = {\n",
    "            \"prompt\": prompts,\n",
    "            \"all_document_indices\": document_indices,\n",
    "            \"generated_indices\": generated_output,\n",
    "            \"id_document\": document_found,\n",
    "            \"generated_id_document_has_answer\": has_answer,\n",
    "            \"answer_in_the_document\": answer_found,\n",
    "            \"answers_target\": answers\n",
    "        }\n",
    "        all_info.append(result)\n",
    "        \n",
    "        #print(f\"Esempio {idx+1}\\n\")\n",
    "        #print(f\"I migliori indici secondo il modello: {generated_output}\\n\")\n",
    "        #print(f\"Gli indici target sono: {prompt_batch['document_indices']}\")\n",
    "\n",
    "    # Scrivi i risultati nel file JSON\n",
    "    with open(json_file_path, \"w\", encoding=\"utf-8\") as json_file:\n",
    "        json.dump(all_info, json_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "    print(f\"Risultati salvati in: {json_file_path}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = parse_arguments()\n",
    "\n",
    "    args.split = \"test\" if args.use_test else \"train\"\n",
    "\n",
    "    print(\"Loading LLM...\")\n",
    "    llm_id = args.llm_id\n",
    "\n",
    "    saved_model_path = r\"C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\lora_training_bgm\\google-t5-base\\lora-checkpoint\\epochs\\epoch_25\"\n",
    "    \n",
    "    bgm = BGM(\n",
    "        llm_id, device, \n",
    "        quantization_bits=args.quantization_bits, \n",
    "        model_max_length=args.model_max_length,\n",
    "        lora_weights_path=saved_model_path\n",
    "    )\n",
    "    tokenizer = bgm.tokenizer\n",
    "    print(\"LLM loaded\")\n",
    "\n",
    "\n",
    "    print(\"Loading corpus and search results...\")\n",
    "    corpus, full_to_subset_idx_map = load_corpus(args)\n",
    "    retriever_search_results = load_search_results(args)\n",
    "    print(\"Corpus and search results loaded\")\n",
    "\n",
    "\n",
    "    print(\"Loading prompt dataset...\")\n",
    "    prompt_dataloader = initialize_dataset_and_loader(\n",
    "        args, corpus, full_to_subset_idx_map, \n",
    "        retriever_search_results, tokenizer\n",
    "    )\n",
    "    print(\"Prompt dataset loaded\")\n",
    "\n",
    "    print_info(args)\n",
    "\n",
    "    #for i in range(5):\n",
    "        #entry = prompt_dataloader.dataset[i]\n",
    "        #print(f\"{entry}\")\n",
    "\n",
    "    generate_and_save(args, bgm, prompt_dataloader)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    seed_everything(SEED)\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
