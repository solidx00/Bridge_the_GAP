{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "NVIDIA GeForce RTX 3080\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'who got the first nobel prize in physics',\n",
       " 'idx_gold_in_corpus': 20994698,\n",
       " 'answers': ['Wilhelm Conrad Röntgen'],\n",
       " 'text': 'The first Nobel Prize in Physics was awarded in 1901 to Wilhelm Conrad Röntgen , of Germany , who received 150,782 SEK , which is equal to 7,731,004 SEK in December 2007 . John Bardeen is the only laureate to win the prize twice -- in 1956 and 1972 . Maria Skłodowska - Curie also won two Nobel Prizes , for physics in 1903 and chemistry in 1911 . William Lawrence Bragg was , until October 2014 , the youngest ever Nobel laureate ; he won the prize in 1915 at the age of 25 . Two women have won the prize : Curie and Maria Goeppert - Mayer ( 1963 ) . As of 2017 , the prize has been awarded to 206 individuals . There have been six years in which the Nobel Prize in Physics was not awarded ( 1916 , 1931 , 1934 , 1940 -- 1942 ) .',\n",
       " 'example_id': -3290814144789249484}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('florin-hf/nq_open_gold')\n",
    "\n",
    "#train_questions = dataset['train']['question']\n",
    "#validation_questions = dataset['validation']['question']\n",
    "test_questions = dataset['test']['question']\n",
    "example=dataset['test'][0]\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import hashlib\n",
    "from typing import List, Tuple, Dict, Any, Optional\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import normalize_text\n",
    "from normalize_answers import *\n",
    "\n",
    "class QueryDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A dataset class for managing queries data into structured prompts suitable for input to LLMS.\n",
    "\n",
    "    Attributes:\n",
    "        data_path (str): Path to the dataset file containing the query and related information.\n",
    "        model_name (str): The name of the language model used for generating answers.\n",
    "        do_normalize_query (bool): Flag to determine if text normalization is applied to the query.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        data_path: str, \n",
    "        model_name: str,\n",
    "        do_normalize_query: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.data_path = data_path\n",
    "        self.model_name = model_name\n",
    "        self.do_normalize_query = do_normalize_query\n",
    "        self._load_data()\n",
    "\n",
    "\n",
    "    def _load_data(self):\n",
    "        \"\"\"\n",
    "        Loads data from the specified path and processes it.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(self.data_path, \"r\") as fin:\n",
    "                data = json.load(fin)\n",
    "            self.process_file_data(data)\n",
    "        except IOError as e:\n",
    "            print(f\"Error reading file {self.data_path}: {e}\")\n",
    "\n",
    "\n",
    "    def process_file_data(self, data: List[Dict]):\n",
    "        \"\"\" Processes each example in the dataset to prepare prompts for the LLM. \"\"\"  \n",
    "        self.questions = []\n",
    "        self.example_ids = []\n",
    "\n",
    "        for example in data:\n",
    "            self.example_ids.append(example['example_id'])\n",
    "\n",
    "            if 'query' in example:\n",
    "                question = example['query']\n",
    "            elif 'question' in example:\n",
    "                question = example['question']\n",
    "            else:\n",
    "                raise ValueError(\"No 'query' or 'question' key in example\")\n",
    "            \n",
    "            if self.do_normalize_query:\n",
    "                question = normalize_text.normalize(question)\n",
    "            self.questions.append(question)\n",
    "\n",
    "\n",
    "    def build_qa_prompt(self, query: str) -> str:\n",
    "        task_instruction = \"You are given a question and you must respond based on the provided documents. You must always provide an answer.\"\n",
    "        prompt = f\"\"\"{task_instruction}\\nQuestion: {query}\\nAnswer:\"\"\"\n",
    "        \n",
    "        # Custom prompt format for mpt models\n",
    "        if 'mpt' in self.model_name:\n",
    "            INSTRUCTION_KEY = \"### Instruction:\"\n",
    "            RESPONSE_KEY = \"### Response:\"\n",
    "            INTRO_BLURB = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "            PROMPT_FOR_GENERATION_FORMAT = \"\"\"{intro}\\n{instruction_key}\\n{instruction}\\n{response_key}\"\"\".format(\n",
    "                intro=INTRO_BLURB,\n",
    "                instruction_key=INSTRUCTION_KEY,\n",
    "                instruction=\"{instruction}\",\n",
    "                response_key=RESPONSE_KEY,\n",
    "            )\n",
    "            prompt = PROMPT_FOR_GENERATION_FORMAT.format(\n",
    "                instruction=prompt[:-8]\n",
    "            )\n",
    "\n",
    "        return prompt\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx: int):   \n",
    "        prompt = self.build_qa_prompt(self.questions[idx])\n",
    "\n",
    "        return {\n",
    "            \"example_id\": self.example_ids[idx],\n",
    "            \"query\": self.questions[idx],\n",
    "            \"prompt\": prompt,\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.example_ids)\n",
    "\n",
    "\n",
    "def hash_document(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate a SHA-256 hash for a given text.\n",
    "    \"\"\"\n",
    "    return hashlib.sha256(text.encode()).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A dataset class for managing, preprocessing, and organizing document data into structured prompts suitable for input to LLMS.\n",
    "\n",
    "    Attributes:\n",
    "        corpus (List[Dict]): The list containing the document corpus.\n",
    "        data_path (str): Path to the dataset file containing the query and related information.\n",
    "        tokenizer (AutoTokenizer): The tokenizer used to tokenize the prompt, in order to check its tokenized length.\n",
    "        max_tokenized_length (int): The maximum length of tokenized prompt. Prompts that exceed this length are excluded from the dataset.\n",
    "        search_results (List[Tuple[List[str], List[float]]]): A list of tuples containing document indices and their scores. The results may come from a retriever.\n",
    "        full_to_subset_idx_map (Dict[int, int]): Dictionary that maps the indices in the full corpus to the given subset (corpus).\n",
    "        do_normalize_query (bool): Flag to determine if text normalization is applied to the query.\n",
    "        num_documents_in_context (int): The total number of documents to consider in the context.\n",
    "        gold_position (int): The specific position (0-indexed) of the gold document in the context.\n",
    "        randomize_gold_position (bool): Flag to determine if the gold document position should be random.\n",
    "        get_documents_without_answer (bool): Flag to determine if documents without the answer should be included in the prompt.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        corpus: List[Dict],\n",
    "        data_path: str,  \n",
    "        tokenizer: AutoTokenizer,\n",
    "        max_tokenized_length: int,\n",
    "        search_results: List[Tuple[List[int], List[float]]],\n",
    "        full_to_subset_idx_map: Dict[int, int] = None,\n",
    "        do_normalize_query: bool = False,\n",
    "        num_documents_in_context: int = 5,\n",
    "        gold_position: int = None,\n",
    "        randomize_gold_position: bool = False,\n",
    "        get_documents_without_answer: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.corpus = corpus\n",
    "        self.data_path = data_path\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_tokenized_length = max_tokenized_length\n",
    "        self.search_results = search_results\n",
    "        self.full_to_subset_idx_map = full_to_subset_idx_map\n",
    "        self.do_normalize_query = do_normalize_query\n",
    "        self.num_documents_in_context = num_documents_in_context\n",
    "        self.gold_position = gold_position\n",
    "        self.randomize_gold_position = randomize_gold_position\n",
    "        self.get_documents_without_answer = get_documents_without_answer\n",
    "    \n",
    "        \n",
    "        self._validate_initialization_parameters()\n",
    "        self._load_data()\n",
    "\n",
    "\n",
    "    def _validate_initialization_parameters(self):\n",
    "        \"\"\"Validates initialization parameters for logical consistency and correctness.\"\"\"\n",
    "        if self.num_documents_in_context <= 0:\n",
    "            raise ValueError(\"num_documents_in_context must be positive.\")\n",
    "        \n",
    "        if self.max_tokenized_length <= 0:\n",
    "            raise ValueError(\"max_tokenized_length must be positive.\")\n",
    "\n",
    "        if self.gold_position is not None:\n",
    "            if self.gold_position < 0 or (self.gold_position >= self.num_documents_in_context):\n",
    "                raise ValueError(f\"Invalid gold position: {self.gold_position}\")\n",
    "        \n",
    "        if self.gold_position is not None and self.randomize_gold_position:\n",
    "            raise ValueError(\"Both 'gold_position' and 'randomize_gold_position' cannot be set at the same time.\")\n",
    "\n",
    "\n",
    "    def _load_data(self):\n",
    "        \"\"\"\n",
    "        Loads data from the specified path and processes it.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(self.data_path, \"r\") as fin:\n",
    "                data = json.load(fin)\n",
    "            self.process_file_data(data)\n",
    "        except IOError as e:\n",
    "            print(f\"Error reading file {self.data_path}: {e}\")\n",
    "\n",
    "\n",
    "    def process_file_data(self, data: List[Dict]):  \n",
    "        \"\"\"\n",
    "        Processes each example in the dataset to prepare prompts for the LLM.\n",
    "\n",
    "        This involves assembling document contexts, normalizing text as needed,\n",
    "        and checking against the maximum token length to ensure compatibility with the LLM's input specifications.\n",
    "\n",
    "        Args:\n",
    "            data (List[Dict]): The dataset, where each entry contains information about an example,\n",
    "            including the example's ID, the gold document index, answers, and the query.\n",
    "        \"\"\"\n",
    "        self.example_ids = []\n",
    "        self.queries = []\n",
    "        self.prompts = []\n",
    "        self.gold_document_idxs = []\n",
    "        self.excluded_samples_ids = []\n",
    "        self.preprocessed_data = []\n",
    "        self.prompt_tokens_lengths = []\n",
    "\n",
    "        for idx, example in enumerate(data):\n",
    "            example_id = str(example['example_id'])\n",
    "            gold_document_idx = str(example['idx_gold_in_corpus'])\n",
    "            answers = example['answers']\n",
    "\n",
    "            formatted_documents, document_indices = self.prepare_documents_for_prompt(\n",
    "                idx, gold_document_idx, answers\n",
    "            )\n",
    "\n",
    "            # Build the prompt\n",
    "            documents_str = '\\n'.join(formatted_documents)\n",
    "            query = example['question']\n",
    "            if self.do_normalize_query:\n",
    "                query = normalize_text.normalize(query)\n",
    "            prompt = self.build_qa_prompt(query, documents_str)\n",
    "\n",
    "            # Check if the prompt exceeds 'max_tokenized_length'\n",
    "            tokens = self.tokenizer.tokenize(prompt)\n",
    "            tokens_len = len(tokens)\n",
    "            if tokens_len >= self.max_tokenized_length:\n",
    "                self.excluded_samples_ids.append((idx, example_id))\n",
    "                print(\"Skipping example {} due to prompt length.\".format((idx, example_id)))\n",
    "                continue  # Skip adding this example\n",
    "\n",
    "            if len(formatted_documents) != self.num_documents_in_context:\n",
    "                print(f\"Warning: Not enough documents for example {idx}.\")\n",
    "\n",
    "            # If prompt is within limit, add to preprocessed data\n",
    "            self.preprocessed_data.append((formatted_documents, list(document_indices)))\n",
    "            self.example_ids.append(example_id)\n",
    "            self.queries.append(query)\n",
    "            self.prompts.append(prompt)\n",
    "            self.gold_document_idxs.append(gold_document_idx)\n",
    "            self.prompt_tokens_lengths.append(tokens_len)\n",
    "\n",
    "\n",
    "    def prepare_documents_for_prompt(\n",
    "        self, \n",
    "        example_idx: int, \n",
    "        gold_document_idx: int, \n",
    "        answers: List[str]\n",
    "    ) -> Tuple[List[str], List[int]]:\n",
    "        \"\"\"\n",
    "        Prepares and formats a set of documents for inclusion in a prompt, including the insertion of a gold document at the appropriate position.\n",
    "\n",
    "        This function performs several key steps to prepare documents for a prompt:\n",
    "        1. Retrieves document indices based on the example index.\n",
    "        2. Inserts the gold document index into the retrieved list of indices at a specified or randomized position, if necessary.\n",
    "        3. Formats the documents corresponding to the updated list of indices, preparing them for inclusion in the prompt. \n",
    "           This includes potentially filtering documents based on answers or other criteria.\n",
    "\n",
    "        Args:\n",
    "            example_idx (int): The index of the current example in the dataset. This is used to retrieve the appropriate set of document indices.\n",
    "            gold_document_idx (int): The index of the gold document within the corpus. \n",
    "            answers (List[str]): A list of answers that can be used to ensure the relevance of documents included in the prompt.\n",
    "\n",
    "        Returns:\n",
    "            A tuple containing two lists:\n",
    "            - The first list contains the formatted documents.\n",
    "            - The second list contains the indices of the included documents.\n",
    "        \"\"\"\n",
    "        indices = self._get_indices(example_idx)\n",
    "        updated_indices, gold_position = self._insert_gold_document_idx(\n",
    "            indices, gold_document_idx\n",
    "        )\n",
    "\n",
    "        # Get the documents and their indices in the corpus\n",
    "        formatted_documents, document_indices = self._get_documents(\n",
    "            updated_indices, answers, gold_document_idx, gold_position\n",
    "        )\n",
    "        return formatted_documents, document_indices\n",
    "\n",
    "\n",
    "    def _get_indices(self, example_idx: int) -> List[int]:\n",
    "        \"\"\" Get the indices in the corpus of the documents retrieved possibly by a retriever. \"\"\"\n",
    "        indices, scores = self.search_results[example_idx]\n",
    "        return indices\n",
    "\n",
    "\n",
    "    def _insert_gold_document_idx(\n",
    "        self, \n",
    "        indices: List[int], \n",
    "        gold_document_idx: int\n",
    "    ) -> Tuple[List[int], int]:\n",
    "        \"\"\"\n",
    "        Inserts the index of a gold document into the provided list of indices at a specified or random position.\n",
    "\n",
    "        Args:\n",
    "            indices: A list of integers representing document indices.\n",
    "            gold_document_idx: The index of the gold document to insert.\n",
    "\n",
    "        Returns:\n",
    "            A tuple containing:\n",
    "            - The updated list of indices with the gold document index inserted.\n",
    "            - The position at which the gold document index was inserted.\n",
    "        \"\"\"\n",
    "        gold_position = None\n",
    "        \n",
    "        if self.gold_position is not None:\n",
    "            # Direct insertion\n",
    "            gold_position = self.gold_position\n",
    "            indices = indices[:gold_position] + [gold_document_idx] + indices[gold_position:]\n",
    "        elif self.randomize_gold_position:\n",
    "            # Insert at a random position\n",
    "            gold_position = random.randint(0, self.num_documents_in_context - 1)\n",
    "            indices = indices[:gold_position] + [gold_document_idx] + indices[gold_position:]\n",
    "        return indices, gold_position\n",
    "\n",
    "\n",
    "    def _get_documents(    \n",
    "        self,\n",
    "        indices: List[int],\n",
    "        answers: List[str],\n",
    "        gold_document_idx: Optional[int],\n",
    "        gold_position: Optional[int]\n",
    "    ) -> Tuple[List[str], List[int]]:\n",
    "        \"\"\" Choose the appropriate method based on the flag \"\"\"\n",
    "        if self.get_documents_without_answer:\n",
    "            return self._get_answerless_documents_from_indices(\n",
    "                indices, answers, gold_document_idx, gold_position\n",
    "            )\n",
    "        else:\n",
    "            return self._get_documents_from_indices(indices)\n",
    "            \n",
    "\n",
    "    def _get_documents_from_indices(self, indices: List[int]) -> Tuple[List[str], List[int]]:\n",
    "        \"\"\"\n",
    "        Selects documents from the corpus based on provided indices and formats them.\n",
    "        Handles both full corpus and subsets by mapping indices if necessary.\n",
    "\n",
    "        Args:\n",
    "            indices: A list of integers representing the positions of documents to retrieve in the corpus.\n",
    "\n",
    "        Returns:\n",
    "            A tuple containing two lists:\n",
    "            - The first list contains the formatted documents.\n",
    "            - The second list contains the indices of the included documents.\n",
    "        \"\"\"\n",
    "        formatted_documents = []\n",
    "        \n",
    "        # Full corpus\n",
    "        if self.full_to_subset_idx_map is None:\n",
    "            documents_info = [self.corpus[i] for i in map(int, indices)]\n",
    "        else: \n",
    "            documents_info: List[Dict] = []\n",
    "            # 'indices' are from the full corpus, so we need to map them to the subset\n",
    "            for i in map(int, indices):\n",
    "                documents_info.append(self.corpus[self.full_to_subset_idx_map[i]])\n",
    "        \n",
    "        seen_hashes = set()\n",
    "        # List to store the indices of documents actually added\n",
    "        document_indices = []  \n",
    "        for doc_info in documents_info:\n",
    "            if len(formatted_documents) == self.num_documents_in_context:\n",
    "                break\n",
    "            \n",
    "            doc_idx = doc_info['full_corpus_idx']\n",
    "            title = doc_info['title']\n",
    "            text = doc_info['text']\n",
    "\n",
    "            doc_hash = hash_document(text)\n",
    "            # Skip the document if it is a duplicate\n",
    "            if doc_hash in seen_hashes:\n",
    "                continue\n",
    "            seen_hashes.add(doc_hash)\n",
    "            \n",
    "            doc_str = f\"Document [{doc_idx}](Title: {title}) {text}\"\n",
    "            formatted_documents.append(doc_str)\n",
    "            document_indices.append(doc_idx)\n",
    "\n",
    "        return formatted_documents, document_indices\n",
    "    \n",
    "\n",
    "    def _get_answerless_documents_from_indices(\n",
    "        self,\n",
    "        indices: List[int],\n",
    "        answers: List[str],\n",
    "        gold_document_idx: Optional[int],\n",
    "        gold_position: Optional[int]\n",
    "    ) -> Tuple[List[str], List[int]]:\n",
    "        \"\"\"\n",
    "        Selects documents from the corpus that do not contain any of the given answers, optionally including\n",
    "        a specific 'gold' document at a designated position.\n",
    "\n",
    "        Args:\n",
    "            indices: A list of integers representing the indices of documents to retrieve from the corpus.\n",
    "            answers: A list of strings representing the answers to exclude from the documents.\n",
    "            gold_document_idx: The index of the gold document in the full corpus.\n",
    "            gold_position: The desired position of the gold document within the returned list, if any.\n",
    "\n",
    "        Returns:\n",
    "            A tuple containing two lists:\n",
    "            - The first list contains the documents that do not contain the answer and possibly the gold.\n",
    "            - The second list contains the indices of the included documents.\n",
    "        \"\"\"\n",
    "        # Full corpus\n",
    "        if self.full_to_subset_idx_map is None:\n",
    "            documents_info = [self.corpus[i] for i in map(int, indices)]\n",
    "        else: \n",
    "            documents_info: List[Dict] = []\n",
    "            # 'indices' are from the full corpus, so we need to map them to the subset\n",
    "            for i in map(int, indices):\n",
    "                documents_info.append(self.corpus[self.full_to_subset_idx_map[i]])\n",
    "\n",
    "        answerless_documents = []\n",
    "        gold_document = None\n",
    "        seen_hashes = set()\n",
    "        # List to store the indices of documents actually added\n",
    "        document_indices = [] \n",
    "\n",
    "        for doc_info in documents_info:\n",
    "            doc_idx = doc_info['full_corpus_idx']\n",
    "            title = doc_info['title']\n",
    "            text = doc_info['text']\n",
    "\n",
    "            doc_hash = hash_document(text)\n",
    "            # Skip the document if it's a duplicate\n",
    "            if doc_hash in seen_hashes:\n",
    "                continue\n",
    "            seen_hashes.add(doc_hash)\n",
    "\n",
    "            if str(doc_idx) == gold_document_idx:\n",
    "                gold_document = f\"Document [{doc_idx}](Title: {title}) {text}\"\n",
    "                continue\n",
    "            \n",
    "            if not is_answer_in_text(text, answers):\n",
    "                answerless_doc = f\"Document [{doc_idx}](Title: {title}) {text}\"\n",
    "                answerless_documents.append(answerless_doc)\n",
    "                document_indices.append(doc_idx)\n",
    "\n",
    "        # Insert gold document at the specified/random position\n",
    "        if gold_position is not None and gold_document is not None:\n",
    "            gold_position = min(gold_position, len(answerless_documents))\n",
    "            answerless_documents.insert(gold_position, gold_document)\n",
    "            document_indices.insert(gold_position, gold_document_idx)\n",
    "\n",
    "        # Limit the number of documents to the specified context size\n",
    "        docs = answerless_documents[:self.num_documents_in_context]\n",
    "        indices = document_indices[:self.num_documents_in_context]\n",
    "        return docs, indices\n",
    "\n",
    "\n",
    "\n",
    "    def build_qa_prompt(self, query: str, documents_str: str) -> str:\n",
    "        task_instruction = \"You are given a question and you must respond based on the provided documents. You must always provide an answer.\"\n",
    "        prompt = f\"\"\"{task_instruction}\\nDocuments:\\n{documents_str}\\nQuestion: {query}\\nAnswer:\"\"\"\n",
    "\n",
    "        # Custom prompt format for mpt models\n",
    "        if 'mpt' in self.tokenizer.name_or_path:\n",
    "            INSTRUCTION_KEY = \"### Instruction:\"\n",
    "            RESPONSE_KEY = \"### Response:\"\n",
    "            INTRO_BLURB = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "            PROMPT_FOR_GENERATION_FORMAT = \"\"\"{intro}\\n{instruction_key}\\n{instruction}\\n{response_key}\"\"\".format(\n",
    "                intro=INTRO_BLURB,\n",
    "                instruction_key=INSTRUCTION_KEY,\n",
    "                instruction=\"{instruction}\",\n",
    "                response_key=RESPONSE_KEY,\n",
    "            )\n",
    "            prompt = PROMPT_FOR_GENERATION_FORMAT.format(\n",
    "                instruction=prompt[:-8]\n",
    "            )\n",
    "\n",
    "        return prompt\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        _, document_indices = self.preprocessed_data[idx]\n",
    "\n",
    "        return {\n",
    "            \"example_id\": self.example_ids[idx],\n",
    "            \"query\": self.queries[idx],\n",
    "            \"prompt\": self.prompts[idx],\n",
    "            \"document_indices\": document_indices,\n",
    "            \"gold_document_idx\": self.gold_document_idxs[idx],\n",
    "            \"prompt_tokens_len\": self.prompt_tokens_lengths[idx]\n",
    "        }\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.example_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LLM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM loaded\n",
      "Loading corpus and search results...\n",
      "Corpus and search results loaded\n",
      "Loading prompt dataset...\n",
      "Warning: Not enough documents for example 239.\n",
      "Warning: Not enough documents for example 1582.\n",
      "Warning: Not enough documents for example 2005.\n",
      "Prompt dataset loaded\n",
      "INFO:\n",
      "DATA: C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\test_dataset.json\n",
      "MODEL: google/gemma-2-2b-it\n",
      "GOLD POSITION: None\n",
      "NUM DOCUMENTS IN CONTEXT: 5\n",
      "DOCUMENTS WITHOUT ANSWER: True\n",
      "BATCH SIZE: None\n",
      "SAVE EVERY: 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 247/2889 [03:50<30:49,  1.43it/s] "
     ]
    }
   ],
   "source": [
    "import os \n",
    "import argparse\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from typing import Tuple, Dict, Optional\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "from llm import LLM\n",
    "from utils import *\n",
    "from prompt_dataset import PromptDataset\n",
    "\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "device = torch.device(f\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "warnings.filterwarnings('ignore')\n",
    "SEED=10\n",
    "\n",
    "info = {\n",
    "    \"data_path\": r\"C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\test_dataset.json\",\n",
    "    \"contriever_search_results_path\": r\"C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\processed\\contriever_test_search_results_at150.pkl\",\n",
    "}\n",
    "\n",
    "class DotDict:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "def parse_arguments(custom_args=None):\n",
    "    \"\"\"\n",
    "    Mimics argparse to parse arguments for LLM generation. Accepts custom arguments as a dictionary for notebooks.\n",
    "    \"\"\"\n",
    "    # Define default values\n",
    "    default_args = {\n",
    "        'output_dir': r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\gen_res_example_llm',\n",
    "        'llm_id': 'google/gemma-2-2b-it',\n",
    "        'model_max_length': 4096,\n",
    "        'gold_position': None,\n",
    "        'num_documents_in_context': None,\n",
    "        'get_documents_without_answer': True,\n",
    "        'max_new_tokens': 15,\n",
    "        'batch_size': None,\n",
    "        'save_every': 250,\n",
    "    }\n",
    "\n",
    "    # If custom_args is provided, update defaults\n",
    "    if custom_args:\n",
    "        default_args.update(custom_args)\n",
    "\n",
    "    # Perform validation\n",
    "    if default_args['num_documents_in_context'] is None:\n",
    "        raise ValueError(\"'num_documents_in_context' must be specified.\")\n",
    "    if default_args['num_documents_in_context'] <= 0:\n",
    "        raise ValueError(\"'num_documents_in_context' must be a positive integer.\")\n",
    "    if default_args['gold_position'] is not None:\n",
    "        if (default_args['gold_position'] < 0 or \n",
    "            default_args['gold_position'] >= default_args['num_documents_in_context']):\n",
    "            raise ValueError(\"'gold_position' must be within the range of 'num_documents_in_context'.\")\n",
    "\n",
    "    return DotDict(**default_args)\n",
    "\n",
    "\n",
    "def load_corpus(\n",
    "    args: argparse.Namespace\n",
    ") -> Tuple[List[Dict], Optional[Dict[int, int]]]:\n",
    "    \n",
    "    # Corpus with documents from Contriever\n",
    "    corpus, full_to_subset_idx_map = read_test_corpus_with_random_and_contriever()\n",
    "\n",
    "    return corpus, full_to_subset_idx_map\n",
    "\n",
    "\n",
    "def load_search_results(args: argparse.Namespace) -> List[Tuple[List[int], List[float]]]:\n",
    "    # Search results from Contriever\n",
    "    search_results_path = info['contriever_search_results_path'] \n",
    "\n",
    "    search_results = read_pickle(search_results_path)\n",
    "    return search_results\n",
    "\n",
    "\n",
    "def initialize_dataset_and_loader(\n",
    "    args: argparse.Namespace, \n",
    "    corpus: List[Dict], \n",
    "    full_to_subset_idx_map: Optional[Dict[int, int]], \n",
    "    search_results: List[Tuple[List[int], List[float]]], \n",
    "    tokenizer: PreTrainedTokenizer\n",
    ") -> DataLoader:\n",
    "    \n",
    "    prompt_ds = PromptDataset(\n",
    "        corpus=corpus, data_path=info['data_path'], \n",
    "        tokenizer=tokenizer, \n",
    "        max_tokenized_length=args.model_max_length - 2, \n",
    "        search_results=search_results,\n",
    "        full_to_subset_idx_map=full_to_subset_idx_map,\n",
    "        do_normalize_query=True, \n",
    "        num_documents_in_context=args.num_documents_in_context,\n",
    "        gold_position=args.gold_position,\n",
    "        get_documents_without_answer=args.get_documents_without_answer,\n",
    "    )\n",
    "    prompt_dataloader = DataLoader(\n",
    "        prompt_ds,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=8,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    return prompt_dataloader\n",
    "\n",
    "\n",
    "def print_info(args: argparse.Namespace):\n",
    "    print(\"INFO:\")\n",
    "    print(f\"DATA: {info['data_path']}\")\n",
    "    print(f\"MODEL: {args.llm_id}\")\n",
    "    print(f\"GOLD POSITION: {args.gold_position}\")\n",
    "    print(f\"NUM DOCUMENTS IN CONTEXT: {args.num_documents_in_context}\")\n",
    "    print(f\"DOCUMENTS WITHOUT ANSWER: {args.get_documents_without_answer}\")\n",
    "    print(f\"BATCH SIZE: {args.batch_size}\")\n",
    "    print(f\"SAVE EVERY: {args.save_every}\")\n",
    "\n",
    "\n",
    "def generate_and_save(\n",
    "    args: argparse.Namespace, \n",
    "    llm: LLM, \n",
    "    prompt_dataloader: DataLoader\n",
    "):\n",
    "    # Info from arguments\n",
    "    llm_id = args.llm_id\n",
    "    num_doc = args.num_documents_in_context\n",
    "    save_every = args.save_every\n",
    "    gold_pos = args.gold_position\n",
    "    retriever_str = \"contriever\"\n",
    "    answerless_str = \"_answerless\" if args.get_documents_without_answer else \"\"\n",
    "\n",
    "    # Create the saving directory\n",
    "    llm_folder = llm_id.split(\"/\")[1] if '/' in llm_id else llm_id\n",
    "    saving_dir = f\"{args.output_dir}/{llm_folder}/train/classic/{retriever_str}/{num_doc}_doc\"\n",
    "    if not os.path.exists(saving_dir):\n",
    "        os.makedirs(saving_dir)\n",
    "\n",
    "    \n",
    "    # MPT has a different answer string in the prompt\n",
    "    answer_string_in_prompt = \"### Response:\" if 'mpt' in llm_id else \"Answer:\"\n",
    "\n",
    "    all_info = []  \n",
    "    for idx, prompt_batch in enumerate(tqdm(prompt_dataloader)):\n",
    "        prompts = prompt_batch['prompt']\n",
    "        generated_output = llm.generate(prompts, max_new_tokens=args.max_new_tokens)\n",
    "        \n",
    "        generated_answers = []\n",
    "        for output in generated_output:\n",
    "            start = output.find(answer_string_in_prompt) + len(answer_string_in_prompt)\n",
    "            response = output[start:].strip()\n",
    "            generated_answers.append(response)\n",
    "\n",
    "        prompt_batch['generated_answer'] = generated_answers\n",
    "        all_info.append(prompt_batch)\n",
    "        \n",
    "        if (idx + 1) % save_every == 0 or (idx + 1) == len(prompt_dataloader):\n",
    "            print(f\"Saving at {idx + 1}...\")\n",
    "            file_name = f\"{saving_dir}/numdoc{num_doc}_gold_at{gold_pos}{answerless_str}_info_{idx+1}.pkl\"\n",
    "            write_pickle(all_info, file_name)\n",
    "            all_info = []\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = parse_arguments({\n",
    "        'output_dir': r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\gen_res_example_llm',\n",
    "        'llm_id': 'google/gemma-2-2b-it',\n",
    "        'model_max_length': 4096,\n",
    "        'gold_position': None,\n",
    "        'num_documents_in_context': 5,\n",
    "        'get_documents_without_answer': True,\n",
    "        'max_new_tokens': 15,\n",
    "        'batch_size': None,\n",
    "        'save_every': 250,\n",
    "    })\n",
    "\n",
    "    print(\"Loading LLM...\")\n",
    "    llm_id = args.llm_id\n",
    "    llm = LLM(\n",
    "        llm_id, device, quantization_bits=4, \n",
    "        model_max_length=args.model_max_length\n",
    "    )\n",
    "    tokenizer = llm.tokenizer\n",
    "    print(\"LLM loaded\")\n",
    "\n",
    "\n",
    "    print(\"Loading corpus and search results...\")\n",
    "    corpus, full_to_subset_idx_map = load_corpus(args)\n",
    "    search_results = load_search_results(args)\n",
    "    print(\"Corpus and search results loaded\")\n",
    "\n",
    "\n",
    "    print(\"Loading prompt dataset...\")\n",
    "    prompt_dataloader = initialize_dataset_and_loader(\n",
    "        args, corpus, full_to_subset_idx_map, search_results, tokenizer\n",
    "    )\n",
    "    print(\"Prompt dataset loaded\")\n",
    "\n",
    "    print_info(args)\n",
    "    generate_and_save(args, llm, prompt_dataloader)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    seed_everything(SEED)\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory:  C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\gen_res_example_llm/gemma-2-2b-it/test/classic/contriever/5_doc\n",
      "I'm using the following files:  ['numdoc5_gold_atNone_answerless_info_250.pkl', 'numdoc5_gold_atNone_answerless_info_500.pkl', 'numdoc5_gold_atNone_answerless_info_750.pkl', 'numdoc5_gold_atNone_answerless_info_1000.pkl', 'numdoc5_gold_atNone_answerless_info_1250.pkl', 'numdoc5_gold_atNone_answerless_info_1500.pkl', 'numdoc5_gold_atNone_answerless_info_1750.pkl', 'numdoc5_gold_atNone_answerless_info_2000.pkl', 'numdoc5_gold_atNone_answerless_info_2250.pkl', 'numdoc5_gold_atNone_answerless_info_2500.pkl', 'numdoc5_gold_atNone_answerless_info_2750.pkl', 'numdoc5_gold_atNone_answerless_info_2889.pkl']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 201\u001b[0m\n\u001b[0;32m    197\u001b[0m     results_df\u001b[38;5;241m.\u001b[39mto_json(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mall_extended.json\u001b[39m\u001b[38;5;124m'\u001b[39m), orient\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecords\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 201\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[14], line 184\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    181\u001b[0m directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;241m.\u001b[39moutput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mllm_folder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mretriever_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mdoc_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDirectory: \u001b[39m\u001b[38;5;124m\"\u001b[39m, directory)\n\u001b[1;32m--> 184\u001b[0m data_df \u001b[38;5;241m=\u001b[39m \u001b[43mload_pickle_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename_prefix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    185\u001b[0m data_path \u001b[38;5;241m=\u001b[39m save_data_to_json(data_df, directory, filename_prefix)\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[14], line 101\u001b[0m, in \u001b[0;36mload_pickle_files\u001b[1;34m(directory, filename_prefix)\u001b[0m\n\u001b[0;32m     97\u001b[0m         data_list\u001b[38;5;241m.\u001b[39mextend(data)\n\u001b[0;32m     99\u001b[0m data_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data_list)\n\u001b[1;32m--> 101\u001b[0m data_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdocument_indices\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdata_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdocument_indices\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert_tensors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28mprint\u001b[39m(data_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdocument_indices\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mhead())\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28mprint\u001b[39m(data_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdocument_indices\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28mtype\u001b[39m)\u001b[38;5;241m.\u001b[39munique())\n",
      "File \u001b[1;32mc:\\Users\\franc\\Documents\\Bridge_the_GAP\\myenv\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4800\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\franc\\Documents\\Bridge_the_GAP\\myenv\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\franc\\Documents\\Bridge_the_GAP\\myenv\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[0;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\franc\\Documents\\Bridge_the_GAP\\myenv\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\franc\\Documents\\Bridge_the_GAP\\myenv\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[14], line 76\u001b[0m, in \u001b[0;36mconvert_tensors\u001b[1;34m(cell)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" Converts tensors in the given cell to lists, if they are tensors. \"\"\"\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(cell, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m---> 76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minner_list\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minner_list\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cell\n",
      "Cell \u001b[1;32mIn[14], line 76\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" Converts tensors in the given cell to lists, if they are tensors. \"\"\"\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(cell, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m---> 76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minner_list\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m inner_list \u001b[38;5;129;01min\u001b[39;00m cell]\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cell\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pickle\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from typing import List, Dict\n",
    "\n",
    "from utils import str2bool\n",
    "from normalize_answers import *\n",
    "\n",
    "\n",
    "def are_answers_matching(prediction: str, ground_truths: List[str]) -> float:\n",
    "    normalized_prediction = normalize_answer(prediction)\n",
    "\n",
    "    for ground_truth in ground_truths:\n",
    "        normalized_ground_truth = normalize_answer(ground_truth)\n",
    "        if normalized_ground_truth in normalized_prediction:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def read_generation_results(file_path: str, df: pd.DataFrame) -> List[Dict]:\n",
    "    data = []\n",
    "    with open(file_path, \"r\") as fin:\n",
    "        file_data = json.load(fin)\n",
    "\n",
    "        for example in file_data:\n",
    "            example_ids = example['example_id']\n",
    "            queries = example['query']\n",
    "            prompts = example['prompt']\n",
    "            document_indices = list(zip(*example['document_indices']))\n",
    "            gold_document_indices = example['gold_document_idx']\n",
    "            generated_answers = example['generated_answer']\n",
    "            prompt_tokens_lens = example['prompt_tokens_len']\n",
    "\n",
    "            for i in range(len(example_ids)):\n",
    "                example_id = example_ids[i]\n",
    "                query = queries[i]\n",
    "                gold_document_idx = gold_document_indices[i]\n",
    "                documents_idx = list(document_indices[i])\n",
    "                generated_answer = generated_answers[i]\n",
    "                prompt = prompts[i]\n",
    "                prompt_tokens_len = prompt_tokens_lens[i]\n",
    "\n",
    "                answers = df[df['example_id'] == int(example_id)].answers.iloc[0]\n",
    "                gold_in_retrieved = False\n",
    "\n",
    "                if int(gold_document_idx) in map(int, documents_idx):\n",
    "                    gold_in_retrieved = True\n",
    "\n",
    "                ans_match_after_norm: bool = are_answers_matching(generated_answer, answers)\n",
    "                ans_in_documents: bool = is_answer_in_text(prompt, answers)\n",
    "                data.append({\n",
    "                    'example_id': int(example_id),\n",
    "                    'query': query,\n",
    "                    'prompt': prompt,\n",
    "                    'document_indices': documents_idx,\n",
    "                    'gold_document_idx': gold_document_idx,\n",
    "                    'generated_answer': generated_answer,\n",
    "                    'answers': answers,\n",
    "                    'ans_match_after_norm': ans_match_after_norm,\n",
    "                    'gold_in_retrieved': gold_in_retrieved,\n",
    "                    'ans_in_documents': ans_in_documents,\n",
    "                    \"prompt_tokens_len\": prompt_tokens_len,\n",
    "                })\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def convert_tensors(cell):\n",
    "    \"\"\" Converts tensors in the given cell to lists, if they are tensors. \"\"\"\n",
    "    if isinstance(cell, list):\n",
    "        return [[t.tolist() if torch.is_tensor(t) else t for t in inner_list] for inner_list in cell]\n",
    "    return cell\n",
    "\n",
    "\n",
    "def extract_number_from_filename(filename: str, pattern: re.Pattern) -> int:\n",
    "    \"\"\" Extracts the number from the filename based on the provided pattern. \"\"\"\n",
    "    match = pattern.search(filename)\n",
    "    return int(match.group(1)) if match else 0\n",
    "\n",
    "\n",
    "def load_pickle_files(directory: str, filename_prefix: str) -> pd.DataFrame:\n",
    "    \"\"\" Loads and concatenates data from all pickle files in the directory with the given prefix. \"\"\"\n",
    "    pattern = re.compile(r'(\\d+).pkl')\n",
    "    files = [f for f in os.listdir(directory) if f.endswith('.pkl') and filename_prefix in f]\n",
    "    files.sort(key=lambda f: extract_number_from_filename(f, pattern))\n",
    "    print(\"I'm using the following files: \", files)\n",
    "\n",
    "    data_list = []\n",
    "    for file in files:\n",
    "        with open(os.path.join(directory, file), 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            data_list.extend(data)\n",
    "    \n",
    "    data_df = pd.DataFrame(data_list)\n",
    "\n",
    "    data_df['document_indices'] = data_df['document_indices'].apply(convert_tensors)\n",
    "    print(data_df['document_indices'].head())\n",
    "    print(data_df['document_indices'].apply(type).unique())\n",
    "\n",
    "    if 'prompt_tokens_len' in data_df.columns:\n",
    "        data_df['prompt_tokens_len'] = data_df['prompt_tokens_len'].apply(lambda x: x.tolist())\n",
    "    return data_df\n",
    "\n",
    "\n",
    "def save_data_to_json(data_df: pd.DataFrame, directory: str, filename_prefix: str):\n",
    "    \"\"\" Saves the given DataFrame to a JSON file. \"\"\"\n",
    "    data_path = os.path.join(directory, f'{filename_prefix}all.json')\n",
    "    # Check if the file already exists\n",
    "    if os.path.exists(data_path):\n",
    "        overwrite = input(f\"File {data_path} already exists. Overwrite? (y/n): \")\n",
    "        if overwrite.lower() != 'y':\n",
    "            print(\"No overwrite.\")\n",
    "\n",
    "            results_df = pd.read_json(f'{directory}/{filename_prefix}all_extended.json')\n",
    "            accuracy = round(results_df['ans_match_after_norm'].sum() / len(results_df), 4)\n",
    "            print(\"ACCURACY: \", accuracy)\n",
    "            return None\n",
    "        \n",
    "    data_df.to_json(data_path, orient='records')\n",
    "    return data_path\n",
    "\n",
    "\n",
    "def get_classic_path(args):\n",
    "    gold_pos = args.gold_position\n",
    "    answerless_str = \"_answerless\" if args.get_documents_without_answer else \"\"\n",
    "\n",
    "    filename_prefix = f'numdoc{args.num_doc}_gold_at{gold_pos}{answerless_str}_info_'\n",
    "    return filename_prefix\n",
    "\n",
    "class DotDict:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "def parse_arguments(custom_args=None):\n",
    "\n",
    "    # Define default values\n",
    "    default_args = {\n",
    "        'output_dir': r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\gen_res_example_llm',\n",
    "        'llm_id': 'google/gemma-2-2b-it',\n",
    "        'model_max_length': 4096,\n",
    "        'gold_position': None,\n",
    "        'num_documents_in_context': 5,\n",
    "        'get_documents_without_answer': True,\n",
    "    }\n",
    "    \n",
    "    # If custom_args is provided, update defaults\n",
    "    if custom_args:\n",
    "        default_args.update(custom_args)\n",
    "\n",
    "    return DotDict(**default_args)\n",
    "\n",
    "def main():\n",
    "    args = parse_arguments({\n",
    "        'output_dir': r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\gen_res_example_llm',\n",
    "        'llm_id': 'google/gemma-2-2b-it',\n",
    "        'gold_position': None,\n",
    "        'num_documents_in_context': 5,\n",
    "        'get_documents_without_answer': True,\n",
    "    }\n",
    "\n",
    "    )\n",
    "    \n",
    "    retriever_str = \"\"\n",
    "    \n",
    "    prompt_type = \"classic\"\n",
    "\n",
    "    retriever_str = \"contriever/\"\n",
    "    args.num_doc = args.num_documents_in_context\n",
    "    filename_prefix = get_classic_path(args)\n",
    "\n",
    "\n",
    "    llm_id = args.llm_id\n",
    "    split = \"test\"\n",
    "    llm_folder = llm_id.split(\"/\")[1] if '/' in llm_id else llm_id\n",
    "    doc_str = f\"{args.num_doc}_doc\"\n",
    "    directory = f'{args.output_dir}/{llm_folder}/{split}/{prompt_type}/{retriever_str}{doc_str}'\n",
    "    print(\"Directory: \", directory)\n",
    "\n",
    "    data_df = load_pickle_files(directory, filename_prefix)\n",
    "    data_path = save_data_to_json(data_df, directory, filename_prefix)\n",
    "    if data_path is None:\n",
    "        return\n",
    "    \n",
    "\n",
    "    df = pd.read_json(r\"C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\test_dataset.json\")\n",
    "\n",
    "    results = read_generation_results(data_path, df)\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    accuracy = round(results_df['ans_match_after_norm'].sum() / len(results_df), 4)\n",
    "    print(\"ACCURACY: \", accuracy)\n",
    "    results_df.to_json(os.path.join(directory, f'{filename_prefix}all_extended.json'), orient='records')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'example_id': '-3290814144789249484',\n",
       " 'query': 'who got the first nobel prize in physics',\n",
       " 'prompt': 'You are given a question and you MUST respond by EXTRACTING the answer (max 5 tokens) from one of the provided documents. If none of the documents contain the answer, respond with NO-RES.\\nDocuments:\\nDocument [2043329](Title: Universology) become the chief proponent of universology today. \"Everything in this universe is part of an uninterrupted sequence of events\" Mohri has said. In 1872 Andrews published \"The Basic Outline of Universology\" which was subtitled \"An introduction to the newly discovered science of the universe, its elementary principles, and the first stages of their development in the special sciences.\" Ilya Romanovich Prigogine (born on January 25, 1917) was a Belgian and American physicist and chemist who was born in Russia and became a Nobel Prize laureate in chemistry. In the book \"Order Out of Chaos: Man\\'s New Dialogue With Nature\", which\\nDocument [1860765](Title: Tsung-Dao Lee) 1956, with her so-called Wu experiment. Lee was the youngest Nobel laureate after World War II until Malala Yousafzai was awarded the Nobel Peace Prize in 2014. He is the fourth youngest Nobel laureate in history after William L. Bragg (who won the prize at 25 with his father William H. Bragg in 1915), Werner Heisenberg (who won in 1932 also at 30) and Malala Yousafzai (awarded at just 17). Lee and Yang were the first Chinese laureates. Since he became a naturalized American citizen in 1962, Lee is also the youngest American ever to have won a Nobel Prize.\\nDocument [439756](Title: University of Chicago) Medal, which is rewarded annually to the best economist under the age of 40, has also been awarded to 4 current members of the university faculty. Notable faculty in physics have included the speed of light calculator A. A. Michelson, elementary charge calculator Robert A. Millikan, discoverer of the Compton Effect Arthur H. Compton, the creator of the first nuclear reactor Enrico Fermi, \"the father of the hydrogen bomb\" Edward Teller, \"one of the most brilliant and productive experimental physicists of the twentieth century\" Luis Walter Alvarez, Murray Gell-Mann who introduced the quark, second female Nobel laureate Maria Goeppert-Mayer, the\\nDocument [3546609](Title: E. C. George Sudarshan) had developed the breakthrough. In 2007, Sudarshan told the \"Hindustan Times\", \"The 2005 Nobel prize for Physics was awarded for my work, but I wasn\\'t the one to get it. Each one of the discoveries that this Nobel was given for work based on my research.\" Sudarshan also commented on not being selected for the 1979 Nobel, \"Steven Weinberg, Sheldon Glashow and Abdus Salam built on work I had done as a 26-year-old student. If you give a prize for a building, shouldn’t the fellow who built the first floor be given the prize before those who built the second\\nDocument [628506](Title: Nobel Prize in Physics) receive a diploma, a medal and a document confirming the prize amount. Nobel Prize in Physics The Nobel Prize in Physics () is a yearly award given by the Royal Swedish Academy of Sciences for those who have made the most outstanding contributions for mankind in the field of physics. It is one of the five Nobel Prizes established by the will of Alfred Nobel in 1895 and awarded since 1901; the others being the Nobel Prize in Chemistry, Nobel Prize in Literature, Nobel Peace Prize, and Nobel Prize in Physiology or Medicine. The first Nobel Prize in Physics was\\nQuestion: who got the first nobel prize in physics\\nAnswer:',\n",
       " 'document_indices': [2043329, 1860765, 439756, 3546609, 628506],\n",
       " 'gold_document_idx': '20994698',\n",
       " 'prompt_tokens_len': 827,\n",
       " 'generated_answer': ['Nobel Prize in Physics']}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the results of generate_answer_llm\n",
    "\n",
    "result_path=r\"C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\gen_res_example_llm\\gemma-2-2b-it\\test\\classic\\contriever\\5_doc\\numdoc5_gold_atNone_answerless_info_250.pkl\"\n",
    "data=read_pickle(result_path)\n",
    "\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/francescociteroni/Documents/Bridge_the_GAP/.conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 94\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     93\u001b[0m     model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt5-small\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Replace with a seq2seq model like T5 or BART\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m     bridge \u001b[38;5;241m=\u001b[39m \u001b[43mBridgeModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the capital of France?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     97\u001b[0m     documents \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     98\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoc_id1\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParis is the capital of France.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoc_id2\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFrance is a country in Europe. Its capital is Paris.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    100\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoc_id3\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe Eiffel Tower is located in Paris, the capital of France.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    101\u001b[0m     }\n",
      "Cell \u001b[0;32mIn[1], line 22\u001b[0m, in \u001b[0;36mBridgeModel.__init__\u001b[0;34m(self, model_id, device, model_max_length)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_max_length \u001b[38;5;241m=\u001b[39m model_max_length\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Initialize the seq2seq model and tokenizer\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_model_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 31\u001b[0m, in \u001b[0;36mBridgeModel._initialize_model_tokenizer\u001b[0;34m(self, model_id)\u001b[0m\n\u001b[1;32m     28\u001b[0m model_config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     29\u001b[0m model_config\u001b[38;5;241m.\u001b[39mmax_seq_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_max_length\n\u001b[0;32m---> 31\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForSeq2SeqLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     38\u001b[0m model\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# Set the model to evaluation mode\u001b[39;00m\n\u001b[1;32m     40\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     41\u001b[0m     model_id, \n\u001b[1;32m     42\u001b[0m     model_max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_max_length,\n\u001b[1;32m     43\u001b[0m     padding_side\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     44\u001b[0m     truncation_side\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     45\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/Bridge_the_GAP/.conda/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/Bridge_the_GAP/.conda/lib/python3.11/site-packages/transformers/modeling_utils.py:3809\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3793\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   3794\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m   3795\u001b[0m     cached_file_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   3796\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m: cache_dir,\n\u001b[1;32m   3797\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforce_download\u001b[39m\u001b[38;5;124m\"\u001b[39m: force_download,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3807\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m: commit_hash,\n\u001b[1;32m   3808\u001b[0m     }\n\u001b[0;32m-> 3809\u001b[0m     resolved_archive_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcached_file_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3811\u001b[0m     \u001b[38;5;66;03m# Since we set _raise_exceptions_for_missing_entries=False, we don't get an exception but a None\u001b[39;00m\n\u001b[1;32m   3812\u001b[0m     \u001b[38;5;66;03m# result when internet is up, the repo and revision exist, but the file does not.\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolved_archive_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m filename \u001b[38;5;241m==\u001b[39m _add_variant(SAFE_WEIGHTS_NAME, variant):\n\u001b[1;32m   3814\u001b[0m         \u001b[38;5;66;03m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Bridge_the_GAP/.conda/lib/python3.11/site-packages/transformers/utils/hub.py:403\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 403\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    418\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m _get_cache_file_to_return(path_or_repo_id, full_filename, cache_dir, revision)\n",
      "File \u001b[0;32m~/Documents/Bridge_the_GAP/.conda/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Bridge_the_GAP/.conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:862\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    842\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[1;32m    843\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[1;32m    844\u001b[0m         local_dir\u001b[38;5;241m=\u001b[39mlocal_dir,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    859\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m    860\u001b[0m     )\n\u001b[1;32m    861\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m    864\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m    866\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m    871\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m    877\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Bridge_the_GAP/.conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:1011\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1009\u001b[0m Path(lock_path)\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[0;32m-> 1011\u001b[0m     \u001b[43m_download_to_tmp_and_move\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.incomplete\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdestination_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1021\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(pointer_path):\n\u001b[1;32m   1022\u001b[0m         _create_symlink(blob_path, pointer_path, new_blob\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Documents/Bridge_the_GAP/.conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:1545\u001b[0m, in \u001b[0;36m_download_to_tmp_and_move\u001b[0;34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download)\u001b[0m\n\u001b[1;32m   1542\u001b[0m         _check_disk_space(expected_size, incomplete_path\u001b[38;5;241m.\u001b[39mparent)\n\u001b[1;32m   1543\u001b[0m         _check_disk_space(expected_size, destination_path\u001b[38;5;241m.\u001b[39mparent)\n\u001b[0;32m-> 1545\u001b[0m     \u001b[43mhttp_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1546\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1547\u001b[0m \u001b[43m        \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1549\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1550\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1551\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1552\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1554\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownload complete. Moving file to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdestination_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1555\u001b[0m _chmod_and_move(incomplete_path, destination_path)\n",
      "File \u001b[0;32m~/Documents/Bridge_the_GAP/.conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:454\u001b[0m, in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[0m\n\u001b[1;32m    452\u001b[0m new_resume_size \u001b[38;5;241m=\u001b[39m resume_size\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 454\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconstants\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDOWNLOAD_CHUNK_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# filter out keep-alive new chunks\u001b[39;49;00m\n\u001b[1;32m    456\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Bridge_the_GAP/.conda/lib/python3.11/site-packages/requests/models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m~/Documents/Bridge_the_GAP/.conda/lib/python3.11/site-packages/urllib3/response.py:1060\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1059\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1060\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1062\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[1;32m   1063\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[0;32m~/Documents/Bridge_the_GAP/.conda/lib/python3.11/site-packages/urllib3/response.py:949\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m amt:\n\u001b[1;32m    947\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer\u001b[38;5;241m.\u001b[39mget(amt)\n\u001b[0;32m--> 949\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    951\u001b[0m flush_decoder \u001b[38;5;241m=\u001b[39m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[1;32m    953\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/Bridge_the_GAP/.conda/lib/python3.11/site-packages/urllib3/response.py:873\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    870\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    872\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 873\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    874\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[1;32m    875\u001b[0m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[1;32m    876\u001b[0m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[1;32m    883\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/Documents/Bridge_the_GAP/.conda/lib/python3.11/site-packages/urllib3/response.py:856\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1()\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m~/Documents/Bridge_the_GAP/.conda/lib/python3.11/http/client.py:473\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[1;32m    472\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[0;32m--> 473\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mread(amt)\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    476\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m~/Documents/Bridge_the_GAP/.conda/lib/python3.11/socket.py:718\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    717\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 718\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    720\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Bridge_the_GAP/.conda/lib/python3.11/ssl.py:1314\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1310\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1311\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1312\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1313\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/Documents/Bridge_the_GAP/.conda/lib/python3.11/ssl.py:1166\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1164\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1166\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1167\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1168\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoConfig, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    ")\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "\n",
    "class BridgeModel:\n",
    "    \"\"\"\n",
    "    Bridge Model for selecting and ranking documents by generating document IDs.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        model_id: str, \n",
    "        device: str = 'cuda', \n",
    "        model_max_length: int = 512\n",
    "    ):\n",
    "        self.device = device\n",
    "        self.model_max_length = model_max_length\n",
    "\n",
    "        # Initialize the seq2seq model and tokenizer\n",
    "        self.model, self.tokenizer = self._initialize_model_tokenizer(model_id)\n",
    "\n",
    "    def _initialize_model_tokenizer(self, model_id: str) -> Tuple[AutoModelForSeq2SeqLM, AutoTokenizer]:\n",
    "        \"\"\"\n",
    "        Initializes the seq2seq model and tokenizer with the given model ID.\n",
    "        \"\"\"\n",
    "        model_config = AutoConfig.from_pretrained(model_id, trust_remote_code=True)\n",
    "        model_config.max_seq_len = self.model_max_length\n",
    "\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "            model_id,\n",
    "            trust_remote_code=True,\n",
    "            config=model_config,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\",\n",
    "        ).to(self.device)\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_id, \n",
    "            model_max_length=self.model_max_length,\n",
    "            padding_side=\"left\",\n",
    "            truncation_side=\"left\"\n",
    "        )\n",
    "        tokenizer.pad_token = tokenizer.eos_token  # Set pad token\n",
    "\n",
    "        return model, tokenizer\n",
    "\n",
    "    def generate(\n",
    "        self, \n",
    "        prompt: str, \n",
    "        max_new_tokens: int = 15\n",
    "    ) -> List[str]:\n",
    "        \"\"\"\n",
    "        Generates the ordered document IDs based on the query and documents.\n",
    "        \"\"\"\n",
    "        # Tokenize input\n",
    "        inputs = self.tokenizer(\n",
    "            prompt, \n",
    "            return_tensors=\"pt\", \n",
    "            max_length=self.model_max_length, \n",
    "            padding=True, \n",
    "            truncation=True\n",
    "        ).to(self.device)\n",
    "\n",
    "        # Generate output\n",
    "        generated_ids = self.model.generate(\n",
    "            **inputs,\n",
    "            do_sample=False,  # Deterministic output\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            repetition_penalty=1.1,\n",
    "            pad_token_id=self.tokenizer.pad_token_id,\n",
    "            eos_token_id=self.tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "        # Decode output\n",
    "        return self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0].split()\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    model_id = \"t5-small\"  # Replace with a seq2seq model like T5 or BART\n",
    "    bridge = BridgeModel(model_id=model_id)\n",
    "\n",
    "    output = bridge.generate(prompt)\n",
    "    print(\"Output document IDs:\", output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
