{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "2\n",
      "NVIDIA GeForce RTX 3080\n",
      "NVIDIA GeForce RTX 3060\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total_examples': 2889, 'examples_with_answer_in_context': 721, 'examples_without_answer_in_context': 2168, 'correct_with_context': 539, 'correct_without_context': 87, 'average_correct_with_context': 0.7475728155339806, 'average_correct_without_context': 0.04012915129151291, 'overall_accuracy': 0.2166839736933195}\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "\n",
    "def analyze_json_responses(my_path):\n",
    "    data = read_json(my_path)  # Legge il file JSON\n",
    "\n",
    "    total_examples = len(data)  # Numero totale di esempi\n",
    "    correct_with_context = 0  # Risposte corrette con risposta nel contesto\n",
    "    correct_without_context = 0  # Risposte corrette senza risposta nel contesto\n",
    "    has_answer_in_context = 0  # Esempi con risposta nel contesto\n",
    "    no_answer_in_context = 0  # Esempi senza risposta nel contesto\n",
    "    total_correct = 0  # Totale delle risposte corrette\n",
    "\n",
    "    for entry in data:\n",
    "        ans_in_documents = entry.get(\"ans_in_documents\", False)\n",
    "        ans_match_after_norm = entry.get(\"ans_match_after_norm\", False)\n",
    "\n",
    "        if ans_in_documents:\n",
    "            has_answer_in_context += 1\n",
    "            if ans_match_after_norm:\n",
    "                correct_with_context += 1\n",
    "        else:\n",
    "            no_answer_in_context += 1\n",
    "            if ans_match_after_norm:\n",
    "                correct_without_context += 1\n",
    "\n",
    "        # Conta ogni risposta corretta\n",
    "        if ans_match_after_norm:\n",
    "            total_correct += 1\n",
    "\n",
    "    # Calcola le medie\n",
    "    avg_correct_with_context = correct_with_context / has_answer_in_context if has_answer_in_context > 0 else 0\n",
    "    avg_correct_without_context = correct_without_context / no_answer_in_context if no_answer_in_context > 0 else 0\n",
    "    overall_accuracy = total_correct / total_examples if total_examples > 0 else 0\n",
    "\n",
    "    return {\n",
    "        \"total_examples\": total_examples,\n",
    "        \"examples_with_answer_in_context\": has_answer_in_context,\n",
    "        \"examples_without_answer_in_context\": no_answer_in_context,\n",
    "        \"correct_with_context\": correct_with_context,\n",
    "        \"correct_without_context\": correct_without_context,\n",
    "        \"average_correct_with_context\": avg_correct_with_context,\n",
    "        \"average_correct_without_context\": avg_correct_without_context,\n",
    "        \"overall_accuracy\": overall_accuracy\n",
    "    }\n",
    "\n",
    "\n",
    "# Esempio di utilizzo\n",
    "path = r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\gen_res_example_llm\\nq\\gemma-2-2b-it\\test\\retrieved\\contriever\\1_doc\\numdoc1_retr1_template_info_all_extended.json'\n",
    "result = analyze_json_responses(path)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LLM...\n",
      "LLM loaded\n",
      "Loading corpus and search results...\n",
      "Corpus and search results loaded\n",
      "Loading prompt dataset...\n",
      "Prompt dataset loaded\n",
      "INFO:\n",
      "DATA: C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\10k_train_dataset.json\n",
      "USE TEST: False\n",
      "MODEL: google/flan-t5-large\n",
      "MODEL MAX LENGTH: 4096\n",
      "MAX NEW TOKENS: 50\n",
      "USE MODEL CHAT TEMPLATE: False\n",
      "TASK WITH PROOF: False\n",
      "GOLD POSITION: None\n",
      "NUM DOCUMENTS IN CONTEXT: 3\n",
      "BATCH SIZE: None\n",
      "SAVE EVERY: 250\n",
      "Saving DataLoader contents to JSON...\n",
      "DataLoader contents saved to C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\dataloader_contents.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from typing import Tuple, Dict, Optional\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "from utils import *\n",
    "from bgm import BGM\n",
    "from default_prompts import *\n",
    "from prompt_dataset import PromptDataset\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "device = torch.device(f\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "warnings.filterwarnings('ignore')\n",
    "SEED=10\n",
    "\n",
    "info = {\n",
    "    \"nq_bgm\": {\n",
    "        \"train\": {\n",
    "            \"data_path\": r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\10k_train_dataset.json',\n",
    "            \"contriever_search_results_path\": r\"C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\processed\\contriever_search_results_at150.pkl\",\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "def save_dataloader_to_json(dataloader, output_file, num_examples=15):\n",
    "    all_batches = []\n",
    "\n",
    "    print(\"Saving DataLoader contents to JSON...\")\n",
    "    for idx, batch in enumerate(dataloader):\n",
    "        if idx >= num_examples:  # Stop after saving the specified number of examples\n",
    "            break\n",
    "\n",
    "        batch_dict = {}\n",
    "        for key, value in batch.items():\n",
    "            # Convert tensors to lists for JSON serialization\n",
    "            if isinstance(value, torch.Tensor):\n",
    "                batch_dict[key] = value.tolist()\n",
    "            else:\n",
    "                batch_dict[key] = value\n",
    "        all_batches.append(batch_dict)\n",
    "    \n",
    "    # Save the entire list of dictionaries to a JSON file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_batches, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"DataLoader contents saved to {output_file}\")\n",
    "\n",
    "class DotDict:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "def parse_arguments(custom_args=None):\n",
    "    \"\"\"\n",
    "    Mimics argparse to parse arguments for LLM generation. Accepts custom arguments as a dictionary for notebooks.\n",
    "    \"\"\"\n",
    "    # Define default values\n",
    "    default_args = {\n",
    "        'output_dir': r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\gen_id_document_bgm',\n",
    "        'llm_id': 'google/flan-t5-large',\n",
    "        'dataset': 'nq_bgm',\n",
    "        'model_max_length': 4096,\n",
    "        'quantization_bits': 4,\n",
    "        'use_model_chat_template': False, \n",
    "        'gold_position': None,\n",
    "        'num_retrieved_documents': 5,\n",
    "        'use_test': False,\n",
    "        'max_new_tokens': 50,\n",
    "        'use_task_with_proof': False,\n",
    "        'batch_size': None,\n",
    "        'save_every': 250,\n",
    "    }\n",
    "\n",
    "    # If custom_args is provided, update defaults\n",
    "    if custom_args:\n",
    "        default_args.update(custom_args)\n",
    "\n",
    "    # Perform validation\n",
    "    if default_args['num_retrieved_documents'] is None:\n",
    "        raise ValueError(\"'num_retrieved_documents' must be specified.\")\n",
    "    if default_args['num_retrieved_documents'] <= 0:\n",
    "        raise ValueError(\"'num_retrieved_documents' must be a positive integer.\")\n",
    "    if default_args['gold_position'] is not None:\n",
    "        if (default_args['gold_position'] < 0 or \n",
    "            default_args['gold_position'] >= default_args['num_retrieved_documents']):\n",
    "            raise ValueError(\"'gold_position' must be within the range of 'num_retrieved_documents'.\")\n",
    "\n",
    "    return DotDict(**default_args)\n",
    "\n",
    "\n",
    "def load_corpus(\n",
    "    args: argparse.Namespace\n",
    ") -> Tuple[List[Dict], Optional[Dict[int, int]]]:\n",
    "    \n",
    "    # Corpus with documents from Contriever\n",
    "    corpus, full_to_subset_idx_map = read_corpus_with_contriever()\n",
    "\n",
    "    return corpus, full_to_subset_idx_map\n",
    "\n",
    "def load_search_results(args: argparse.Namespace) -> List[Tuple[List[int], List[float]]]:\n",
    "\n",
    "    search_results_path = info[args.dataset][args.split]['contriever_search_results_path']\n",
    "    retriever_search_results = read_pickle(search_results_path)\n",
    "\n",
    "    return retriever_search_results\n",
    "\n",
    "\n",
    "def get_prompt_template(args: argparse.Namespace):\n",
    "    prompt_configuration = args.dataset\n",
    "    if args.use_model_chat_template:\n",
    "        chat_task_template_str = chat_task_templates[args.llm_id]['template']\n",
    "        \n",
    "        task_instruction = task_instructions[prompt_configuration]\n",
    "\n",
    "        prompt_template = apply_chat_task_template(chat_task_template_str, task_instruction)\n",
    "    else:\n",
    "        task_template = task_templates[prompt_configuration]\n",
    "\n",
    "        prompt_template = task_template.create_prompt_template()\n",
    "\n",
    "    return prompt_template\n",
    "\n",
    "\n",
    "def initialize_dataset_and_loader(\n",
    "    args: argparse.Namespace, \n",
    "    corpus: List[Dict], \n",
    "    full_to_subset_idx_map: Optional[Dict[int, int]], \n",
    "    retriever_search_results: List[Tuple[List[int], List[float]]], \n",
    "    tokenizer: PreTrainedTokenizer\n",
    ") -> DataLoader:\n",
    "    \n",
    "    prompt_template = get_prompt_template(args)\n",
    "    \n",
    "    prompt_ds = PromptDataset(\n",
    "        corpus=corpus, data_path=info[args.dataset][args.split]['data_path'], \n",
    "        tokenizer=tokenizer, \n",
    "        max_tokenized_length=args.model_max_length - 2, \n",
    "        search_results=retriever_search_results,\n",
    "        prompt_template=prompt_template,\n",
    "        full_to_subset_idx_map=full_to_subset_idx_map,\n",
    "        do_normalize_query=True, \n",
    "        num_documents_in_context=args.num_retrieved_documents,\n",
    "        gold_position=args.gold_position, # None in these experiments\n",
    "    )\n",
    "        \n",
    "    prompt_dataloader = DataLoader(\n",
    "        prompt_ds,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=8,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    return prompt_dataloader\n",
    "\n",
    "\n",
    "def print_info(args: argparse.Namespace):\n",
    "    print(\"INFO:\")    \n",
    "    print(f\"DATA: {info[args.dataset][args.split]['data_path']}\")\n",
    "    print(f\"USE TEST: {args.use_test}\")\n",
    "    print(f\"MODEL: {args.llm_id}\")\n",
    "    print(f\"MODEL MAX LENGTH: {args.model_max_length}\")\n",
    "    print(f'MAX NEW TOKENS: {args.max_new_tokens}')\n",
    "    print(f\"USE MODEL CHAT TEMPLATE: {args.use_model_chat_template}\")\n",
    "    print(f\"TASK WITH PROOF:\", args.use_task_with_proof)\n",
    "    print(f\"GOLD POSITION: {args.gold_position}\")\n",
    "    print(f\"NUM DOCUMENTS IN CONTEXT: {args.num_retrieved_documents}\")\n",
    "    print(f\"BATCH SIZE: {args.batch_size}\")\n",
    "    print(f\"SAVE EVERY: {args.save_every}\")\n",
    "\n",
    "\n",
    "def extract_generate_answers(\n",
    "    args: argparse.Namespace, \n",
    "    generated_output: List[str]\n",
    ") -> List[str]:\n",
    "    answer_prefix = \"Answer:\"\n",
    "    if args.use_model_chat_template:\n",
    "        answer_prefix = re.escape(chat_task_templates[args.llm_id]['answer_prefix'])\n",
    "\n",
    "    generated_answers = []\n",
    "    for output in generated_output:\n",
    "        matches = list(re.finditer(answer_prefix, output))\n",
    "        match_idx = 0\n",
    "\n",
    "        # When using the proof there is a one-shot example that already \n",
    "        # contains the string \"Answer:\". Thus, we should get the second (match_idx=1) match.\n",
    "        if args.use_model_chat_template and answer_prefix != \"Answer:\":\n",
    "            match_idx = 0\n",
    " \n",
    "        answer_end = matches[match_idx].end()\n",
    "        response = output[answer_end:].strip()\n",
    "        generated_answers.append(response)\n",
    "    \n",
    "    return generated_answers\n",
    "\n",
    "\n",
    "def BGMTraining(\n",
    "    args: argparse.Namespace, \n",
    "    prompt_ds: PromptDataset,\n",
    "    llm: BGM, \n",
    "    prompt_dataloader: DataLoader\n",
    "):\n",
    "    # Info from arguments\n",
    "    llm_id = args.llm_id\n",
    "    num_doc = args.num_retrieved_documents\n",
    "    save_every = args.save_every\n",
    "    retriever_str = \"contriever\" \n",
    "    chat_template_str = \"_template\" if args.use_model_chat_template else \"\"\n",
    "    prompt_type = \"retrieved_proof\" if args.use_task_with_proof else \"retrieved\"\n",
    "\n",
    "    # Create the saving directory\n",
    "    llm_folder = llm_id.split(\"/\")[1] if '/' in llm_id else llm_id\n",
    "    saving_dir = f\"{args.output_dir}/{args.dataset}/{llm_folder}/{args.split}/{prompt_type}/{retriever_str}/{num_doc}_doc\"\n",
    "    os.makedirs(saving_dir, exist_ok=True)\n",
    "\n",
    "    all_info = []  \n",
    "    for idx, prompt_batch in enumerate(tqdm(prompt_dataloader)):\n",
    "        prompts = prompt_batch['prompt']\n",
    "        example_id = prompt_batch['example_id']\n",
    "        prompts = prompt_batch['prompt']\n",
    "        query = prompt_batch['query']\n",
    "        document_indices=prompt_batch['document_indices']\n",
    "        \n",
    "        for doc_idx in document_indices:\n",
    "            \n",
    "            candidate_docs += doc_idx\n",
    "\n",
    "            formatted_docs, _ = prompt_ds._get_documents_from_indices(candidate_docs)\n",
    "\n",
    "            if '\\nAnswer:' not in candidate_prompt:\n",
    "                candidate_prompt += '\\nAnswer:'\n",
    "\n",
    "            \n",
    "\n",
    "        generated_output = llm.generate(\n",
    "            prompts, \n",
    "            max_new_tokens=args.max_new_tokens\n",
    "        )\n",
    "\n",
    "        generated_answers = extract_generate_answers(args, generated_output)\n",
    "        prompt_batch['generated_answer'] = generated_answers\n",
    "        \n",
    "        all_info.append(prompt_batch)\n",
    "        '''\n",
    "        if (idx + 1) % save_every == 0 or (idx + 1) == len(prompt_dataloader):\n",
    "            print(f\"Saving at {idx + 1}...\")\n",
    "            file_name = f\"{saving_dir}/numdoc{num_doc}_retr{args.num_retrieved_documents}{chat_template_str}_info_{idx+1}.pkl\"\n",
    "            write_pickle(all_info, file_name)\n",
    "            all_info = []\n",
    "        '''\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = parse_arguments()\n",
    "\n",
    "    args.split = \"test\" if args.use_test else \"train\"\n",
    "\n",
    "    print(\"Loading LLM...\")\n",
    "    llm_id = args.llm_id\n",
    "    bgm = BGM(\n",
    "        llm_id, device,  \n",
    "        model_max_length=args.model_max_length\n",
    "    )\n",
    "    tokenizer = bgm.tokenizer\n",
    "    print(\"LLM loaded\")\n",
    "\n",
    "\n",
    "    print(\"Loading corpus and search results...\")\n",
    "    corpus, full_to_subset_idx_map = load_corpus(args)\n",
    "    retriever_search_results = load_search_results(args)\n",
    "    print(\"Corpus and search results loaded\")\n",
    "\n",
    "\n",
    "    print(\"Loading prompt dataset...\")\n",
    "    prompt_ds, prompt_dataloader = initialize_dataset_and_loader(\n",
    "        args, corpus, full_to_subset_idx_map, \n",
    "        retriever_search_results, tokenizer\n",
    "    )\n",
    "    print(\"Prompt dataset loaded\")\n",
    "\n",
    "    print_info(args)\n",
    "\n",
    "    #output_json_path = r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\dataloader_contents.json'\n",
    "    #save_dataloader_to_json(prompt_dataloader, output_json_path, num_examples=15)\n",
    "        \n",
    "    BGMTraining(args, prompt_ds, bgm, prompt_dataloader)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    seed_everything(SEED)\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File aggiornato salvato in: C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\gen_ids_document_training_set_bgm\\nq_training\\gemma-2-2b-it\\train\\retrieved\\contriever\\5_doc\\numdoc5_retr5_template_info_all_extended_updated.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def match_example_ids(file1_path, file2_path, output_path):\n",
    "    \"\"\"\n",
    "    Modifica il file1 aggiungendo l'example_id da file2 quando query e question corrispondono.\n",
    "\n",
    "    Args:\n",
    "        file1_path (str): Percorso al file JSON di input 1.\n",
    "        file2_path (str): Percorso al file JSON di input 2.\n",
    "        output_path (str): Percorso al file JSON di output aggiornato.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Caricamento dei file JSON\n",
    "        with open(file1_path, 'r') as f1:\n",
    "            file1 = json.load(f1)\n",
    "\n",
    "        with open(file2_path, 'r') as f2:\n",
    "            file2 = json.load(f2)\n",
    "\n",
    "        # Creazione di un dizionario per mappare le domande agli example_id\n",
    "        question_to_example_id = {item['question']: item['example_id'] for item in file2}\n",
    "\n",
    "        # Modifica del primo file\n",
    "        for entry in file1:\n",
    "            query = entry.get('query')\n",
    "            if query in question_to_example_id:\n",
    "                entry['example_id'] = question_to_example_id[query]\n",
    "\n",
    "        # Salvataggio del file aggiornato\n",
    "        with open(output_path, 'w') as f1_updated:\n",
    "            json.dump(file1, f1_updated, indent=4)\n",
    "\n",
    "        print(f\"File aggiornato salvato in: {output_path}\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Errore: {e}\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Errore nel parsing del file JSON: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Errore imprevisto: {e}\")\n",
    "\n",
    "def update_queries_with_document_indices(file1_path, file2_path, output_path):\n",
    "    # Carica i dati dai file JSON\n",
    "    with open(file1_path, 'r', encoding='utf-8') as f1, open(file2_path, 'r', encoding='utf-8') as f2:\n",
    "        file1_data = json.load(f1)\n",
    "        file2_data = json.load(f2)\n",
    "\n",
    "    # Crea un dizionario per mappare le query ai document_indices di File 2\n",
    "    query_to_indices = {\n",
    "        entry['query']: entry.get('document_indices', [])\n",
    "        for entry in file2_data\n",
    "    }\n",
    "\n",
    "    # Aggiorna File 1 aggiungendo i document_indices associati alle query\n",
    "    for entry in file1_data:\n",
    "        query = entry['query']\n",
    "        if query in query_to_indices:\n",
    "            entry['document_indices'] = query_to_indices[query]\n",
    "\n",
    "    # Salva il risultato in un nuovo file JSON\n",
    "    with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "        json.dump(file1_data, output_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "\n",
    "path_output=r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\gen_ids_document_training_set_bgm\\nq_training\\gemma-2-2b-it\\train\\retrieved\\contriever\\5_doc\\numdoc5_retr5_template_info_all_extended_updated.json'\n",
    "file_da_modificare = r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\gen_ids_document_training_set_bgm\\nq_training\\gemma-2-2b-it\\train\\retrieved\\contriever\\5_doc\\numdoc5_retr5_template_info_all_extended.json'\n",
    "file_di_confronto = r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\10k_train_dataset.json'\n",
    "\n",
    "match_example_ids(file_da_modificare, file_di_confronto, path_output)\n",
    "\n",
    "update_queries_with_document_indices(r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\gen_ids_document_training_set_bgm\\nq_training\\gemma-2-2b-it\\train\\retrieved\\contriever\\5_doc\\numdoc5_retr5_template_info_all_extended_updated.json', r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\gen_ids_document_training_set_bgm\\nq_training\\gemma-2-2b-it\\train\\retrieved\\contriever\\5_doc\\numdoc5_retr5_template_info_all_extended_updated.json', r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\gen_ids_document_training_set_bgm\\nq_training\\gemma-2-2b-it\\train\\retrieved\\contriever\\5_doc\\numdoc5_retr5_template_info_all_extended_updated_last.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "# Percentuali per ogni caso\n",
    "percentages = {\n",
    "    \"case_1_single_doc\": 0.1,\n",
    "    \"case_2_multiple_docs\": 0.2,\n",
    "    \"case_3_no_docs\": 0.1,\n",
    "    \"case_4_less_docs\": 0.4,\n",
    "    \"case_5_reranking\": 0.2,\n",
    "}\n",
    "\n",
    "# Task instruction da aggiungere a ogni query\n",
    "task_instruction = \"Output only the document IDs relevant to the query. Use this format: [ID1, ID2, ...].\"\n",
    "\n",
    "def process_data(input_file, output_file):\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        examples = json.load(f)\n",
    "\n",
    "    dataset = []\n",
    "    \n",
    "    # Shuffle examples to ensure random sampling\n",
    "    random.shuffle(examples)\n",
    "\n",
    "    # Total examples to be processed for each case\n",
    "    total_examples = len(examples)\n",
    "    case_limits = {case: int(total_examples * perc) for case, perc in percentages.items()}\n",
    "    case_counters = {case: 0 for case in percentages}\n",
    "\n",
    "    for example in examples:\n",
    "        if all(count >= case_limits[case] for case, count in case_counters.items()):\n",
    "            break  # Stop if all case limits are met\n",
    "        \n",
    "        query = f\"Task Instruction: {task_instruction}\\nQuestion:{example['query']}\"  # Aggiunge la task instruction\n",
    "        retrieved_docs = example[\"document_indices\"]\n",
    "        selected_docs = example[\"selected_documents\"]\n",
    "        are_answer = example[\"are_answer\"]\n",
    "\n",
    "        # Case 1: Single document correct answer\n",
    "        if are_answer and len(selected_docs) == 1 and case_counters[\"case_1_single_doc\"] < case_limits[\"case_1_single_doc\"]:\n",
    "            dataset.append({\n",
    "                \"input\": {\n",
    "                    \"query\": query,\n",
    "                    \"retrieved_docs\": retrieved_docs,\n",
    "                },\n",
    "                \"output\": selected_docs,\n",
    "            })\n",
    "            case_counters[\"case_1_single_doc\"] += 1\n",
    "\n",
    "        # Case 2: Multiple documents correct answer\n",
    "        elif are_answer and len(selected_docs) > 1 and case_counters[\"case_2_multiple_docs\"] < case_limits[\"case_2_multiple_docs\"]:\n",
    "            dataset.append({\n",
    "                \"input\": {\n",
    "                    \"query\": query,\n",
    "                    \"retrieved_docs\": retrieved_docs,\n",
    "                },\n",
    "                \"output\": selected_docs,\n",
    "            })\n",
    "            case_counters[\"case_2_multiple_docs\"] += 1\n",
    "\n",
    "        # Case 3: No documents correct answer\n",
    "        elif are_answer and len(selected_docs) == 0 and case_counters[\"case_3_no_docs\"] < case_limits[\"case_3_no_docs\"]:\n",
    "            dataset.append({\n",
    "                \"input\": {\n",
    "                    \"query\": query,\n",
    "                    \"retrieved_docs\": [],\n",
    "                },\n",
    "                \"output\": [],\n",
    "            })\n",
    "            case_counters[\"case_3_no_docs\"] += 1\n",
    "\n",
    "        # Case 4: Input and output unchanged\n",
    "        elif are_answer and len(selected_docs) > 2 and case_counters[\"case_4_less_docs\"] < case_limits[\"case_4_less_docs\"]:\n",
    "            dataset.append({\n",
    "                \"input\": {\n",
    "                    \"query\": query,\n",
    "                    \"retrieved_docs\": selected_docs,\n",
    "                },\n",
    "                \"output\": selected_docs,\n",
    "            })\n",
    "            case_counters[\"case_4_less_docs\"] += 1\n",
    "\n",
    "        # Case 5: Reranking\n",
    "        elif are_answer and len(selected_docs) > 2 and case_counters[\"case_5_reranking\"] < case_limits[\"case_5_reranking\"]:\n",
    "            reranked_docs = random.sample(selected_docs, len(selected_docs))  # Randomize order\n",
    "            dataset.append({\n",
    "                \"input\": {\n",
    "                    \"query\": query,\n",
    "                    \"retrieved_docs\": reranked_docs,\n",
    "                },\n",
    "                \"output\": selected_docs,\n",
    "            })\n",
    "            case_counters[\"case_5_reranking\"] += 1\n",
    "\n",
    "    # Save the dataset to a file\n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(dataset, f, indent=4)\n",
    "\n",
    "# Path to input and output files\n",
    "input_file = r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\gen_ids_document_training_set_bgm\\nq_training\\gemma-2-2b-it\\train\\retrieved\\contriever\\5_doc\\numdoc5_retr5_info_all_extended_training_set.json'\n",
    "output_file = r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\training_dataset.json'\n",
    "\n",
    "process_data(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Totale esempi nel file di input: 3000\n",
      "Esempi con 'are_answer=True': 1233\n",
      "Esempi con 'selected_documents == 0': 366\n",
      "Esempi con 'selected_documents == 1': 736\n",
      "Esempi con 'selected_documents > 1': 131\n",
      "Distribuzione pianificata degli esempi nel dataset creato:\n",
      "case_1_single_doc: 51\n",
      "case_2_multiple_docs: 52\n",
      "case_3_no_docs: 36\n",
      "case_4_multi_doc_unchanged: 45\n",
      "case_5_reranking: 65\n",
      "case_6_single_doc_unchanged: 36\n",
      "Esempi effettivamente inclusi nel dataset creato:\n",
      "case_1_single_doc: 51\n",
      "case_2_multiple_docs: 52\n",
      "case_3_no_docs: 36\n",
      "case_4_multi_doc_unchanged: 45\n",
      "case_5_reranking: 34\n",
      "case_6_single_doc_unchanged: 36\n",
      "Totale degli Esempi inclusi nel training dataset creato: 254\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "# Percentuali per ogni caso\n",
    "percentages = {\n",
    "    \"case_1_single_doc\": 0.07,\n",
    "    \"case_2_multiple_docs\": 0.4,\n",
    "    \"case_3_no_docs\": 0.1,\n",
    "    \"case_4_multi_doc_unchanged\": 0.35,\n",
    "    \"case_5_reranking\": 0.5,\n",
    "    \"case_6_single_doc_unchanged\": 0.05,\n",
    "}\n",
    "\n",
    "# Task instruction da aggiungere a ogni query\n",
    "task_instruction = \"Output only the document IDs relevant to the query. Use this format: [ID1, ID2, ...].\"\n",
    "\n",
    "def process_data(input_file, output_file):\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        examples = json.load(f)\n",
    "\n",
    "    # Filtra gli esempi con are_answer = true\n",
    "    valid_examples = [ex for ex in examples if ex[\"are_answer\"] is True]\n",
    "\n",
    "    print(f\"Totale esempi nel file di input: {len(examples)}\")\n",
    "    print(f\"Esempi con 'are_answer=True': {len(valid_examples)}\")\n",
    "\n",
    "    # Raggruppa per numero di selected_documents\n",
    "    grouped_examples = {\n",
    "        \"len_0\": [ex for ex in valid_examples if len(ex[\"selected_documents\"]) == 0],\n",
    "        \"len_1\": [ex for ex in valid_examples if len(ex[\"selected_documents\"]) == 1],\n",
    "        \"len_gt_1\": [ex for ex in valid_examples if len(ex[\"selected_documents\"]) > 1],\n",
    "    }\n",
    "\n",
    "    print(f\"Esempi con 'selected_documents == 0': {len(grouped_examples['len_0'])}\")\n",
    "    print(f\"Esempi con 'selected_documents == 1': {len(grouped_examples['len_1'])}\")\n",
    "    print(f\"Esempi con 'selected_documents > 1': {len(grouped_examples['len_gt_1'])}\")\n",
    "\n",
    "    # Calcola le suddivisioni per ogni gruppo\n",
    "    group_case_limits = {\n",
    "        \"case_1_single_doc\": int(len(grouped_examples[\"len_1\"]) * percentages[\"case_1_single_doc\"]),\n",
    "        \"case_2_multiple_docs\": int(len(grouped_examples[\"len_gt_1\"]) * percentages[\"case_2_multiple_docs\"]),\n",
    "        \"case_3_no_docs\": int(len(grouped_examples[\"len_0\"]) * percentages[\"case_3_no_docs\"]),\n",
    "        \"case_4_multi_doc_unchanged\": int(len(grouped_examples[\"len_gt_1\"]) * percentages[\"case_4_multi_doc_unchanged\"]),\n",
    "        \"case_5_reranking\": int(len(grouped_examples[\"len_gt_1\"]) * percentages[\"case_5_reranking\"]),\n",
    "        \"case_6_single_doc_unchanged\": int(len(grouped_examples[\"len_1\"]) * percentages[\"case_6_single_doc_unchanged\"]),\n",
    "    }\n",
    "\n",
    "    print(\"Distribuzione pianificata degli esempi nel dataset creato:\")\n",
    "    for case, limit in group_case_limits.items():\n",
    "        print(f\"{case}: {limit}\")\n",
    "\n",
    "    dataset = []\n",
    "    case_counters = {case: 0 for case in group_case_limits}\n",
    "\n",
    "    # Processa gli esempi\n",
    "    for example in valid_examples:\n",
    "        query = f\"Task Instruction: {task_instruction}\\nQuestion:{example['query']}\"\n",
    "        retrieved_docs = example[\"document_indices\"]\n",
    "        selected_docs = example[\"selected_documents\"]\n",
    "\n",
    "        # Case 1: Single document correct answer\n",
    "        if len(selected_docs) == 1 and case_counters[\"case_1_single_doc\"] < group_case_limits[\"case_1_single_doc\"]:\n",
    "            dataset.append({\n",
    "                \"input\": {\n",
    "                    \"query\": query,\n",
    "                    \"retrieved_docs\": retrieved_docs,\n",
    "                },\n",
    "                \"output\": selected_docs,\n",
    "            })\n",
    "            case_counters[\"case_1_single_doc\"] += 1\n",
    "\n",
    "        # Case 2: Multiple documents correct answer\n",
    "        elif len(selected_docs) > 1 and case_counters[\"case_2_multiple_docs\"] < group_case_limits[\"case_2_multiple_docs\"]:\n",
    "            dataset.append({\n",
    "                \"input\": {\n",
    "                    \"query\": query,\n",
    "                    \"retrieved_docs\": retrieved_docs,\n",
    "                },\n",
    "                \"output\": selected_docs,\n",
    "            })\n",
    "            case_counters[\"case_2_multiple_docs\"] += 1\n",
    "\n",
    "        # Case 3: No documents correct answer\n",
    "        elif len(selected_docs) == 0 and case_counters[\"case_3_no_docs\"] < group_case_limits[\"case_3_no_docs\"]:\n",
    "            dataset.append({\n",
    "                \"input\": {\n",
    "                    \"query\": query,\n",
    "                    \"retrieved_docs\": [],\n",
    "                },\n",
    "                \"output\": [],\n",
    "            })\n",
    "            case_counters[\"case_3_no_docs\"] += 1\n",
    "\n",
    "        # Case 4: Input and output unchanged for multiple docs\n",
    "        elif len(selected_docs) > 1 and case_counters[\"case_4_multi_doc_unchanged\"] < group_case_limits[\"case_4_multi_doc_unchanged\"]:\n",
    "            dataset.append({\n",
    "                \"input\": {\n",
    "                    \"query\": query,\n",
    "                    \"retrieved_docs\": selected_docs,\n",
    "                },\n",
    "                \"output\": selected_docs,\n",
    "            })\n",
    "            case_counters[\"case_4_multi_doc_unchanged\"] += 1\n",
    "\n",
    "        # Case 6: Input and output unchanged for single doc\n",
    "        elif len(selected_docs) == 1 and case_counters[\"case_6_single_doc_unchanged\"] < group_case_limits[\"case_6_single_doc_unchanged\"]:\n",
    "            dataset.append({\n",
    "                \"input\": {\n",
    "                    \"query\": query,\n",
    "                    \"retrieved_docs\": selected_docs,\n",
    "                },\n",
    "                \"output\": selected_docs,\n",
    "            })\n",
    "            case_counters[\"case_6_single_doc_unchanged\"] += 1\n",
    "\n",
    "        # Case 5: Reranking\n",
    "        elif len(selected_docs) > 1 and case_counters[\"case_5_reranking\"] < group_case_limits[\"case_5_reranking\"]:\n",
    "            reranked_docs = selected_docs[:]\n",
    "            while reranked_docs == selected_docs:  # Garantisce che l'ordine sia diverso\n",
    "                reranked_docs = random.sample(selected_docs, len(selected_docs))\n",
    "            dataset.append({\n",
    "                \"input\": {\n",
    "                    \"query\": query,\n",
    "                    \"retrieved_docs\": reranked_docs,\n",
    "                },\n",
    "                \"output\": selected_docs,\n",
    "            })\n",
    "            case_counters[\"case_5_reranking\"] += 1\n",
    "\n",
    "    print(\"Esempi effettivamente inclusi nel dataset creato:\")\n",
    "    tot=0\n",
    "    for case, count in case_counters.items():\n",
    "        tot += count\n",
    "        print(f\"{case}: {count}\")\n",
    "\n",
    "    print(f\"Totale degli Esempi inclusi nel training dataset creato: {tot}\")\n",
    "    \n",
    "\n",
    "    # Save the dataset to a file\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(dataset, f, indent=4)\n",
    "\n",
    "# Path to input and output files\n",
    "input_file = r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\gen_ids_document_training_set_bgm\\nq_training\\gemma-2-2b-it\\train\\retrieved\\contriever\\5_doc\\numdoc5_retr5_info_all_extended_training_set.json'\n",
    "output_file = r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\training_dataset.json'\n",
    "\n",
    "process_data(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\Documents\\Bridge_the_GAP\\myenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BGM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BGM loaded\n",
      "Loading LLM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM loaded\n",
      "Loading corpus and search results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (778 > 15). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus and search results loaded\n",
      "Loading prompt dataset...\n",
      "Prompt dataset loaded\n",
      "INFO:\n",
      "DATA: C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\test_dataset.json\n",
      "TASK INSTRUCTION: Output only the document IDs relevant to the query. Use this format: [Id_1, Id_2, ...].\n",
      "USE TEST: True\n",
      "BGM MODEL: meta-llama/Llama-3.2-1B\n",
      "LLm MODEL: google/gemma-2-2b-it\n",
      "MODEL MAX LENGTH: 4096\n",
      "MAX NEW TOKENS: 50\n",
      "USE MODEL CHAT TEMPLATE: False\n",
      "TASK WITH PROOF: False\n",
      "GOLD POSITION: None\n",
      "NUM DOCUMENTS IN CONTEXT: 5\n",
      "BATCH SIZE: None\n",
      "SAVE EVERY: 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2889 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Example 1:\n",
      "\n",
      "Query originale trovata: who got the first nobel prize in physics\n",
      "ID disponibili nel prompt: ['628506', '3546609', '439756', '1860765', '2043329']\n",
      "ID validi generati dal modello: ['628506', '2043329', '1860765']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/2889 [01:51<89:07:43, 111.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La risposta generata dal modello e':\n",
      "['The provided text does not state who received the first Nobel Prize in Physics.']\n",
      "Processing Example 2:\n",
      "\n",
      "Query originale trovata: when is the next deadpool movie being released\n",
      "ID disponibili nel prompt: ['3201523', '3246420', '10435180', '3201509', '3180056']\n",
      "ID validi generati dal modello: ['3201523', '3180056']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/2889 [02:55<67:03:36, 83.62s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La risposta generata dal modello e':\n",
      "['The provided documents do not contain information about the release date of the next Deadpool movie.']\n",
      "Processing Example 3:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/2889 [03:21<80:58:00, 100.96s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 446\u001b[0m\n\u001b[0;32m    444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    445\u001b[0m     seed_everything(SEED)\n\u001b[1;32m--> 446\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 440\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    437\u001b[0m print_info(args)\n\u001b[0;32m    439\u001b[0m training_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mfranc\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDocuments\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mBridge_the_GAP\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mSFT_training_bgm\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmeta-llama-Llama-3.2-1B\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mcheckpoint-800\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m#best checkpoint 700-800\u001b[39;00m\n\u001b[1;32m--> 440\u001b[0m \u001b[43mgenerate_and_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 364\u001b[0m, in \u001b[0;36mgenerate_and_save\u001b[1;34m(args, model_weights_path, llm, tokenizer, dataset, num_examples, max_length)\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[38;5;66;03m# Generate the model's response\u001b[39;00m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 364\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;66;03m# Decode the generated response\u001b[39;00m\n\u001b[0;32m    367\u001b[0m generated_text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\franc\\Documents\\Bridge_the_GAP\\myenv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\franc\\Documents\\Bridge_the_GAP\\myenv\\Lib\\site-packages\\transformers\\generation\\utils.py:2246\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   2238\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   2239\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   2240\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   2241\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   2242\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2243\u001b[0m     )\n\u001b[0;32m   2245\u001b[0m     \u001b[38;5;66;03m# 13. run beam sample\u001b[39;00m\n\u001b[1;32m-> 2246\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_beam_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2247\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeam_scorer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2252\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2254\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2256\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGROUP_BEAM_SEARCH:\n\u001b[0;32m   2257\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[0;32m   2258\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[0;32m   2259\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   2260\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2266\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[0;32m   2267\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\franc\\Documents\\Bridge_the_GAP\\myenv\\Lib\\site-packages\\transformers\\generation\\utils.py:3523\u001b[0m, in \u001b[0;36mGenerationMixin._beam_search\u001b[1;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[0;32m   3520\u001b[0m next_tokens \u001b[38;5;241m=\u001b[39m next_tokens \u001b[38;5;241m%\u001b[39m vocab_size\n\u001b[0;32m   3522\u001b[0m \u001b[38;5;66;03m# stateless\u001b[39;00m\n\u001b[1;32m-> 3523\u001b[0m beam_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mbeam_scorer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3524\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3525\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnext_token_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3526\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnext_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3527\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnext_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3528\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3529\u001b[0m \u001b[43m    \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3530\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeam_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeam_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3531\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_prompt_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_prompt_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3532\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3534\u001b[0m beam_scores \u001b[38;5;241m=\u001b[39m beam_outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext_beam_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   3535\u001b[0m beam_next_tokens \u001b[38;5;241m=\u001b[39m beam_outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext_beam_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\franc\\Documents\\Bridge_the_GAP\\myenv\\Lib\\site-packages\\transformers\\generation\\beam_search.py:269\u001b[0m, in \u001b[0;36mBeamSearchScorer.process\u001b[1;34m(self, input_ids, next_scores, next_tokens, next_indices, pad_token_id, eos_token_id, beam_indices, group_index, decoder_prompt_len)\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# next tokens for this sentence\u001b[39;00m\n\u001b[0;32m    267\u001b[0m beam_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m beam_token_rank, (next_token, next_score, next_index) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\n\u001b[1;32m--> 269\u001b[0m     \u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnext_tokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_scores\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_indices\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    270\u001b[0m ):\n\u001b[0;32m    271\u001b[0m     batch_beam_idx \u001b[38;5;241m=\u001b[39m batch_idx \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroup_size \u001b[38;5;241m+\u001b[39m next_index\n\u001b[0;32m    272\u001b[0m     \u001b[38;5;66;03m# add to generated hypotheses if end of sentence\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\franc\\Documents\\Bridge_the_GAP\\myenv\\Lib\\site-packages\\torch\\_tensor.py:1099\u001b[0m, in \u001b[0;36mTensor.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1089\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1090\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing len to get tensor shape might cause the trace to be incorrect. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1091\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecommended usage would be tensor.shape[0]. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1095\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m   1096\u001b[0m         )\n\u001b[0;32m   1097\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m-> 1099\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1100\u001b[0m     \u001b[38;5;66;03m# NB: we use 'imap' and not 'map' here, so that in Python 2 we get a\u001b[39;00m\n\u001b[0;32m   1101\u001b[0m     \u001b[38;5;66;03m# generator and don't eagerly perform all the indexes.  This could\u001b[39;00m\n\u001b[0;32m   1102\u001b[0m     \u001b[38;5;66;03m# save us work, and also helps keep trace ordering deterministic\u001b[39;00m\n\u001b[0;32m   1103\u001b[0m     \u001b[38;5;66;03m# (e.g., if you zip(*hiddens), the eager map will force all the\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m     \u001b[38;5;66;03m# indexes of hiddens[0] before hiddens[1], while the generator\u001b[39;00m\n\u001b[0;32m   1105\u001b[0m     \u001b[38;5;66;03m# map will interleave them.)\u001b[39;00m\n\u001b[0;32m   1106\u001b[0m     \u001b[38;5;66;03m# NB: We have intentionally skipped __torch_function__ dispatch here.\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m     \u001b[38;5;66;03m# See gh-54457\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miteration over a 0-d tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re \n",
    "import argparse\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from typing import Tuple, Dict, Optional, Union, List\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import PreTrainedTokenizer, AutoModelForCausalLM\n",
    "from trl import setup_chat_format\n",
    "\n",
    "from utils import *\n",
    "from bgm import BGM\n",
    "from llm import LLM\n",
    "from default_prompts import *\n",
    "from prompt_dataset import PromptDataset\n",
    "from datasets import Dataset\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "device = torch.device(f\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "warnings.filterwarnings('ignore')\n",
    "SEED=10\n",
    "\n",
    "info = {\n",
    "    \"nq\": {\n",
    "        \"test\": {\n",
    "            \"data_path\": r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\test_dataset.json',\n",
    "            \"contriever_search_results_path\": r\"C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\processed\\contriever_test_search_results_at150.pkl\",\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "class DotDict:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "def parse_arguments(custom_args=None):\n",
    "    \"\"\"\n",
    "    Mimics argparse to parse arguments for LLM generation. Accepts custom arguments as a dictionary for notebooks.\n",
    "    \"\"\"\n",
    "    # Define default values\n",
    "    default_args = {\n",
    "        'output_dir': r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\gen_res_example_llm_with_bgm',\n",
    "        'bgm_id': 'meta-llama/Llama-3.2-1B',\n",
    "        'llm_id': 'google/gemma-2-2b-it',\n",
    "        'dataset': 'nq',\n",
    "        'task_instruction' : \"Output only the document IDs relevant to the query. Use this format: [Id_1, Id_2, ...].\",\n",
    "        'model_max_length': 4096,\n",
    "        'quantization_bits': 4,\n",
    "        'gold_position': None,\n",
    "        'use_model_chat_template': False, \n",
    "        'num_retrieved_documents': 5,\n",
    "        'use_test': True,\n",
    "        'padding_strategy': 'longest',\n",
    "        'max_new_tokens': 50,\n",
    "        'use_task_with_proof': False,\n",
    "        'batch_size': None,\n",
    "        'save_every': 250,\n",
    "    }\n",
    "\n",
    "    # If custom_args is provided, update defaults\n",
    "    if custom_args:\n",
    "        default_args.update(custom_args)\n",
    "\n",
    "    # Perform validation\n",
    "    if default_args['num_retrieved_documents'] is None:\n",
    "        raise ValueError(\"'num_retrieved_documents' must be specified.\")\n",
    "    if default_args['num_retrieved_documents'] <= 0:\n",
    "        raise ValueError(\"'num_retrieved_documents' must be a positive integer.\")\n",
    "    if default_args['gold_position'] is not None:\n",
    "        if (default_args['gold_position'] < 0 or \n",
    "            default_args['gold_position'] >= default_args['num_retrieved_documents']):\n",
    "            raise ValueError(\"'gold_position' must be within the range of 'num_retrieved_documents'.\")\n",
    "\n",
    "    return DotDict(**default_args)\n",
    "\n",
    "\n",
    "def load_corpus(\n",
    "    args: argparse.Namespace\n",
    ") -> Tuple[List[Dict], Optional[Dict[int, int]]]:\n",
    "    \n",
    "    # Corpus with documents from Contriever\n",
    "    corpus, full_to_subset_idx_map = read_test_corpus_with_random_and_contriever()\n",
    "\n",
    "    return corpus, full_to_subset_idx_map\n",
    "\n",
    "def load_search_results(args: argparse.Namespace) -> List[Tuple[List[int], List[float]]]:\n",
    "\n",
    "    search_results_path = info[args.dataset][args.split]['contriever_search_results_path']\n",
    "    retriever_search_results = read_pickle(search_results_path)\n",
    "\n",
    "    return retriever_search_results\n",
    "\n",
    "\n",
    "def get_prompt_template(args: argparse.Namespace):\n",
    "    prompt_configuration = args.dataset\n",
    "    if args.use_model_chat_template:\n",
    "        chat_task_template_str = chat_task_templates[args.llm_id]['template']\n",
    "        \n",
    "        task_instruction = task_instructions[prompt_configuration]\n",
    "\n",
    "        prompt_template = apply_chat_task_template(chat_task_template_str, task_instruction)\n",
    "    else:\n",
    "        task_template = task_templates[prompt_configuration]\n",
    "\n",
    "        prompt_template = task_template.create_prompt_template()\n",
    "\n",
    "    return prompt_template\n",
    "\n",
    "def process_dataset(dataset, task_instruction):\n",
    "    \"\"\"\n",
    "    Processes the dataset by applying the chat template transformation.\n",
    "\n",
    "    Args:\n",
    "        dataset (List[Dict]): The dataset to be processed.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict]: The processed dataset with formatted text.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the chat messages format\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": task_instruction},\n",
    "        {\"role\": \"user\", \"content\": dataset},\n",
    "    ]\n",
    "\n",
    "    return messages\n",
    "\n",
    "def initialize_dataset_and_loader(\n",
    "    args: argparse.Namespace, \n",
    "    corpus: List[Dict], \n",
    "    full_to_subset_idx_map: Optional[Dict[int, int]], \n",
    "    retriever_search_results: List[Tuple[List[int], List[float]]], \n",
    "    tokenizer: PreTrainedTokenizer,\n",
    ") -> DataLoader:\n",
    "    \n",
    "    prompt_template = get_prompt_template(args)\n",
    "    \n",
    "    prompt_ds = PromptDataset(\n",
    "        corpus=corpus, data_path=info[args.dataset][args.split]['data_path'], \n",
    "        tokenizer=tokenizer, \n",
    "        max_tokenized_length=args.model_max_length - 2, \n",
    "        search_results=retriever_search_results,\n",
    "        prompt_template=prompt_template,\n",
    "        full_to_subset_idx_map=full_to_subset_idx_map,\n",
    "        do_normalize_query=True, \n",
    "        num_documents_in_context=args.num_retrieved_documents,\n",
    "        gold_position=args.gold_position, # None in these experiments\n",
    "    )\n",
    "        \n",
    "    prompt_dataloader = DataLoader(\n",
    "        prompt_ds,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=8,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    return prompt_dataloader\n",
    "\n",
    "    \n",
    "def map_document_indices(prompt: str, document_indices: list) -> tuple:\n",
    "    \"\"\"\n",
    "    Modifica il prompt mappando i document ID originali con ID sequenziali.\n",
    "    \n",
    "    Args:\n",
    "        prompt (str): Il prompt originale contenente i document ID.\n",
    "        document_indices (list): Lista degli indici dei documenti nel prompt.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Il prompt modificato e la mappatura (original_id -> new_id).\n",
    "    \"\"\"\n",
    "    id_mapping = {}\n",
    "    \n",
    "    # Mappa i document ID ai nuovi ID sequenziali (Id_1, Id_2, ...)\n",
    "    for idx, doc_id in enumerate(document_indices, start=1):\n",
    "        new_id = f\"Id_{idx}\"\n",
    "        id_mapping[str(doc_id)] = new_id\n",
    "    \n",
    "    # Sostituisce i document ID nel prompt\n",
    "    modified_prompt = prompt\n",
    "    for original_id, new_id in id_mapping.items():\n",
    "        modified_prompt = re.sub(rf'\\[{original_id}\\]', f'[{new_id}]', modified_prompt)\n",
    "    \n",
    "    return modified_prompt, id_mapping\n",
    "\n",
    "def extract_and_convert_answer_indices(generated_output: str, id_mapping: dict) -> str:\n",
    "    \"\"\"\n",
    "    Estrae e converte gli ID generati dal modello dopo '<|im_start|>assistant'.\n",
    "    Converte gli ID nel formato originale. Se non trova un ID nella mappatura, restituisce 'Unknown(ID)'.\n",
    "\n",
    "    Args:\n",
    "        generated_output (str): Testo con la risposta generata dal modello.\n",
    "        id_mapping (dict): Mappatura {original_id: Id_n}.\n",
    "\n",
    "    Returns:\n",
    "        str: Stringa con gli ID originali separati da virgola.\n",
    "    \"\"\"\n",
    "    # Invertire la mappatura per ottenere {Id_n: original_id}\n",
    "    inverse_mapping = {v: k for k, v in id_mapping.items()}\n",
    "\n",
    "    # Estrae la risposta dopo '<|im_start|>assistant'\n",
    "    match = re.search(r'<\\|im_start\\|>assistant\\s*(.*)', generated_output, re.DOTALL)\n",
    "    if not match:\n",
    "        return \"Nessuna risposta trovata\"\n",
    "    #if match:\n",
    "        #print(\"Contenuto dopo assistant:\", repr(match.group(1)))\n",
    "\n",
    "    # Ottieni la stringa con gli ID dopo assistant\n",
    "    answer_string = match.group(1).strip().split(\"<|im_end|>\")[0].strip()\n",
    "\n",
    "    # Dividi e converte gli ID\n",
    "    generated_ids = [id_.strip() for id_ in answer_string.split(\",\") if id_.strip()]\n",
    "    original_ids = [inverse_mapping.get(id_, f\"Unknown({id_})\") for id_ in generated_ids]\n",
    "\n",
    "    # Restituisci gli ID originali come stringa separata da virgole\n",
    "    return \",\".join(original_ids)\n",
    "\n",
    "def reconstruct_prompt_from_ids(\n",
    "    original_ids: str,\n",
    "    prompt: str,\n",
    "    task_instruction: str = \"You are given a question and you must respond based on the provided documents. You must always provide an answer.\"\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Ricostruisce il prompt originale utilizzando solo i documenti selezionati dal modello,\n",
    "    ignorando gli ID non presenti nella lista generale.\n",
    "\n",
    "    Args:\n",
    "        original_ids (str): Stringa contenente gli ID separati da virgola (es. \"628506,3546609\").\n",
    "        prompt (str): Testo completo del prompt originale.\n",
    "\n",
    "    Returns:\n",
    "        str: Il prompt ricostruito contenente solo i documenti selezionati.\n",
    "    \"\"\"\n",
    "    query_match = re.search(r\"Question:\\s*(.*?)(?=Document|\\Z)\", prompt, re.DOTALL)\n",
    "    query = query_match.group(1).strip() if query_match else \"Query non trovata\"\n",
    "\n",
    "    #print(f\"Query originale trovata: {query}\")\n",
    "    \n",
    "    documents = re.findall(\n",
    "        r\"(Document \\[(\\d+)\\]\\(.*?\\)\\s.*?)(?=Document \\[\\d+\\]|$)\", \n",
    "        prompt, \n",
    "        re.DOTALL\n",
    "    )\n",
    "    documents_dict = {doc_id.strip(): full_text.strip() for full_text, doc_id in documents}\n",
    "\n",
    "    print(f\"ID disponibili nel prompt: {list(documents_dict.keys())}\")\n",
    "\n",
    "    selected_ids = [id_.strip() for id_ in original_ids.split(\",\") if id_.strip()]\n",
    "    \n",
    "    valid_ids = [id_ for id_ in selected_ids if id_ in documents_dict]\n",
    "\n",
    "    print(f\"ID validi generati dal modello: {valid_ids}\")\n",
    "\n",
    "    selected_documents = [\n",
    "        re.sub(r'Answer:\\s*\\n?', '', documents_dict[id_]) \n",
    "        for id_ in valid_ids\n",
    "    ]\n",
    "\n",
    "    if not selected_documents:\n",
    "        return #nesun documento trovato\n",
    "\n",
    "    reconstructed_prompt = \"\\n\".join(selected_documents)\n",
    "\n",
    "    final_prompt = (f\"{task_instruction}\\nQuestion:{query}\\nDocuments:\\n{reconstructed_prompt}\\nAnswer:\")\n",
    "\n",
    "    final_prompt = re.sub(r'\\n+', '\\n', final_prompt).strip() + \"\\n\"\n",
    "\n",
    "    return final_prompt\n",
    "\n",
    "def extract_generate_answers(\n",
    "    args: argparse.Namespace, \n",
    "    generated_output: List[str]\n",
    ") -> List[str]:\n",
    "    answer_prefix = \"Answer:\"\n",
    "    if args.use_model_chat_template:\n",
    "        answer_prefix = re.escape(chat_task_templates[args.llm_id]['answer_prefix'])\n",
    "\n",
    "    generated_answers = []\n",
    "    for output in generated_output:\n",
    "        matches = list(re.finditer(answer_prefix, output))\n",
    "        match_idx = 0\n",
    "\n",
    "        # When using the proof there is a one-shot example that already \n",
    "        # contains the string \"Answer:\". Thus, we should get the second (match_idx=1) match.\n",
    "        if args.use_task_with_proof:\n",
    "            match_idx = 1\n",
    "            if args.use_model_chat_template and answer_prefix != \"Answer:\":\n",
    "                match_idx = 0\n",
    " \n",
    "        answer_end = matches[match_idx].end()\n",
    "        response = output[answer_end:].strip()\n",
    "        generated_answers.append(response)\n",
    "    \n",
    "    return generated_answers\n",
    "\n",
    "def print_info(args: argparse.Namespace):\n",
    "    print(\"INFO:\")    \n",
    "    print(f\"DATA: {info[args.dataset]['test']['data_path']}\")\n",
    "    print(f\"TASK INSTRUCTION: {args.task_instruction}\")\n",
    "    print(f\"USE TEST: {args.use_test}\")\n",
    "    print(f\"BGM MODEL: {args.bgm_id}\")\n",
    "    print(f\"LLm MODEL: {args.llm_id}\")\n",
    "    print(f\"MODEL MAX LENGTH: {args.model_max_length}\")\n",
    "    print(f'MAX NEW TOKENS: {args.max_new_tokens}')\n",
    "    print(f\"USE MODEL CHAT TEMPLATE: {args.use_model_chat_template}\")\n",
    "    print(f\"TASK WITH PROOF:\", args.use_task_with_proof)\n",
    "    print(f\"GOLD POSITION: {args.gold_position}\")\n",
    "    print(f\"NUM DOCUMENTS IN CONTEXT: {args.num_retrieved_documents}\")\n",
    "    print(f\"BATCH SIZE: {args.batch_size}\")\n",
    "    print(f\"SAVE EVERY: {args.save_every}\")\n",
    "\n",
    "\n",
    "def generate_and_save(\n",
    "    args: argparse.Namespace, \n",
    "    model_weights_path,\n",
    "    llm: LLM, \n",
    "    tokenizer,\n",
    "    dataset,\n",
    "    num_examples=10,\n",
    "    max_length=50\n",
    "):\n",
    "    \n",
    "    llm_id = args.llm_id\n",
    "    num_doc = args.num_retrieved_documents\n",
    "    save_every = args.save_every\n",
    "    retriever_str = \"contriever\"\n",
    "    padding_str = f\"_{args.padding_strategy}{args.model_max_length}\" if args.padding_strategy != \"longest\" else \"\" \n",
    "    chat_template_str = \"_template\" if args.use_model_chat_template else \"\"\n",
    "    prompt_type = \"retrieved_proof\" if args.use_task_with_proof else \"retrieved\"\n",
    "\n",
    "    # Create the saving directory\n",
    "    llm_folder = llm_id.split(\"/\")[1] if '/' in llm_id else llm_id\n",
    "    saving_dir = f\"{args.output_dir}/{args.dataset}/{llm_folder}/{args.split}/{prompt_type}/{retriever_str}/{num_doc}_doc\"\n",
    "    #os.makedirs(saving_dir, exist_ok=True)\n",
    "\n",
    "    # Load the trained model\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_weights_path)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_info = []  \n",
    "    for idx, prompt_batch in enumerate(tqdm(dataset)):\n",
    "        if idx >= num_examples:\n",
    "            break\n",
    "\n",
    "        prompt = prompt_batch['prompt'].replace('You are given a question and you must respond based on the provided documents. You must always provide an answer.', \"\")\n",
    "        document_indices= prompt_batch['document_indices']\n",
    "\n",
    "        # Mappa i document ID nel prompt\n",
    "        modified_prompt, id_mapping = map_document_indices(prompt, document_indices)\n",
    "        print(f\"Processing Example {idx+1}:\\n\")\n",
    "\n",
    "        prompt_formatted = process_dataset(modified_prompt, args.task_instruction)\n",
    "        prompt_formatted = tokenizer.apply_chat_template(\n",
    "            prompt_formatted, tokenize=False, add_generation_prompt=False, add_special_tokens=False\n",
    "        )\n",
    "\n",
    "        # Tokenize the input\n",
    "        inputs = tokenizer(prompt_formatted, return_tensors=\"pt\", truncation=False).to(device)\n",
    "\n",
    "        # Generate the model's response\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(**inputs, max_new_tokens=max_length, num_beams=5)\n",
    "\n",
    "        # Decode the generated response\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "\n",
    "        # Convertire gli ID generati nei document ID originali\n",
    "        original_ids = extract_and_convert_answer_indices(generated_text, id_mapping)\n",
    "\n",
    "        filtered_prompt = reconstruct_prompt_from_ids(original_ids, prompt)\n",
    "        print(filtered_prompt)\n",
    "\n",
    "        generated_output = llm.generate(\n",
    "            filtered_prompt,\n",
    "            padding_strategy=args.padding_strategy, \n",
    "            max_new_tokens=args.max_new_tokens\n",
    "        )\n",
    "\n",
    "        generated_answers = extract_generate_answers(args, generated_output)\n",
    "        print(f\"La risposta generata dal modello e':\\n{generated_answers}\")\n",
    "        prompt_batch['generated_answer'] = generated_answers\n",
    "        all_info.append(prompt_batch)\n",
    "\n",
    "        '''\n",
    "        if (idx + 1) % save_every == 0 or (idx + 1) == len(dataset):\n",
    "            print(f\"Saving at {idx + 1}...\")\n",
    "            file_name = f\"{saving_dir}/numdoc{num_doc}_retr{args.num_retrieved_documents}{padding_str}{chat_template_str}_info_{idx+1}.pkl\"\n",
    "            write_pickle(all_info, file_name)\n",
    "            all_info = []\n",
    "        '''\n",
    "        \n",
    "\n",
    "def main():\n",
    "    args = parse_arguments()\n",
    "\n",
    "    args.split = \"test\" if args.use_test else \"train\"\n",
    "\n",
    "    print(\"Loading BGM...\")\n",
    "    bgm_id = args.bgm_id\n",
    "    \n",
    "    bgm = BGM(\n",
    "        bgm_id, device, \n",
    "        quantization_bits=args.quantization_bits, \n",
    "        model_max_length=15,\n",
    "    )\n",
    "    bgm_model= bgm.model\n",
    "\n",
    "    tokenizer = bgm.tokenizer\n",
    "    model, tokenizer = setup_chat_format(bgm_model, tokenizer)\n",
    "    print(\"BGM loaded\")\n",
    "    \n",
    "    print(\"Loading LLM...\")\n",
    "    llm_id = args.llm_id\n",
    "    llm = LLM(\n",
    "        llm_id, device, \n",
    "        quantization_bits=args.quantization_bits, \n",
    "        model_max_length=args.model_max_length\n",
    "    )\n",
    "    print(\"LLM loaded\")\n",
    "\n",
    "    task_instruction = args.task_instruction\n",
    "\n",
    "    print(\"Loading corpus and search results...\")\n",
    "    corpus, full_to_subset_idx_map = load_corpus(args)\n",
    "    retriever_search_results = load_search_results(args)\n",
    "    print(\"Corpus and search results loaded\")\n",
    "\n",
    "\n",
    "    print(\"Loading prompt dataset...\")\n",
    "    prompt_dataloader = initialize_dataset_and_loader(\n",
    "        args, corpus, full_to_subset_idx_map, \n",
    "        retriever_search_results, tokenizer\n",
    "    )\n",
    "    print(\"Prompt dataset loaded\")\n",
    "\n",
    "    print_info(args)\n",
    "\n",
    "    training_path=r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\SFT_training_bgm\\meta-llama-Llama-3.2-1B\\checkpoint-800'  #best checkpoint 700-800\n",
    "    generate_and_save(args, training_path, llm, tokenizer, prompt_dataloader, num_examples=10, max_length=15)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    seed_everything(SEED)\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
