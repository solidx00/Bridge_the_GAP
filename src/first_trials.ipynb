{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "NVIDIA GeForce RTX 3080\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\Documents\\Bridge_the_GAP\\myenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LLM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM loaded\n",
      "Loading corpus and search results...\n",
      "Corpus and search results loaded\n",
      "Loading prompt dataset...\n",
      "Prompt dataset loaded\n",
      "INFO:\n",
      "DATA: C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\test_dataset.json\n",
      "USE TEST: True\n",
      "MODEL: google/gemma-2-2b-it\n",
      "MODEL MAX LENGTH: 4096\n",
      "MAX NEW TOKENS: 50\n",
      "USE MODEL CHAT TEMPLATE: True\n",
      "TASK WITH PROOF: False\n",
      "GOLD POSITION: None\n",
      "NUM DOCUMENTS IN CONTEXT: 5\n",
      "BATCH SIZE: None\n",
      "SAVE EVERY: 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 87/2889 [05:19<2:20:33,  3.01s/it]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re \n",
    "import argparse\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from typing import Tuple, Dict, Optional\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "from utils import *\n",
    "from llm import LLM\n",
    "from default_prompts import *\n",
    "from prompt_dataset import PromptDataset\n",
    "\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "device = torch.device(f\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "warnings.filterwarnings('ignore')\n",
    "SEED=10\n",
    "\n",
    "info = {\n",
    "    \"nq\": {\n",
    "        \"test\": {\n",
    "            \"data_path\": r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\test_dataset.json',\n",
    "            \"contriever_search_results_path\": r\"C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\processed\\contriever_test_search_results_at150.pkl\",\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "class DotDict:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "def parse_arguments(custom_args=None):\n",
    "    \"\"\"\n",
    "    Mimics argparse to parse arguments for LLM generation. Accepts custom arguments as a dictionary for notebooks.\n",
    "    \"\"\"\n",
    "    # Define default values\n",
    "    default_args = {\n",
    "        'output_dir': r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\gen_res_example_llm',\n",
    "        'llm_id': 'google/gemma-2-2b-it',\n",
    "        'dataset': 'nq',\n",
    "        'model_max_length': 4096,\n",
    "        'quantization_bits': 4,\n",
    "        'use_model_chat_template': True, \n",
    "        'gold_position': None,\n",
    "        'num_retrieved_documents': 5,\n",
    "        'use_test': True,\n",
    "        'padding_strategy': 'longest',\n",
    "        'max_new_tokens': 50,\n",
    "        'use_task_with_proof': False,\n",
    "        'batch_size': None,\n",
    "        'save_every': 250,\n",
    "    }\n",
    "\n",
    "    # If custom_args is provided, update defaults\n",
    "    if custom_args:\n",
    "        default_args.update(custom_args)\n",
    "\n",
    "    # Perform validation\n",
    "    if default_args['num_retrieved_documents'] is None:\n",
    "        raise ValueError(\"'num_retrieved_documents' must be specified.\")\n",
    "    if default_args['num_retrieved_documents'] <= 0:\n",
    "        raise ValueError(\"'num_retrieved_documents' must be a positive integer.\")\n",
    "    if default_args['gold_position'] is not None:\n",
    "        if (default_args['gold_position'] < 0 or \n",
    "            default_args['gold_position'] >= default_args['num_retrieved_documents']):\n",
    "            raise ValueError(\"'gold_position' must be within the range of 'num_retrieved_documents'.\")\n",
    "\n",
    "    return DotDict(**default_args)\n",
    "\n",
    "\n",
    "def load_corpus(\n",
    "    args: argparse.Namespace\n",
    ") -> Tuple[List[Dict], Optional[Dict[int, int]]]:\n",
    "    \n",
    "    # Corpus with documents from Contriever\n",
    "    corpus, full_to_subset_idx_map = read_test_corpus_with_random_and_contriever()\n",
    "\n",
    "    return corpus, full_to_subset_idx_map\n",
    "\n",
    "def load_search_results(args: argparse.Namespace) -> List[Tuple[List[int], List[float]]]:\n",
    "    # Search results from Contriever\n",
    "    search_results_path = info['nq']['test']['contriever_search_results_path'] \n",
    "\n",
    "    search_results = read_pickle(search_results_path)\n",
    "    return search_results\n",
    "\n",
    "\n",
    "def get_prompt_template(args: argparse.Namespace):\n",
    "    prompt_configuration = args.dataset\n",
    "\n",
    "    if args.use_model_chat_template:\n",
    "        chat_task_template_str = chat_task_templates[args.llm_id]['template']\n",
    "        \n",
    "        task_instruction = task_instructions[prompt_configuration]\n",
    "        if args.use_task_with_proof:\n",
    "            task_instruction = task_instructions['qa_proof'][prompt_configuration]\n",
    "\n",
    "        prompt_template = apply_chat_task_template(chat_task_template_str, task_instruction)\n",
    "    else:\n",
    "        task_template = task_templates[prompt_configuration]\n",
    "\n",
    "        if args.use_task_with_proof:\n",
    "            task_template = task_templates['qa_proof'][prompt_configuration]\n",
    "\n",
    "        prompt_template = task_template.create_prompt_template()\n",
    "\n",
    "    return prompt_template\n",
    "\n",
    "\n",
    "def initialize_dataset_and_loader(\n",
    "    args: argparse.Namespace, \n",
    "    corpus: List[Dict], \n",
    "    full_to_subset_idx_map: Optional[Dict[int, int]], \n",
    "    retriever_search_results: List[Tuple[List[int], List[float]]], \n",
    "    tokenizer: PreTrainedTokenizer\n",
    ") -> DataLoader:\n",
    "    \n",
    "    prompt_template = get_prompt_template(args)\n",
    "    \n",
    "    prompt_ds = PromptDataset(\n",
    "        corpus=corpus, data_path=info[args.dataset]['test']['data_path'], \n",
    "        tokenizer=tokenizer, \n",
    "        max_tokenized_length=args.model_max_length - 2, \n",
    "        search_results=retriever_search_results,\n",
    "        prompt_template=prompt_template,\n",
    "        full_to_subset_idx_map=full_to_subset_idx_map,\n",
    "        do_normalize_query=True, \n",
    "        num_documents_in_context=args.num_retrieved_documents,\n",
    "        gold_position=args.gold_position, # None in these experiments\n",
    "    )\n",
    "        \n",
    "    prompt_dataloader = DataLoader(\n",
    "        prompt_ds,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=8,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    return prompt_dataloader\n",
    "\n",
    "\n",
    "def print_info(args: argparse.Namespace):\n",
    "    print(\"INFO:\")    \n",
    "    print(f\"DATA: {info[args.dataset]['test']['data_path']}\")\n",
    "    print(f\"USE TEST: {args.use_test}\")\n",
    "    print(f\"MODEL: {args.llm_id}\")\n",
    "    print(f\"MODEL MAX LENGTH: {args.model_max_length}\")\n",
    "    print(f'MAX NEW TOKENS: {args.max_new_tokens}')\n",
    "    print(f\"USE MODEL CHAT TEMPLATE: {args.use_model_chat_template}\")\n",
    "    print(f\"TASK WITH PROOF:\", args.use_task_with_proof)\n",
    "    print(f\"GOLD POSITION: {args.gold_position}\")\n",
    "    print(f\"NUM DOCUMENTS IN CONTEXT: {args.num_retrieved_documents}\")\n",
    "    print(f\"BATCH SIZE: {args.batch_size}\")\n",
    "    print(f\"SAVE EVERY: {args.save_every}\")\n",
    "\n",
    "\n",
    "def extract_generate_answers(\n",
    "    args: argparse.Namespace, \n",
    "    generated_output: List[str]\n",
    ") -> List[str]:\n",
    "    answer_prefix = \"Answer:\"\n",
    "    if args.use_model_chat_template:\n",
    "        #answer_prefix = re.escape(chat_task_templates[args.llm_id]['answer_prefix'])\n",
    "\n",
    "        answer_prefix = re.escape(\"Answer:\") + r\"\\nmodel\"  #adjust this\n",
    "\n",
    "\n",
    "    generated_answers = []\n",
    "    for output in generated_output:\n",
    "        matches = list(re.finditer(answer_prefix, output))\n",
    "\n",
    "        match_idx = 0\n",
    "\n",
    "        # When using the proof there is a one-shot example that already \n",
    "        # contains the string \"Answer:\". Thus, we should get the second (match_idx=1) match.\n",
    "        if args.use_task_with_proof:\n",
    "            match_idx = 1\n",
    "            if args.use_model_chat_template and answer_prefix != \"Answer:\":\n",
    "                match_idx = 0\n",
    " \n",
    "        answer_end = matches[match_idx].end()\n",
    "        response = output[answer_end:].strip()\n",
    "        generated_answers.append(response)\n",
    "    \n",
    "    return generated_answers\n",
    "\n",
    "\n",
    "def generate_and_save(\n",
    "    args: argparse.Namespace, \n",
    "    llm: LLM, \n",
    "    prompt_dataloader: DataLoader\n",
    "):\n",
    "    # Info from arguments\n",
    "    llm_id = args.llm_id\n",
    "    num_doc = args.num_retrieved_documents\n",
    "    save_every = args.save_every\n",
    "    retriever_str = \"contriever\"\n",
    "    padding_str = f\"_{args.padding_strategy}{args.model_max_length}\" if args.padding_strategy != \"longest\" else \"\" \n",
    "    chat_template_str = \"_template\" if args.use_model_chat_template else \"\"\n",
    "    prompt_type = \"retrieved_proof\" if args.use_task_with_proof else \"retrieved\"\n",
    "\n",
    "    # Create the saving directory\n",
    "    llm_folder = llm_id.split(\"/\")[1] if '/' in llm_id else llm_id\n",
    "    saving_dir = f\"{args.output_dir}/{args.dataset}/{llm_folder}/test/{prompt_type}/{retriever_str}/{num_doc}_doc\"\n",
    "    os.makedirs(saving_dir, exist_ok=True)\n",
    "\n",
    "    all_info = []  \n",
    "    for idx, prompt_batch in enumerate(tqdm(prompt_dataloader)):\n",
    "        prompts = prompt_batch['prompt']\n",
    "        generated_output = llm.generate(\n",
    "            prompts,\n",
    "            padding_strategy=args.padding_strategy, \n",
    "            max_new_tokens=args.max_new_tokens\n",
    "        )\n",
    "\n",
    "        generated_answers = extract_generate_answers(args, generated_output)\n",
    "        prompt_batch['generated_answer'] = generated_answers\n",
    "        all_info.append(prompt_batch)\n",
    "        \n",
    "        if (idx + 1) % save_every == 0 or (idx + 1) == len(prompt_dataloader):\n",
    "            print(f\"Saving at {idx + 1}...\")\n",
    "            file_name = f\"{saving_dir}/numdoc{num_doc}_retr{args.num_retrieved_documents}{padding_str}{chat_template_str}_info_{idx+1}.pkl\"\n",
    "            write_pickle(all_info, file_name)\n",
    "            all_info = []\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = parse_arguments()\n",
    "\n",
    "    print(\"Loading LLM...\")\n",
    "    llm_id = args.llm_id\n",
    "    llm = LLM(\n",
    "        llm_id, device, \n",
    "        quantization_bits=args.quantization_bits, \n",
    "        model_max_length=args.model_max_length\n",
    "    )\n",
    "    tokenizer = llm.tokenizer\n",
    "    print(\"LLM loaded\")\n",
    "\n",
    "\n",
    "    print(\"Loading corpus and search results...\")\n",
    "    corpus, full_to_subset_idx_map = load_corpus(args)\n",
    "    retriever_search_results = load_search_results(args)\n",
    "    print(\"Corpus and search results loaded\")\n",
    "\n",
    "\n",
    "    print(\"Loading prompt dataset...\")\n",
    "    prompt_dataloader = initialize_dataset_and_loader(\n",
    "        args, corpus, full_to_subset_idx_map, \n",
    "        retriever_search_results, tokenizer\n",
    "    )\n",
    "    print(\"Prompt dataset loaded\")\n",
    "\n",
    "    print_info(args)\n",
    "    generate_and_save(args, llm, prompt_dataloader)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    seed_everything(SEED)\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the results of generate_answer_llm\n",
    "from utils import *\n",
    "\n",
    "result_path=r\"C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\gen_res_example_llm\\gemma-2-2b-it\\test\\classic\\contriever\\5_doc\\numdoc5_gold_atNone_answerless_info_250.pkl\"\n",
    "data=read_pickle(result_path)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pickle\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from typing import List, Dict\n",
    "\n",
    "from utils import str2bool\n",
    "from normalize_answers import *\n",
    "from read_negative_rejection import *\n",
    "\n",
    "\n",
    "\n",
    "def are_answers_matching(prediction: str, ground_truths: List[str]) -> float:\n",
    "    normalized_prediction = normalize_answer(prediction)\n",
    "\n",
    "    for ground_truth in ground_truths:\n",
    "        normalized_ground_truth = normalize_answer(ground_truth)\n",
    "        if normalized_ground_truth in normalized_prediction:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def extract_proof_from_text(text: str) -> str:\n",
    "    matches = list(re.finditer(\"Proof:\", text))\n",
    "    \n",
    "    if matches:\n",
    "        proof_end = matches[0].end()\n",
    "        proof = text[proof_end:].strip()\n",
    "        # Get the text until the first new line\n",
    "        proof = proof.split('\\n', 1)[0] \n",
    "    else:\n",
    "        proof = \"NO-PROOF\"\n",
    "\n",
    "    return proof\n",
    "\n",
    "\n",
    "def compute_df_accuracy(df: pd.DataFrame, attribute: str) -> float:\n",
    "    return round(df[attribute].sum() / len(df), 4) * 100\n",
    "\n",
    "\n",
    "def read_generation_results(file_path: str, df: pd.DataFrame) -> List[Dict]:\n",
    "    data = []\n",
    "    with open(file_path, \"r\") as fin:\n",
    "        file_data = json.load(fin)\n",
    "\n",
    "        for example in file_data:\n",
    "            example_ids = example['example_id']\n",
    "            queries = example['query']\n",
    "            prompts = example['prompt']\n",
    "            document_indices = list(zip(*example['document_indices']))\n",
    "            gold_document_indices = example['gold_document_idx']\n",
    "            generated_answers = example['generated_answer']\n",
    "            prompt_tokens_lens = example['prompt_tokens_len']\n",
    "\n",
    "            for i in range(len(example_ids)):\n",
    "                example_id = example_ids[i]\n",
    "                query = queries[i]\n",
    "                gold_document_idx = gold_document_indices[i]\n",
    "                documents_idx = list(document_indices[i])\n",
    "                # After the first new line, LLMs usually generate random text,\n",
    "                # so it is skipped in the matching comparison\n",
    "                generated_answer = generated_answers[i].split('\\n', 1)[0]\n",
    "                \n",
    "                prompt = prompts[i]\n",
    "                prompt_tokens_len = prompt_tokens_lens[i]\n",
    "\n",
    "                answers = df[df['example_id'].astype(str) == str(example_id)].answers.iloc[0]\n",
    "                gold_in_retrieved = False\n",
    "\n",
    "                if int(gold_document_idx) in map(int, documents_idx):\n",
    "                    gold_in_retrieved = True\n",
    "\n",
    "                ans_match_after_norm: bool = are_answers_matching(generated_answer, answers)\n",
    "                ans_in_documents: bool = is_answer_in_text(prompt, answers)\n",
    "                data.append({\n",
    "                    'example_id': str(example_id),\n",
    "                    'query': query,\n",
    "                    'prompt': prompt,\n",
    "                    'document_indices': documents_idx,\n",
    "                    'gold_document_idx': gold_document_idx,\n",
    "                    'generated_answer': generated_answers[i],\n",
    "                    'answers': answers,\n",
    "                    'ans_match_after_norm': ans_match_after_norm,\n",
    "                    'gold_in_retrieved': gold_in_retrieved,\n",
    "                    'ans_in_documents': ans_in_documents,\n",
    "                    \"prompt_tokens_len\": prompt_tokens_len,\n",
    "                })\n",
    "\n",
    "                if 'proof' in file_path:\n",
    "                    proof = extract_proof_from_text(generated_answers[i]) \n",
    "                    data[-1]['proof'] = proof\n",
    "                    data[-1]['ans_in_proof'] = is_answer_in_text(proof, [generated_answer])\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def read_generation_results_only_query(file_path: str, df: pd.DataFrame) -> List[Dict]:\n",
    "    data = []\n",
    "    with open(file_path, \"r\") as fin:\n",
    "        file_data = json.load(fin)\n",
    "\n",
    "        for example in file_data:\n",
    "            example_ids = example['example_id']\n",
    "            queries = example['query']\n",
    "            prompts = example['prompt']\n",
    "            generated_answers = example['generated_answer']\n",
    "\n",
    "            for i in range(len(example_ids)):\n",
    "                example_id = example_ids[i]\n",
    "                query = queries[i]\n",
    "                # After the first new line, LLMs usually generate random text,\n",
    "                # so it is skipped in the matching comparison\n",
    "                generated_answer = generated_answers[i].split('\\n', 1)[0]\n",
    "                prompt = prompts[i]\n",
    "\n",
    "                answers = df[df['example_id'].astype(str) == str(example_id)].answers.iloc[0]\n",
    "\n",
    "                ans_match_after_norm: bool = are_answers_matching(generated_answer, answers)\n",
    "                ans_in_documents: bool = is_answer_in_text(prompt, answers)\n",
    "                data.append({\n",
    "                    'example_id': str(example_id),\n",
    "                    'query': query,\n",
    "                    'prompt': prompt,\n",
    "                    'generated_answer': generated_answers[i],\n",
    "                    'answers': answers,\n",
    "                    'ans_match_after_norm': ans_match_after_norm,\n",
    "                    'ans_in_documents': ans_in_documents,\n",
    "                })\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def convert_tensors(cell):\n",
    "    \"\"\" Converts tensors in the given cell to lists, if they are tensors. \"\"\"\n",
    "    if isinstance(cell, list):\n",
    "        return [[t.tolist() if torch.is_tensor(t) else t for t in inner_list] for inner_list in cell]\n",
    "    return cell\n",
    "\n",
    "\n",
    "def extract_number_from_filename(filename: str, pattern: re.Pattern) -> int:\n",
    "    \"\"\" Extracts the number from the filename based on the provided pattern. \"\"\"\n",
    "    match = pattern.search(filename)\n",
    "    return int(match.group(1)) if match else 0\n",
    "\n",
    "\n",
    "def load_pickle_files(directory: str, filename_prefix: str) -> pd.DataFrame:\n",
    "    \"\"\" Loads and concatenates data from all pickle files in the directory with the given prefix. \"\"\"\n",
    "    pattern = re.compile(r'(\\d+).pkl')\n",
    "    files = [f for f in os.listdir(directory) if f.endswith('.pkl') and filename_prefix in f]\n",
    "    files.sort(key=lambda f: extract_number_from_filename(f, pattern))\n",
    "    print(\"I'm using the following files: \", files)\n",
    "\n",
    "    data_list = []\n",
    "    for file in files:\n",
    "        with open(os.path.join(directory, file), 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            data_list.extend(data)\n",
    "    \n",
    "    data_df = pd.DataFrame(data_list)\n",
    "    if 'only_query' in directory:\n",
    "        if data_df['example_id'].dtype != \"O\":\n",
    "            data_df['example_id'] = data_df['example_id'].apply(lambda x: x.tolist())\n",
    "    else:\n",
    "        data_df['document_indices'] = data_df['document_indices'].apply(convert_tensors)\n",
    "\n",
    "    if 'prompt_tokens_len' in data_df.columns:\n",
    "        data_df['prompt_tokens_len'] = data_df['prompt_tokens_len'].apply(lambda x: x.tolist())\n",
    "    return data_df\n",
    "\n",
    "\n",
    "def save_data_to_json(data_df: pd.DataFrame, directory: str, filename_prefix: str):\n",
    "    \"\"\" Saves the given DataFrame to a JSON file. \"\"\"\n",
    "    data_path = os.path.join(directory, f'{filename_prefix}all.json')\n",
    "    # Check if the file already exists\n",
    "    if os.path.exists(data_path):\n",
    "        overwrite = input(f\"File {data_path} already exists. Overwrite? (y/n): \")\n",
    "        if overwrite.lower() != 'y':\n",
    "            print(\"No overwrite.\")\n",
    "\n",
    "            results_df = pd.read_json(f'{directory}/{filename_prefix}all_extended.json')\n",
    "            accuracy = compute_df_accuracy(results_df, 'ans_match_after_norm')\n",
    "            print(\"ACCURACY: \", accuracy)\n",
    "\n",
    "            if 'proof' in directory:\n",
    "                accuracy_ans_in_proof = compute_df_accuracy(results_df, 'ans_in_proof')\n",
    "                print(\"ACCURACY ANS IN PROOF\", accuracy_ans_in_proof)\n",
    "\n",
    "            correct_ans_not_in_context_accuracy = compute_accuracy_correct_answer_not_in_context(results_df)\n",
    "            print(f\"Correct Answer Not in Context Accuracy: {correct_ans_not_in_context_accuracy}\")\n",
    "\n",
    "            return None\n",
    "        \n",
    "    data_df.to_json(data_path, orient='records')\n",
    "    return data_path\n",
    "\n",
    "\n",
    "def get_retrieved_path(args):\n",
    "    padding_str = f\"_{args.padding_strategy}{args.model_max_length}\" if args.padding_strategy != \"longest\" else \"\" \n",
    "    chat_template_str = \"_template\" if args.use_model_chat_template else \"\"\n",
    "    prompt_configuration = \"_no_rejection\" if args.use_no_rejection_prompt else \"\"\n",
    "\n",
    "    filename_prefix = f\"numdoc{args.num_doc}_retr{args.num_retrieved_documents}{padding_str}{chat_template_str}{prompt_configuration}_info_\"\n",
    "    return filename_prefix\n",
    "\n",
    "\n",
    "def get_only_query_path(args):\n",
    "    chat_template_str = \"_template\" if args.use_model_chat_template else \"\"\n",
    "\n",
    "    filename_prefix = f\"only_query{chat_template_str}_info_\"\n",
    "    return filename_prefix\n",
    "\n",
    "\n",
    "def parse_arguments():\n",
    "    parser = argparse.ArgumentParser(description=\"Read Generation Results.\")\n",
    "    parser.add_argument('--output_dir', type=str, default='data/gen_res', help='Output directory')\n",
    "    parser.add_argument('--dataset', type=str, default='nq', help='Dataset to use')\n",
    "    parser.add_argument('--llm_id', type=str, default='meta-llama/Llama-2-7b-chat-hf', help='LLM model identifier')\n",
    "    parser.add_argument('--model_max_length', type=int, help='Maximum input length for the LLM model', default=4096)\n",
    "    parser.add_argument('--use_model_chat_template', type=str2bool, default=False, help='Whether to use the standard chat/instruct template of the model')\n",
    "    parser.add_argument('--padding_strategy', type=str, help='Padding strategy for the LLM tokenizer', default='longest')\n",
    "    parser.add_argument('--use_test', type=str2bool, help='Use the test set')\n",
    "    parser.add_argument('--prompt_type', type=str, default='retrieved', help='Which type of prompt to use [retrieved, retrieved_proof, only_query]')\n",
    "    parser.add_argument('--use_no_rejection_prompt', type=str2bool, help='Whether to use the prompt without the NO-RES part', default=False)\n",
    "    parser.add_argument('--gold_position', type=int, help='The (0-indexed) position of the gold document in the context')\n",
    "    parser.add_argument('--num_retrieved_documents', type=int, help='Number of retrieved documents in the context')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if not args.prompt_type in ['retrieved', 'retrieved_proof', 'only_query']:\n",
    "        parser.error(\"Invalid prompt type. Must be one of ['retrieved', 'retrieved_proof', 'only_query']\")\n",
    "\n",
    "    return args\n",
    "\n",
    "\n",
    "info = {\n",
    "    \"nq\": {\n",
    "        \"test\": 'data/nq/nq-open/test_dataset.json',\n",
    "    },\n",
    "    \"triviaqa\": {\n",
    "        \"test\": 'data/triviaqa/triviaqa-unfiltered/main_test.json',\n",
    "    },\n",
    "}\n",
    "\n",
    "def main():\n",
    "    args = parse_arguments()\n",
    "    \n",
    "    retriever_str = \"contriever/\"\n",
    "\n",
    "    prompt_type = args.prompt_type\n",
    "    if 'retrieved' in prompt_type:    \n",
    "        args.num_doc = args.num_retrieved_documents\n",
    "        filename_prefix = get_retrieved_path(args)\n",
    "    elif prompt_type == 'only_query':\n",
    "        filename_prefix = get_only_query_path(args)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid prompt type\")\n",
    "\n",
    "\n",
    "    llm_id = args.llm_id\n",
    "    split = \"test\" if args.use_test else \"train\"\n",
    "    llm_folder = llm_id.split(\"/\")[1] if '/' in llm_id else llm_id\n",
    "    doc_str = f\"{args.num_doc}_doc\" if 'only_query' not in prompt_type else \"\"\n",
    "    directory = f'{args.output_dir}/{args.dataset}/{llm_folder}/{split}/{prompt_type}/{retriever_str}{doc_str}'\n",
    "    print(\"Directory: \", directory)\n",
    "\n",
    "    df = pd.read_json(info[args.dataset][split], dtype={'example_id': str})\n",
    "\n",
    "    data_df = load_pickle_files(directory, filename_prefix)\n",
    "    data_path = save_data_to_json(data_df, directory, filename_prefix)\n",
    "    if data_path is None:\n",
    "        return\n",
    "    \n",
    "    if 'only_query' in directory:\n",
    "        results = read_generation_results_only_query(data_path, df)\n",
    "    else:\n",
    "        results = read_generation_results(data_path, df)\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    accuracy = compute_df_accuracy(results_df, 'ans_match_after_norm')\n",
    "    print(\"ACCURACY: \", accuracy)\n",
    "    if 'proof' in directory:\n",
    "        accuracy_ans_in_proof = compute_df_accuracy(results_df, 'ans_in_proof')\n",
    "        print(\"ACCURACY ANS IN PROOF\", accuracy_ans_in_proof)\n",
    "        \n",
    "    results_df.to_json(os.path.join(directory, f'{filename_prefix}all_extended.json'), orient='records')\n",
    "\n",
    "    correct_ans_not_in_context_accuracy = compute_accuracy_correct_answer_not_in_context(results_df)\n",
    "    print(f\"Correct Answer Not in Context Accuracy: {correct_ans_not_in_context_accuracy}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoConfig, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    ")\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "\n",
    "class BridgeModel:\n",
    "    \"\"\"\n",
    "    Bridge Model for selecting and ranking documents by generating document IDs.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        model_id: str, \n",
    "        device: str = 'cuda', \n",
    "        model_max_length: int = 512\n",
    "    ):\n",
    "        self.device = device\n",
    "        self.model_max_length = model_max_length\n",
    "\n",
    "        # Initialize the seq2seq model and tokenizer\n",
    "        self.model, self.tokenizer = self._initialize_model_tokenizer(model_id)\n",
    "\n",
    "    def _initialize_model_tokenizer(self, model_id: str) -> Tuple[AutoModelForSeq2SeqLM, AutoTokenizer]:\n",
    "        \"\"\"\n",
    "        Initializes the seq2seq model and tokenizer with the given model ID.\n",
    "        \"\"\"\n",
    "        model_config = AutoConfig.from_pretrained(model_id, trust_remote_code=True)\n",
    "        model_config.max_seq_len = self.model_max_length\n",
    "\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "            model_id,\n",
    "            trust_remote_code=True,\n",
    "            config=model_config,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\",\n",
    "        ).to(self.device)\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_id, \n",
    "            model_max_length=self.model_max_length,\n",
    "            padding_side=\"left\",\n",
    "            truncation_side=\"left\"\n",
    "        )\n",
    "        tokenizer.pad_token = tokenizer.eos_token  # Set pad token\n",
    "\n",
    "        return model, tokenizer\n",
    "\n",
    "    def generate(\n",
    "        self, \n",
    "        prompt: str, \n",
    "        max_new_tokens: int = 15\n",
    "    ) -> List[str]:\n",
    "        \"\"\"\n",
    "        Generates the ordered document IDs based on the query and documents.\n",
    "        \"\"\"\n",
    "        # Tokenize input\n",
    "        inputs = self.tokenizer(\n",
    "            prompt, \n",
    "            return_tensors=\"pt\", \n",
    "            max_length=self.model_max_length, \n",
    "            padding=True, \n",
    "            truncation=True\n",
    "        ).to(self.device)\n",
    "\n",
    "        # Generate output\n",
    "        generated_ids = self.model.generate(\n",
    "            **inputs,\n",
    "            do_sample=False,  # Deterministic output\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            repetition_penalty=1.1,\n",
    "            pad_token_id=self.tokenizer.pad_token_id,\n",
    "            eos_token_id=self.tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "        # Decode output\n",
    "        return self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0].split()\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    model_id = \"t5-small\"  # Replace with a seq2seq model like T5 or BART\n",
    "    bridge = BridgeModel(model_id=model_id)\n",
    "\n",
    "    output = bridge.generate(prompt)\n",
    "    print(\"Output document IDs:\", output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
