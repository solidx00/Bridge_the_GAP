{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "2\n",
      "NVIDIA GeForce RTX 3080\n",
      "NVIDIA GeForce RTX 3060\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.cuda.get_device_name(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total_examples': 2889, 'examples_with_answer_in_context': 721, 'examples_without_answer_in_context': 2168, 'correct_with_context': 539, 'correct_without_context': 87, 'average_correct_with_context': 0.7475728155339806, 'average_correct_without_context': 0.04012915129151291, 'overall_accuracy': 0.2166839736933195}\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "\n",
    "def analyze_json_responses(my_path):\n",
    "    data = read_json(my_path)  # Legge il file JSON\n",
    "\n",
    "    total_examples = len(data)  # Numero totale di esempi\n",
    "    correct_with_context = 0  # Risposte corrette con risposta nel contesto\n",
    "    correct_without_context = 0  # Risposte corrette senza risposta nel contesto\n",
    "    has_answer_in_context = 0  # Esempi con risposta nel contesto\n",
    "    no_answer_in_context = 0  # Esempi senza risposta nel contesto\n",
    "    total_correct = 0  # Totale delle risposte corrette\n",
    "\n",
    "    for entry in data:\n",
    "        ans_in_documents = entry.get(\"ans_in_documents\", False)\n",
    "        ans_match_after_norm = entry.get(\"ans_match_after_norm\", False)\n",
    "\n",
    "        if ans_in_documents:\n",
    "            has_answer_in_context += 1\n",
    "            if ans_match_after_norm:\n",
    "                correct_with_context += 1\n",
    "        else:\n",
    "            no_answer_in_context += 1\n",
    "            if ans_match_after_norm:\n",
    "                correct_without_context += 1\n",
    "\n",
    "        # Conta ogni risposta corretta\n",
    "        if ans_match_after_norm:\n",
    "            total_correct += 1\n",
    "\n",
    "    # Calcola le medie\n",
    "    avg_correct_with_context = correct_with_context / has_answer_in_context if has_answer_in_context > 0 else 0\n",
    "    avg_correct_without_context = correct_without_context / no_answer_in_context if no_answer_in_context > 0 else 0\n",
    "    overall_accuracy = total_correct / total_examples if total_examples > 0 else 0\n",
    "\n",
    "    return {\n",
    "        \"total_examples\": total_examples,\n",
    "        \"examples_with_answer_in_context\": has_answer_in_context,\n",
    "        \"examples_without_answer_in_context\": no_answer_in_context,\n",
    "        \"correct_with_context\": correct_with_context,\n",
    "        \"correct_without_context\": correct_without_context,\n",
    "        \"average_correct_with_context\": avg_correct_with_context,\n",
    "        \"average_correct_without_context\": avg_correct_without_context,\n",
    "        \"overall_accuracy\": overall_accuracy\n",
    "    }\n",
    "\n",
    "\n",
    "# Esempio di utilizzo\n",
    "path = r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\gen_res_example_llm\\nq\\gemma-2-2b-it\\test\\retrieved\\contriever\\1_doc\\numdoc1_retr1_template_info_all_extended.json'\n",
    "result = analyze_json_responses(path)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LLM...\n",
      "LLM loaded\n",
      "Loading corpus and search results...\n",
      "Corpus and search results loaded\n",
      "Loading prompt dataset...\n",
      "Prompt dataset loaded\n",
      "INFO:\n",
      "DATA: C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\10k_train_dataset.json\n",
      "USE TEST: False\n",
      "MODEL: google/flan-t5-large\n",
      "MODEL MAX LENGTH: 4096\n",
      "MAX NEW TOKENS: 50\n",
      "USE MODEL CHAT TEMPLATE: False\n",
      "TASK WITH PROOF: False\n",
      "GOLD POSITION: None\n",
      "NUM DOCUMENTS IN CONTEXT: 3\n",
      "BATCH SIZE: None\n",
      "SAVE EVERY: 250\n",
      "Saving DataLoader contents to JSON...\n",
      "DataLoader contents saved to C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\dataloader_contents.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from typing import Tuple, Dict, Optional\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "from utils import *\n",
    "from bgm import BGM\n",
    "from default_prompts import *\n",
    "from prompt_dataset import PromptDataset\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "device = torch.device(f\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "warnings.filterwarnings('ignore')\n",
    "SEED=10\n",
    "\n",
    "info = {\n",
    "    \"nq_bgm\": {\n",
    "        \"train\": {\n",
    "            \"data_path\": r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\10k_train_dataset.json',\n",
    "            \"contriever_search_results_path\": r\"C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\processed\\contriever_search_results_at150.pkl\",\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "def save_dataloader_to_json(dataloader, output_file, num_examples=15):\n",
    "    all_batches = []\n",
    "\n",
    "    print(\"Saving DataLoader contents to JSON...\")\n",
    "    for idx, batch in enumerate(dataloader):\n",
    "        if idx >= num_examples:  # Stop after saving the specified number of examples\n",
    "            break\n",
    "\n",
    "        batch_dict = {}\n",
    "        for key, value in batch.items():\n",
    "            # Convert tensors to lists for JSON serialization\n",
    "            if isinstance(value, torch.Tensor):\n",
    "                batch_dict[key] = value.tolist()\n",
    "            else:\n",
    "                batch_dict[key] = value\n",
    "        all_batches.append(batch_dict)\n",
    "    \n",
    "    # Save the entire list of dictionaries to a JSON file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_batches, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"DataLoader contents saved to {output_file}\")\n",
    "\n",
    "class DotDict:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "def parse_arguments(custom_args=None):\n",
    "    \"\"\"\n",
    "    Mimics argparse to parse arguments for LLM generation. Accepts custom arguments as a dictionary for notebooks.\n",
    "    \"\"\"\n",
    "    # Define default values\n",
    "    default_args = {\n",
    "        'output_dir': r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\gen_id_document_bgm',\n",
    "        'llm_id': 'google/flan-t5-large',\n",
    "        'dataset': 'nq_bgm',\n",
    "        'model_max_length': 4096,\n",
    "        'quantization_bits': 4,\n",
    "        'use_model_chat_template': False, \n",
    "        'gold_position': None,\n",
    "        'num_retrieved_documents': 5,\n",
    "        'use_test': False,\n",
    "        'max_new_tokens': 50,\n",
    "        'use_task_with_proof': False,\n",
    "        'batch_size': None,\n",
    "        'save_every': 250,\n",
    "    }\n",
    "\n",
    "    # If custom_args is provided, update defaults\n",
    "    if custom_args:\n",
    "        default_args.update(custom_args)\n",
    "\n",
    "    # Perform validation\n",
    "    if default_args['num_retrieved_documents'] is None:\n",
    "        raise ValueError(\"'num_retrieved_documents' must be specified.\")\n",
    "    if default_args['num_retrieved_documents'] <= 0:\n",
    "        raise ValueError(\"'num_retrieved_documents' must be a positive integer.\")\n",
    "    if default_args['gold_position'] is not None:\n",
    "        if (default_args['gold_position'] < 0 or \n",
    "            default_args['gold_position'] >= default_args['num_retrieved_documents']):\n",
    "            raise ValueError(\"'gold_position' must be within the range of 'num_retrieved_documents'.\")\n",
    "\n",
    "    return DotDict(**default_args)\n",
    "\n",
    "\n",
    "def load_corpus(\n",
    "    args: argparse.Namespace\n",
    ") -> Tuple[List[Dict], Optional[Dict[int, int]]]:\n",
    "    \n",
    "    # Corpus with documents from Contriever\n",
    "    corpus, full_to_subset_idx_map = read_corpus_with_contriever()\n",
    "\n",
    "    return corpus, full_to_subset_idx_map\n",
    "\n",
    "def load_search_results(args: argparse.Namespace) -> List[Tuple[List[int], List[float]]]:\n",
    "\n",
    "    search_results_path = info[args.dataset][args.split]['contriever_search_results_path']\n",
    "    retriever_search_results = read_pickle(search_results_path)\n",
    "\n",
    "    return retriever_search_results\n",
    "\n",
    "\n",
    "def get_prompt_template(args: argparse.Namespace):\n",
    "    prompt_configuration = args.dataset\n",
    "    if args.use_model_chat_template:\n",
    "        chat_task_template_str = chat_task_templates[args.llm_id]['template']\n",
    "        \n",
    "        task_instruction = task_instructions[prompt_configuration]\n",
    "\n",
    "        prompt_template = apply_chat_task_template(chat_task_template_str, task_instruction)\n",
    "    else:\n",
    "        task_template = task_templates[prompt_configuration]\n",
    "\n",
    "        prompt_template = task_template.create_prompt_template()\n",
    "\n",
    "    return prompt_template\n",
    "\n",
    "\n",
    "def initialize_dataset_and_loader(\n",
    "    args: argparse.Namespace, \n",
    "    corpus: List[Dict], \n",
    "    full_to_subset_idx_map: Optional[Dict[int, int]], \n",
    "    retriever_search_results: List[Tuple[List[int], List[float]]], \n",
    "    tokenizer: PreTrainedTokenizer\n",
    ") -> DataLoader:\n",
    "    \n",
    "    prompt_template = get_prompt_template(args)\n",
    "    \n",
    "    prompt_ds = PromptDataset(\n",
    "        corpus=corpus, data_path=info[args.dataset][args.split]['data_path'], \n",
    "        tokenizer=tokenizer, \n",
    "        max_tokenized_length=args.model_max_length - 2, \n",
    "        search_results=retriever_search_results,\n",
    "        prompt_template=prompt_template,\n",
    "        full_to_subset_idx_map=full_to_subset_idx_map,\n",
    "        do_normalize_query=True, \n",
    "        num_documents_in_context=args.num_retrieved_documents,\n",
    "        gold_position=args.gold_position, # None in these experiments\n",
    "    )\n",
    "        \n",
    "    prompt_dataloader = DataLoader(\n",
    "        prompt_ds,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=8,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    return prompt_dataloader\n",
    "\n",
    "\n",
    "def print_info(args: argparse.Namespace):\n",
    "    print(\"INFO:\")    \n",
    "    print(f\"DATA: {info[args.dataset][args.split]['data_path']}\")\n",
    "    print(f\"USE TEST: {args.use_test}\")\n",
    "    print(f\"MODEL: {args.llm_id}\")\n",
    "    print(f\"MODEL MAX LENGTH: {args.model_max_length}\")\n",
    "    print(f'MAX NEW TOKENS: {args.max_new_tokens}')\n",
    "    print(f\"USE MODEL CHAT TEMPLATE: {args.use_model_chat_template}\")\n",
    "    print(f\"TASK WITH PROOF:\", args.use_task_with_proof)\n",
    "    print(f\"GOLD POSITION: {args.gold_position}\")\n",
    "    print(f\"NUM DOCUMENTS IN CONTEXT: {args.num_retrieved_documents}\")\n",
    "    print(f\"BATCH SIZE: {args.batch_size}\")\n",
    "    print(f\"SAVE EVERY: {args.save_every}\")\n",
    "\n",
    "\n",
    "def extract_generate_answers(\n",
    "    args: argparse.Namespace, \n",
    "    generated_output: List[str]\n",
    ") -> List[str]:\n",
    "    answer_prefix = \"Answer:\"\n",
    "    if args.use_model_chat_template:\n",
    "        answer_prefix = re.escape(chat_task_templates[args.llm_id]['answer_prefix'])\n",
    "\n",
    "    generated_answers = []\n",
    "    for output in generated_output:\n",
    "        matches = list(re.finditer(answer_prefix, output))\n",
    "        match_idx = 0\n",
    "\n",
    "        # When using the proof there is a one-shot example that already \n",
    "        # contains the string \"Answer:\". Thus, we should get the second (match_idx=1) match.\n",
    "        if args.use_model_chat_template and answer_prefix != \"Answer:\":\n",
    "            match_idx = 0\n",
    " \n",
    "        answer_end = matches[match_idx].end()\n",
    "        response = output[answer_end:].strip()\n",
    "        generated_answers.append(response)\n",
    "    \n",
    "    return generated_answers\n",
    "\n",
    "\n",
    "def BGMTraining(\n",
    "    args: argparse.Namespace, \n",
    "    prompt_ds: PromptDataset,\n",
    "    llm: BGM, \n",
    "    prompt_dataloader: DataLoader\n",
    "):\n",
    "    # Info from arguments\n",
    "    llm_id = args.llm_id\n",
    "    num_doc = args.num_retrieved_documents\n",
    "    save_every = args.save_every\n",
    "    retriever_str = \"contriever\" \n",
    "    chat_template_str = \"_template\" if args.use_model_chat_template else \"\"\n",
    "    prompt_type = \"retrieved_proof\" if args.use_task_with_proof else \"retrieved\"\n",
    "\n",
    "    # Create the saving directory\n",
    "    llm_folder = llm_id.split(\"/\")[1] if '/' in llm_id else llm_id\n",
    "    saving_dir = f\"{args.output_dir}/{args.dataset}/{llm_folder}/{args.split}/{prompt_type}/{retriever_str}/{num_doc}_doc\"\n",
    "    os.makedirs(saving_dir, exist_ok=True)\n",
    "\n",
    "    all_info = []  \n",
    "    for idx, prompt_batch in enumerate(tqdm(prompt_dataloader)):\n",
    "        prompts = prompt_batch['prompt']\n",
    "        example_id = prompt_batch['example_id']\n",
    "        prompts = prompt_batch['prompt']\n",
    "        query = prompt_batch['query']\n",
    "        document_indices=prompt_batch['document_indices']\n",
    "        \n",
    "        for doc_idx in document_indices:\n",
    "            \n",
    "            candidate_docs += doc_idx\n",
    "\n",
    "            formatted_docs, _ = prompt_ds._get_documents_from_indices(candidate_docs)\n",
    "\n",
    "            if '\\nAnswer:' not in candidate_prompt:\n",
    "                candidate_prompt += '\\nAnswer:'\n",
    "\n",
    "            \n",
    "\n",
    "        generated_output = llm.generate(\n",
    "            prompts, \n",
    "            max_new_tokens=args.max_new_tokens\n",
    "        )\n",
    "\n",
    "        generated_answers = extract_generate_answers(args, generated_output)\n",
    "        prompt_batch['generated_answer'] = generated_answers\n",
    "        \n",
    "        all_info.append(prompt_batch)\n",
    "        '''\n",
    "        if (idx + 1) % save_every == 0 or (idx + 1) == len(prompt_dataloader):\n",
    "            print(f\"Saving at {idx + 1}...\")\n",
    "            file_name = f\"{saving_dir}/numdoc{num_doc}_retr{args.num_retrieved_documents}{chat_template_str}_info_{idx+1}.pkl\"\n",
    "            write_pickle(all_info, file_name)\n",
    "            all_info = []\n",
    "        '''\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = parse_arguments()\n",
    "\n",
    "    args.split = \"test\" if args.use_test else \"train\"\n",
    "\n",
    "    print(\"Loading LLM...\")\n",
    "    llm_id = args.llm_id\n",
    "    bgm = BGM(\n",
    "        llm_id, device,  \n",
    "        model_max_length=args.model_max_length\n",
    "    )\n",
    "    tokenizer = bgm.tokenizer\n",
    "    print(\"LLM loaded\")\n",
    "\n",
    "\n",
    "    print(\"Loading corpus and search results...\")\n",
    "    corpus, full_to_subset_idx_map = load_corpus(args)\n",
    "    retriever_search_results = load_search_results(args)\n",
    "    print(\"Corpus and search results loaded\")\n",
    "\n",
    "\n",
    "    print(\"Loading prompt dataset...\")\n",
    "    prompt_ds, prompt_dataloader = initialize_dataset_and_loader(\n",
    "        args, corpus, full_to_subset_idx_map, \n",
    "        retriever_search_results, tokenizer\n",
    "    )\n",
    "    print(\"Prompt dataset loaded\")\n",
    "\n",
    "    print_info(args)\n",
    "\n",
    "    #output_json_path = r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\dataloader_contents.json'\n",
    "    #save_dataloader_to_json(prompt_dataloader, output_json_path, num_examples=15)\n",
    "        \n",
    "    BGMTraining(args, prompt_ds, bgm, prompt_dataloader)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    seed_everything(SEED)\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File aggiornato salvato in: C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\gen_ids_document_training_set_bgm\\nq_training\\gemma-2-2b-it\\train\\retrieved\\contriever\\5_doc\\numdoc5_retr5_template_info_all_extended_updated.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def match_example_ids(file1_path, file2_path, output_path):\n",
    "    \"\"\"\n",
    "    Modifica il file1 aggiungendo l'example_id da file2 quando query e question corrispondono.\n",
    "\n",
    "    Args:\n",
    "        file1_path (str): Percorso al file JSON di input 1.\n",
    "        file2_path (str): Percorso al file JSON di input 2.\n",
    "        output_path (str): Percorso al file JSON di output aggiornato.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Caricamento dei file JSON\n",
    "        with open(file1_path, 'r') as f1:\n",
    "            file1 = json.load(f1)\n",
    "\n",
    "        with open(file2_path, 'r') as f2:\n",
    "            file2 = json.load(f2)\n",
    "\n",
    "        # Creazione di un dizionario per mappare le domande agli example_id\n",
    "        question_to_example_id = {item['question']: item['example_id'] for item in file2}\n",
    "\n",
    "        # Modifica del primo file\n",
    "        for entry in file1:\n",
    "            query = entry.get('query')\n",
    "            if query in question_to_example_id:\n",
    "                entry['example_id'] = question_to_example_id[query]\n",
    "\n",
    "        # Salvataggio del file aggiornato\n",
    "        with open(output_path, 'w') as f1_updated:\n",
    "            json.dump(file1, f1_updated, indent=4)\n",
    "\n",
    "        print(f\"File aggiornato salvato in: {output_path}\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Errore: {e}\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Errore nel parsing del file JSON: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Errore imprevisto: {e}\")\n",
    "\n",
    "def update_queries_with_document_indices(file1_path, file2_path, output_path):\n",
    "    # Carica i dati dai file JSON\n",
    "    with open(file1_path, 'r', encoding='utf-8') as f1, open(file2_path, 'r', encoding='utf-8') as f2:\n",
    "        file1_data = json.load(f1)\n",
    "        file2_data = json.load(f2)\n",
    "\n",
    "    # Crea un dizionario per mappare le query ai document_indices di File 2\n",
    "    query_to_indices = {\n",
    "        entry['query']: entry.get('document_indices', [])\n",
    "        for entry in file2_data\n",
    "    }\n",
    "\n",
    "    # Aggiorna File 1 aggiungendo i document_indices associati alle query\n",
    "    for entry in file1_data:\n",
    "        query = entry['query']\n",
    "        if query in query_to_indices:\n",
    "            entry['document_indices'] = query_to_indices[query]\n",
    "\n",
    "    # Salva il risultato in un nuovo file JSON\n",
    "    with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "        json.dump(file1_data, output_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "\n",
    "path_output=r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\gen_ids_document_training_set_bgm\\nq_training\\gemma-2-2b-it\\train\\retrieved\\contriever\\5_doc\\numdoc5_retr5_template_info_all_extended_updated.json'\n",
    "file_da_modificare = r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\gen_ids_document_training_set_bgm\\nq_training\\gemma-2-2b-it\\train\\retrieved\\contriever\\5_doc\\numdoc5_retr5_template_info_all_extended.json'\n",
    "file_di_confronto = r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\10k_train_dataset.json'\n",
    "\n",
    "match_example_ids(file_da_modificare, file_di_confronto, path_output)\n",
    "\n",
    "update_queries_with_document_indices(r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\gen_ids_document_training_set_bgm\\nq_training\\gemma-2-2b-it\\train\\retrieved\\contriever\\5_doc\\numdoc5_retr5_template_info_all_extended_updated.json', r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\gen_ids_document_training_set_bgm\\nq_training\\gemma-2-2b-it\\train\\retrieved\\contriever\\5_doc\\numdoc5_retr5_template_info_all_extended_updated.json', r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\gen_ids_document_training_set_bgm\\nq_training\\gemma-2-2b-it\\train\\retrieved\\contriever\\5_doc\\numdoc5_retr5_template_info_all_extended_updated_last.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "# Percentuali per ogni caso\n",
    "percentages = {\n",
    "    \"case_1_single_doc\": 0.1,\n",
    "    \"case_2_multiple_docs\": 0.2,\n",
    "    \"case_3_no_docs\": 0.1,\n",
    "    \"case_4_less_docs\": 0.4,\n",
    "    \"case_5_reranking\": 0.2,\n",
    "}\n",
    "\n",
    "# Task instruction da aggiungere a ogni query\n",
    "task_instruction = \"Output only the document IDs relevant to the query. Use this format: [ID1, ID2, ...].\"\n",
    "\n",
    "def process_data(input_file, output_file):\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        examples = json.load(f)\n",
    "\n",
    "    dataset = []\n",
    "    \n",
    "    # Shuffle examples to ensure random sampling\n",
    "    random.shuffle(examples)\n",
    "\n",
    "    # Total examples to be processed for each case\n",
    "    total_examples = len(examples)\n",
    "    case_limits = {case: int(total_examples * perc) for case, perc in percentages.items()}\n",
    "    case_counters = {case: 0 for case in percentages}\n",
    "\n",
    "    for example in examples:\n",
    "        if all(count >= case_limits[case] for case, count in case_counters.items()):\n",
    "            break  # Stop if all case limits are met\n",
    "        \n",
    "        query = f\"Task Instruction: {task_instruction}\\nQuestion:{example['query']}\"  # Aggiunge la task instruction\n",
    "        retrieved_docs = example[\"document_indices\"]\n",
    "        selected_docs = example[\"selected_documents\"]\n",
    "        are_answer = example[\"are_answer\"]\n",
    "\n",
    "        # Case 1: Single document correct answer\n",
    "        if are_answer and len(selected_docs) == 1 and case_counters[\"case_1_single_doc\"] < case_limits[\"case_1_single_doc\"]:\n",
    "            dataset.append({\n",
    "                \"input\": {\n",
    "                    \"query\": query,\n",
    "                    \"retrieved_docs\": retrieved_docs,\n",
    "                },\n",
    "                \"output\": selected_docs,\n",
    "            })\n",
    "            case_counters[\"case_1_single_doc\"] += 1\n",
    "\n",
    "        # Case 2: Multiple documents correct answer\n",
    "        elif are_answer and len(selected_docs) > 1 and case_counters[\"case_2_multiple_docs\"] < case_limits[\"case_2_multiple_docs\"]:\n",
    "            dataset.append({\n",
    "                \"input\": {\n",
    "                    \"query\": query,\n",
    "                    \"retrieved_docs\": retrieved_docs,\n",
    "                },\n",
    "                \"output\": selected_docs,\n",
    "            })\n",
    "            case_counters[\"case_2_multiple_docs\"] += 1\n",
    "\n",
    "        # Case 3: No documents correct answer\n",
    "        elif are_answer and len(selected_docs) == 0 and case_counters[\"case_3_no_docs\"] < case_limits[\"case_3_no_docs\"]:\n",
    "            dataset.append({\n",
    "                \"input\": {\n",
    "                    \"query\": query,\n",
    "                    \"retrieved_docs\": [],\n",
    "                },\n",
    "                \"output\": [],\n",
    "            })\n",
    "            case_counters[\"case_3_no_docs\"] += 1\n",
    "\n",
    "        # Case 4: Input and output unchanged\n",
    "        elif are_answer and len(selected_docs) > 2 and case_counters[\"case_4_less_docs\"] < case_limits[\"case_4_less_docs\"]:\n",
    "            dataset.append({\n",
    "                \"input\": {\n",
    "                    \"query\": query,\n",
    "                    \"retrieved_docs\": selected_docs,\n",
    "                },\n",
    "                \"output\": selected_docs,\n",
    "            })\n",
    "            case_counters[\"case_4_less_docs\"] += 1\n",
    "\n",
    "        # Case 5: Reranking\n",
    "        elif are_answer and len(selected_docs) > 2 and case_counters[\"case_5_reranking\"] < case_limits[\"case_5_reranking\"]:\n",
    "            reranked_docs = random.sample(selected_docs, len(selected_docs))  # Randomize order\n",
    "            dataset.append({\n",
    "                \"input\": {\n",
    "                    \"query\": query,\n",
    "                    \"retrieved_docs\": reranked_docs,\n",
    "                },\n",
    "                \"output\": selected_docs,\n",
    "            })\n",
    "            case_counters[\"case_5_reranking\"] += 1\n",
    "\n",
    "    # Save the dataset to a file\n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(dataset, f, indent=4)\n",
    "\n",
    "# Path to input and output files\n",
    "input_file = r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\gen_ids_document_training_set_bgm\\nq_training\\gemma-2-2b-it\\train\\retrieved\\contriever\\5_doc\\numdoc5_retr5_info_all_extended_training_set.json'\n",
    "output_file = r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\training_dataset.json'\n",
    "\n",
    "process_data(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Totale esempi nel file di input: 3000\n",
      "Esempi con 'are_answer=True': 1233\n",
      "Esempi con 'selected_documents == 0': 366\n",
      "Esempi con 'selected_documents == 1': 736\n",
      "Esempi con 'selected_documents > 1': 131\n",
      "Distribuzione pianificata degli esempi nel dataset creato:\n",
      "case_1_single_doc: 51\n",
      "case_2_multiple_docs: 52\n",
      "case_3_no_docs: 36\n",
      "case_4_multi_doc_unchanged: 45\n",
      "case_5_reranking: 65\n",
      "case_6_single_doc_unchanged: 36\n",
      "Esempi effettivamente inclusi nel dataset creato:\n",
      "case_1_single_doc: 51\n",
      "case_2_multiple_docs: 52\n",
      "case_3_no_docs: 36\n",
      "case_4_multi_doc_unchanged: 45\n",
      "case_5_reranking: 34\n",
      "case_6_single_doc_unchanged: 36\n",
      "Totale degli Esempi inclusi nel training dataset creato: 254\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "# Percentuali per ogni caso\n",
    "percentages = {\n",
    "    \"case_1_single_doc\": 0.07,\n",
    "    \"case_2_multiple_docs\": 0.4,\n",
    "    \"case_3_no_docs\": 0.1,\n",
    "    \"case_4_multi_doc_unchanged\": 0.35,\n",
    "    \"case_5_reranking\": 0.5,\n",
    "    \"case_6_single_doc_unchanged\": 0.05,\n",
    "}\n",
    "\n",
    "# Task instruction da aggiungere a ogni query\n",
    "task_instruction = \"Output only the document IDs relevant to the query. Use this format: [ID1, ID2, ...].\"\n",
    "\n",
    "def process_data(input_file, output_file):\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        examples = json.load(f)\n",
    "\n",
    "    # Filtra gli esempi con are_answer = true\n",
    "    valid_examples = [ex for ex in examples if ex[\"are_answer\"] is True]\n",
    "\n",
    "    print(f\"Totale esempi nel file di input: {len(examples)}\")\n",
    "    print(f\"Esempi con 'are_answer=True': {len(valid_examples)}\")\n",
    "\n",
    "    # Raggruppa per numero di selected_documents\n",
    "    grouped_examples = {\n",
    "        \"len_0\": [ex for ex in valid_examples if len(ex[\"selected_documents\"]) == 0],\n",
    "        \"len_1\": [ex for ex in valid_examples if len(ex[\"selected_documents\"]) == 1],\n",
    "        \"len_gt_1\": [ex for ex in valid_examples if len(ex[\"selected_documents\"]) > 1],\n",
    "    }\n",
    "\n",
    "    print(f\"Esempi con 'selected_documents == 0': {len(grouped_examples['len_0'])}\")\n",
    "    print(f\"Esempi con 'selected_documents == 1': {len(grouped_examples['len_1'])}\")\n",
    "    print(f\"Esempi con 'selected_documents > 1': {len(grouped_examples['len_gt_1'])}\")\n",
    "\n",
    "    # Calcola le suddivisioni per ogni gruppo\n",
    "    group_case_limits = {\n",
    "        \"case_1_single_doc\": int(len(grouped_examples[\"len_1\"]) * percentages[\"case_1_single_doc\"]),\n",
    "        \"case_2_multiple_docs\": int(len(grouped_examples[\"len_gt_1\"]) * percentages[\"case_2_multiple_docs\"]),\n",
    "        \"case_3_no_docs\": int(len(grouped_examples[\"len_0\"]) * percentages[\"case_3_no_docs\"]),\n",
    "        \"case_4_multi_doc_unchanged\": int(len(grouped_examples[\"len_gt_1\"]) * percentages[\"case_4_multi_doc_unchanged\"]),\n",
    "        \"case_5_reranking\": int(len(grouped_examples[\"len_gt_1\"]) * percentages[\"case_5_reranking\"]),\n",
    "        \"case_6_single_doc_unchanged\": int(len(grouped_examples[\"len_1\"]) * percentages[\"case_6_single_doc_unchanged\"]),\n",
    "    }\n",
    "\n",
    "    print(\"Distribuzione pianificata degli esempi nel dataset creato:\")\n",
    "    for case, limit in group_case_limits.items():\n",
    "        print(f\"{case}: {limit}\")\n",
    "\n",
    "    dataset = []\n",
    "    case_counters = {case: 0 for case in group_case_limits}\n",
    "\n",
    "    # Processa gli esempi\n",
    "    for example in valid_examples:\n",
    "        query = f\"Task Instruction: {task_instruction}\\nQuestion:{example['query']}\"\n",
    "        retrieved_docs = example[\"document_indices\"]\n",
    "        selected_docs = example[\"selected_documents\"]\n",
    "\n",
    "        # Case 1: Single document correct answer\n",
    "        if len(selected_docs) == 1 and case_counters[\"case_1_single_doc\"] < group_case_limits[\"case_1_single_doc\"]:\n",
    "            dataset.append({\n",
    "                \"input\": {\n",
    "                    \"query\": query,\n",
    "                    \"retrieved_docs\": retrieved_docs,\n",
    "                },\n",
    "                \"output\": selected_docs,\n",
    "            })\n",
    "            case_counters[\"case_1_single_doc\"] += 1\n",
    "\n",
    "        # Case 2: Multiple documents correct answer\n",
    "        elif len(selected_docs) > 1 and case_counters[\"case_2_multiple_docs\"] < group_case_limits[\"case_2_multiple_docs\"]:\n",
    "            dataset.append({\n",
    "                \"input\": {\n",
    "                    \"query\": query,\n",
    "                    \"retrieved_docs\": retrieved_docs,\n",
    "                },\n",
    "                \"output\": selected_docs,\n",
    "            })\n",
    "            case_counters[\"case_2_multiple_docs\"] += 1\n",
    "\n",
    "        # Case 3: No documents correct answer\n",
    "        elif len(selected_docs) == 0 and case_counters[\"case_3_no_docs\"] < group_case_limits[\"case_3_no_docs\"]:\n",
    "            dataset.append({\n",
    "                \"input\": {\n",
    "                    \"query\": query,\n",
    "                    \"retrieved_docs\": [],\n",
    "                },\n",
    "                \"output\": [],\n",
    "            })\n",
    "            case_counters[\"case_3_no_docs\"] += 1\n",
    "\n",
    "        # Case 4: Input and output unchanged for multiple docs\n",
    "        elif len(selected_docs) > 1 and case_counters[\"case_4_multi_doc_unchanged\"] < group_case_limits[\"case_4_multi_doc_unchanged\"]:\n",
    "            dataset.append({\n",
    "                \"input\": {\n",
    "                    \"query\": query,\n",
    "                    \"retrieved_docs\": selected_docs,\n",
    "                },\n",
    "                \"output\": selected_docs,\n",
    "            })\n",
    "            case_counters[\"case_4_multi_doc_unchanged\"] += 1\n",
    "\n",
    "        # Case 6: Input and output unchanged for single doc\n",
    "        elif len(selected_docs) == 1 and case_counters[\"case_6_single_doc_unchanged\"] < group_case_limits[\"case_6_single_doc_unchanged\"]:\n",
    "            dataset.append({\n",
    "                \"input\": {\n",
    "                    \"query\": query,\n",
    "                    \"retrieved_docs\": selected_docs,\n",
    "                },\n",
    "                \"output\": selected_docs,\n",
    "            })\n",
    "            case_counters[\"case_6_single_doc_unchanged\"] += 1\n",
    "\n",
    "        # Case 5: Reranking\n",
    "        elif len(selected_docs) > 1 and case_counters[\"case_5_reranking\"] < group_case_limits[\"case_5_reranking\"]:\n",
    "            reranked_docs = selected_docs[:]\n",
    "            while reranked_docs == selected_docs:  # Garantisce che l'ordine sia diverso\n",
    "                reranked_docs = random.sample(selected_docs, len(selected_docs))\n",
    "            dataset.append({\n",
    "                \"input\": {\n",
    "                    \"query\": query,\n",
    "                    \"retrieved_docs\": reranked_docs,\n",
    "                },\n",
    "                \"output\": selected_docs,\n",
    "            })\n",
    "            case_counters[\"case_5_reranking\"] += 1\n",
    "\n",
    "    print(\"Esempi effettivamente inclusi nel dataset creato:\")\n",
    "    tot=0\n",
    "    for case, count in case_counters.items():\n",
    "        tot += count\n",
    "        print(f\"{case}: {count}\")\n",
    "\n",
    "    print(f\"Totale degli Esempi inclusi nel training dataset creato: {tot}\")\n",
    "    \n",
    "\n",
    "    # Save the dataset to a file\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(dataset, f, indent=4)\n",
    "\n",
    "# Path to input and output files\n",
    "input_file = r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\gen_ids_document_training_set_bgm\\nq_training\\gemma-2-2b-it\\train\\retrieved\\contriever\\5_doc\\numdoc5_retr5_info_all_extended_training_set.json'\n",
    "output_file = r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\training_dataset.json'\n",
    "\n",
    "process_data(input_file, output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
