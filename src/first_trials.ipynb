{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "2\n",
      "NVIDIA GeForce RTX 3080\n",
      "NVIDIA GeForce RTX 3060\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total_examples': 2889, 'examples_with_answer_in_context': 1483, 'examples_without_answer_in_context': 1406, 'correct_with_context': 718, 'correct_without_context': 25, 'average_correct_with_context': 0.48415374241402565, 'average_correct_without_context': 0.017780938833570414, 'overall_accuracy': 0.25718241606092074}\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "\n",
    "def analyze_json_responses(my_path):\n",
    "    data = read_json(my_path)  # Legge il file JSON\n",
    "\n",
    "    total_examples = len(data)  # Numero totale di esempi\n",
    "    correct_with_context = 0  # Risposte corrette con risposta nel contesto\n",
    "    correct_without_context = 0  # Risposte corrette senza risposta nel contesto\n",
    "    has_answer_in_context = 0  # Esempi con risposta nel contesto\n",
    "    no_answer_in_context = 0  # Esempi senza risposta nel contesto\n",
    "    total_correct = 0  # Totale delle risposte corrette\n",
    "\n",
    "    for entry in data:\n",
    "        ans_in_documents = entry.get(\"ans_in_documents\", False)\n",
    "        ans_match_after_norm = entry.get(\"ans_match_after_norm\", False)\n",
    "\n",
    "        if ans_in_documents:\n",
    "            has_answer_in_context += 1\n",
    "            if ans_match_after_norm:\n",
    "                correct_with_context += 1\n",
    "        else:\n",
    "            no_answer_in_context += 1\n",
    "            if ans_match_after_norm:\n",
    "                correct_without_context += 1\n",
    "\n",
    "        # Conta ogni risposta corretta\n",
    "        if ans_match_after_norm:\n",
    "            total_correct += 1\n",
    "\n",
    "    # Calcola le medie\n",
    "    avg_correct_with_context = correct_with_context / has_answer_in_context if has_answer_in_context > 0 else 0\n",
    "    avg_correct_without_context = correct_without_context / no_answer_in_context if no_answer_in_context > 0 else 0\n",
    "    overall_accuracy = total_correct / total_examples if total_examples > 0 else 0\n",
    "\n",
    "    return {\n",
    "        \"total_examples\": total_examples,\n",
    "        \"examples_with_answer_in_context\": has_answer_in_context,\n",
    "        \"examples_without_answer_in_context\": no_answer_in_context,\n",
    "        \"correct_with_context\": correct_with_context,\n",
    "        \"correct_without_context\": correct_without_context,\n",
    "        \"average_correct_with_context\": avg_correct_with_context,\n",
    "        \"average_correct_without_context\": avg_correct_without_context,\n",
    "        \"overall_accuracy\": overall_accuracy\n",
    "    }\n",
    "\n",
    "\n",
    "# Esempio di utilizzo\n",
    "path = r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\gen_res_example_llm\\nq\\gemma-2-2b-it\\test\\retrieved\\contriever\\5_doc\\numdoc5_retr5_template_info_all_extended.json'\n",
    "result = analyze_json_responses(path)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Vedere l'accuracy dell'LLM passando i 5 Doc retrivati nei casi in comune con la risposta nel contesto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esempi comuni con ans_in_documents=True in entrambi i dataset: 520\n",
      "Accuracy (gen_bgm): 60.3846%\n",
      "Accuracy (gen_only_llm): 55.3846%\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "\n",
    "def compare_json_data(gen_bgm, gen_only_llm):\n",
    "    \"\"\"\n",
    "    Confronta due liste di dati JSON, trova gli esempi comuni con ans_in_documents=True in entrambi i dataset\n",
    "    e calcola l'accuracy per entrambi basandosi su ans_match_after_norm.\n",
    "\n",
    "    Args:\n",
    "        gen_bgm (list): Lista di dizionari JSON dal primo file (con BGM).\n",
    "        gen_only_llm (list): Lista di dizionari JSON dal secondo file (solo LLM).\n",
    "\n",
    "    Returns:\n",
    "        tuple: (lista di esempi comuni, accuracy_gen_bgm, accuracy_gen_only_llm)\n",
    "    \"\"\"\n",
    "    # Convertire in dizionari per ricerca veloce\n",
    "    gen_bgm_dict = {entry[\"example_id\"]: entry for entry in gen_bgm}\n",
    "    gen_only_llm_dict = {entry[\"example_id\"]: entry for entry in gen_only_llm}\n",
    "\n",
    "    # Trovare gli example_id in comune che hanno ans_in_documents=True in entrambi i dataset\n",
    "    common_ids = {\n",
    "        example_id for example_id in gen_bgm_dict.keys() & gen_only_llm_dict.keys()\n",
    "        if gen_bgm_dict[example_id].get(\"ans_in_documents\") is True and \n",
    "           gen_only_llm_dict[example_id].get(\"ans_in_documents\") is True\n",
    "    }\n",
    "\n",
    "    # Filtrare gli esempi in comune con ans_in_documents=True\n",
    "    common_examples = [gen_bgm_dict[example_id] for example_id in common_ids]\n",
    "\n",
    "    # Funzione per calcolare l'accuracy basata su ans_match_after_norm\n",
    "    def calculate_accuracy(dataset):\n",
    "        correct = sum(1 for example_id in common_ids if dataset[example_id].get(\"ans_match_after_norm\", False))\n",
    "        total = len(common_ids)\n",
    "        return correct / total if total > 0 else 0\n",
    "\n",
    "    # Calcolare l'accuracy per entrambi i dataset\n",
    "    accuracy_gen_bgm = calculate_accuracy(gen_bgm_dict)\n",
    "    accuracy_gen_only_llm = calculate_accuracy(gen_only_llm_dict)\n",
    "\n",
    "    return common_examples, accuracy_gen_bgm, accuracy_gen_only_llm\n",
    "\n",
    "\n",
    "# Esempio di utilizzo con la tua funzione read_json\n",
    "gen_bgm = read_json(r\"C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\gen_res_example_llm_with_bgm\\steps_1800\\nq\\gemma-2-2b-it\\test\\retrieved\\contriever\\5_doc\\numdoc5_retr5_template_info_all_extended.json\")\n",
    "gen_only_llm = read_json(r\"C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\gen_res_example_llm\\nq\\gemma-2-2b-it\\test\\retrieved\\contriever\\5_doc\\numdoc5_retr5_template_info_all_extended.json\")\n",
    "\n",
    "common_data, accuracy_bgm, accuracy_llm = compare_json_data(gen_bgm, gen_only_llm)\n",
    "\n",
    "# Stampare i risultati\n",
    "print(f\"Esempi comuni con ans_in_documents=True in entrambi i dataset: {len(common_data)}\")\n",
    "print(f\"Accuracy (gen_bgm): {accuracy_bgm:.4%}\")\n",
    "print(f\"Accuracy (gen_only_llm): {accuracy_llm:.4%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistiche sui documenti passati nel file gen_bgm:\n",
      "num_examples_no_docs: 0\n",
      "num_examples_1_doc: 11\n",
      "num_examples_2_docs: 6\n",
      "num_examples_3_docs: 2855\n",
      "num_examples_4_docs: 15\n",
      "num_examples_5_docs: 0\n",
      "num_examples_with_relevant_doc: 520\n",
      "\n",
      "Quante volte almeno un documento rilevante è presente nei vari casi:\n",
      "0_docs: 0\n",
      "1_doc: 2\n",
      "2_docs: 0\n",
      "3_docs: 517\n",
      "4_docs: 1\n",
      "5_docs: 0\n",
      "total_examples: 2889\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def analyze_gen_bgm_stats(gen_bgm):\n",
    "    \"\"\"\n",
    "    Analizza il file gen_bgm e calcola le statistiche sui documenti passati \n",
    "    e quante volte almeno un documento è rilevante in ogni categoria.\n",
    "\n",
    "    Args:\n",
    "        gen_bgm (list): Lista di dizionari JSON dal file gen_bgm.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dizionario con statistiche sui documenti passati e sulla rilevanza.\n",
    "    \"\"\"\n",
    "    doc_count_distribution = Counter()\n",
    "    relevant_doc_count_distribution = Counter()  # Quante volte almeno un doc è rilevante in ogni caso\n",
    "\n",
    "    for example in gen_bgm:\n",
    "        # Estrarre gli ID dei documenti, escludendo quelli nel formato \"Unknown(Id_X)\"\n",
    "        bgm_indices = example.get(\"bgm_indices\", \"\").split(\",\")\n",
    "        valid_doc_ids = [doc for doc in bgm_indices if not re.match(r\"Unknown\\(Id_\\d+\\)\", doc)]\n",
    "\n",
    "        num_valid_docs = len(valid_doc_ids)  # Conta solo i documenti validi\n",
    "        doc_count_distribution[num_valid_docs] += 1  # Conta quante volte troviamo N documenti\n",
    "\n",
    "        # Verificare se almeno un documento passato è rilevante\n",
    "        if example.get(\"ans_in_documents\", False):\n",
    "            relevant_doc_count_distribution[num_valid_docs] += 1  # Conta nei casi con doc rilevanti\n",
    "\n",
    "    # Costruzione del risultato finale\n",
    "    stats = {\n",
    "        \"num_examples_no_docs\": doc_count_distribution[0],  # Nessun documento passato\n",
    "        \"num_examples_1_doc\": doc_count_distribution[1],  # Esattamente 1 documento passato\n",
    "        \"num_examples_2_docs\": doc_count_distribution[2],\n",
    "        \"num_examples_3_docs\": doc_count_distribution[3],\n",
    "        \"num_examples_4_docs\": doc_count_distribution[4],\n",
    "        \"num_examples_5_docs\": doc_count_distribution[5],  # Esattamente 5 documenti passati\n",
    "        \"num_examples_with_relevant_doc\": sum(relevant_doc_count_distribution.values()),  # Totale esempi con almeno 1 doc rilevante\n",
    "        \"relevant_docs_per_case\": {  # Quante volte almeno un doc è rilevante in ogni categoria\n",
    "            \"0_docs\": relevant_doc_count_distribution[0],\n",
    "            \"1_doc\": relevant_doc_count_distribution[1],\n",
    "            \"2_docs\": relevant_doc_count_distribution[2],\n",
    "            \"3_docs\": relevant_doc_count_distribution[3],\n",
    "            \"4_docs\": relevant_doc_count_distribution[4],\n",
    "            \"5_docs\": relevant_doc_count_distribution[5],\n",
    "        },\n",
    "        \"total_examples\": len(gen_bgm)  # Numero totale di esempi analizzati\n",
    "    }\n",
    "\n",
    "    return stats\n",
    "\n",
    "\n",
    "# Esempio di utilizzo con la tua funzione read_json\n",
    "gen_bgm = read_json(r\"C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\gen_res_example_llm_with_bgm\\steps_1800\\nq\\gemma-2-2b-it\\test\\retrieved\\contriever\\5_doc\\numdoc5_retr5_template_info_all_extended.json\")\n",
    "\n",
    "# Ottenere le statistiche\n",
    "stats = analyze_gen_bgm_stats(gen_bgm)\n",
    "\n",
    "# Stampare i risultati\n",
    "print(\"Statistiche sui documenti passati nel file gen_bgm:\")\n",
    "for key, value in stats.items():\n",
    "    if key == \"relevant_docs_per_case\":\n",
    "        print(\"\\nQuante volte almeno un documento rilevante è presente nei vari casi:\")\n",
    "        for sub_key, sub_value in value.items():\n",
    "            print(f\"{sub_key}: {sub_value}\")\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Controlla quanti esempi ha la generazione dei migliori indici con il 600 esimo steps del training\n",
    "\n",
    "Per analizzare il file JSON generato e contare quanti esempi rientrano nelle varie categorie in base al numero di ID presenti in bgm_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistiche sugli ID in bgm_indices:\n",
      "num_examples_0_docs: 287\n",
      "num_examples_1_doc: 1\n",
      "num_examples_2_docs: 1500\n",
      "num_examples_3_docs: 1101\n",
      "num_examples_4_docs: 0\n",
      "num_examples_5_docs: 0\n",
      "total_examples: 2889\n",
      "num_examples_with_answer_in_prompt_0_docs: 0\n",
      "num_examples_with_answer_in_prompt_1_doc: 0\n",
      "num_examples_with_answer_in_prompt_2_docs: 420\n",
      "num_examples_with_answer_in_prompt_3_docs: 411\n",
      "num_examples_with_answer_in_prompt_4_docs: 0\n",
      "num_examples_with_answer_in_prompt_5_docs: 0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def analyze_bgm_indices(file_path):\n",
    "    \"\"\"\n",
    "    Analizza il file JSON della generazione dei migliori indici con il 600° step del training.\n",
    "    Conta quanti esempi hanno 1,2,3,4,5 ID in bgm_indices e quanti non ne hanno trovati,\n",
    "    escludendo gli ID che contengono 'Unknown()' o 'Unknown(\"\")'.\n",
    "    Inoltre, controlla se nel prompt è presente una delle risposte nel campo \"answers\",\n",
    "    considerando solo il contenuto dopo \"Documents:\" e associandolo alla categoria di documenti trovati.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Percorso del file JSON.\n",
    "\n",
    "    Returns:\n",
    "        dict: Statistiche sul numero di ID in bgm_indices e presenza di risposte nel prompt.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Conta quante volte troviamo 0, 1, 2, 3, 4, 5 ID\n",
    "    bgm_counts = Counter()\n",
    "    answer_in_prompt_counts = Counter()\n",
    "\n",
    "    for example in data:\n",
    "        bgm_indices = example.get(\"bgm_indices\", \"\")\n",
    "        prompt = example.get(\"prompt\", \"\").lower()\n",
    "        answers = [ans.lower() for ans in example.get(\"answers\", [])]\n",
    "        \n",
    "        # Estrarre solo la parte dopo \"Documents:\" se presente\n",
    "        if \"documents:\" in prompt:\n",
    "            prompt = prompt.split(\"documents:\", 1)[-1]\n",
    "        else:\n",
    "            prompt = \"\"  # Se non c'è \"Documents:\", significa che non ci sono documenti\n",
    "\n",
    "        # Controlla se una delle risposte è presente nel prompt\n",
    "        answer_in_prompt = any(answer in prompt for answer in answers) if prompt else False\n",
    "\n",
    "        # Se non ha trovato nessun ID buono\n",
    "        if bgm_indices in [\"NO_DOCS\", \"Unknown(\"\")\"]:\n",
    "            bgm_counts[0] += 1\n",
    "            if answer_in_prompt:\n",
    "                answer_in_prompt_counts[0] += 1\n",
    "        else:\n",
    "            # Dividere gli ID separati da \",\"\n",
    "            ids = [id_.strip() for id_ in bgm_indices.split(\",\") if id_.strip()]\n",
    "\n",
    "            # Escludere gli ID che contengono \"Unknown()\" o \"Unknown('')\"\n",
    "            valid_ids = [id_ for id_ in ids if not re.match(r\"Unknown\\(.*\\)\", id_)]\n",
    "\n",
    "            # Conta quanti ID validi ci sono\n",
    "            num_valid_ids = len(valid_ids)\n",
    "\n",
    "            # Se non rimangono ID validi, conta come 0 documenti\n",
    "            bgm_counts[num_valid_ids] += 1\n",
    "            if answer_in_prompt:\n",
    "                answer_in_prompt_counts[num_valid_ids] += 1\n",
    "\n",
    "    # Creazione delle statistiche\n",
    "    stats = {\n",
    "        \"num_examples_0_docs\": bgm_counts[0],  # Nessun ID trovato\n",
    "        \"num_examples_1_doc\": bgm_counts[1],  # 1 ID in bgm_indices\n",
    "        \"num_examples_2_docs\": bgm_counts[2],  # 2 ID\n",
    "        \"num_examples_3_docs\": bgm_counts[3],  # 3 ID\n",
    "        \"num_examples_4_docs\": bgm_counts[4],  # 4 ID\n",
    "        \"num_examples_5_docs\": bgm_counts[5],  # 5 ID\n",
    "        \"total_examples\": len(data),  # Numero totale di esempi analizzati\n",
    "        \"num_examples_with_answer_in_prompt_0_docs\": answer_in_prompt_counts[0],\n",
    "        \"num_examples_with_answer_in_prompt_1_doc\": answer_in_prompt_counts[1],\n",
    "        \"num_examples_with_answer_in_prompt_2_docs\": answer_in_prompt_counts[2],\n",
    "        \"num_examples_with_answer_in_prompt_3_docs\": answer_in_prompt_counts[3],\n",
    "        \"num_examples_with_answer_in_prompt_4_docs\": answer_in_prompt_counts[4],\n",
    "        \"num_examples_with_answer_in_prompt_5_docs\": answer_in_prompt_counts[5],\n",
    "    }\n",
    "\n",
    "    return stats\n",
    "\n",
    "# Percorso del file JSON\n",
    "file_path = r\"C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\gen_best_ids_doc_test_set_according_bgm\\nq\\gemma-2-2b-it\\test\\retrieved\\contriever\\5_doc\\numdoc5_retr5_template_bgm_training_epoch1600_info.json\"\n",
    "\n",
    "# Analizza il file JSON e ottieni le statistiche\n",
    "stats = analyze_bgm_indices(file_path)\n",
    "\n",
    "# Stampare i risultati\n",
    "print(\"Statistiche sugli ID in bgm_indices:\")\n",
    "for key, value in stats.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LLM...\n",
      "LLM loaded\n",
      "Loading corpus and search results...\n",
      "Corpus and search results loaded\n",
      "Loading prompt dataset...\n",
      "Prompt dataset loaded\n",
      "INFO:\n",
      "DATA: C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\10k_train_dataset.json\n",
      "USE TEST: False\n",
      "MODEL: google/flan-t5-large\n",
      "MODEL MAX LENGTH: 4096\n",
      "MAX NEW TOKENS: 50\n",
      "USE MODEL CHAT TEMPLATE: False\n",
      "TASK WITH PROOF: False\n",
      "GOLD POSITION: None\n",
      "NUM DOCUMENTS IN CONTEXT: 3\n",
      "BATCH SIZE: None\n",
      "SAVE EVERY: 250\n",
      "Saving DataLoader contents to JSON...\n",
      "DataLoader contents saved to C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\dataloader_contents.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from typing import Tuple, Dict, Optional\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "from utils import *\n",
    "from bgm import BGM\n",
    "from default_prompts import *\n",
    "from prompt_dataset import PromptDataset\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "device = torch.device(f\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "warnings.filterwarnings('ignore')\n",
    "SEED=10\n",
    "\n",
    "info = {\n",
    "    \"nq_bgm\": {\n",
    "        \"train\": {\n",
    "            \"data_path\": r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\10k_train_dataset.json',\n",
    "            \"contriever_search_results_path\": r\"C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\processed\\contriever_search_results_at150.pkl\",\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "def save_dataloader_to_json(dataloader, output_file, num_examples=15):\n",
    "    all_batches = []\n",
    "\n",
    "    print(\"Saving DataLoader contents to JSON...\")\n",
    "    for idx, batch in enumerate(dataloader):\n",
    "        if idx >= num_examples:  # Stop after saving the specified number of examples\n",
    "            break\n",
    "\n",
    "        batch_dict = {}\n",
    "        for key, value in batch.items():\n",
    "            # Convert tensors to lists for JSON serialization\n",
    "            if isinstance(value, torch.Tensor):\n",
    "                batch_dict[key] = value.tolist()\n",
    "            else:\n",
    "                batch_dict[key] = value\n",
    "        all_batches.append(batch_dict)\n",
    "    \n",
    "    # Save the entire list of dictionaries to a JSON file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_batches, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"DataLoader contents saved to {output_file}\")\n",
    "\n",
    "class DotDict:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "def parse_arguments(custom_args=None):\n",
    "    \"\"\"\n",
    "    Mimics argparse to parse arguments for LLM generation. Accepts custom arguments as a dictionary for notebooks.\n",
    "    \"\"\"\n",
    "    # Define default values\n",
    "    default_args = {\n",
    "        'output_dir': r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\gen_id_document_bgm',\n",
    "        'llm_id': 'google/flan-t5-large',\n",
    "        'dataset': 'nq_bgm',\n",
    "        'model_max_length': 4096,\n",
    "        'quantization_bits': 4,\n",
    "        'use_model_chat_template': False, \n",
    "        'gold_position': None,\n",
    "        'num_retrieved_documents': 5,\n",
    "        'use_test': False,\n",
    "        'max_new_tokens': 50,\n",
    "        'use_task_with_proof': False,\n",
    "        'batch_size': None,\n",
    "        'save_every': 250,\n",
    "    }\n",
    "\n",
    "    # If custom_args is provided, update defaults\n",
    "    if custom_args:\n",
    "        default_args.update(custom_args)\n",
    "\n",
    "    # Perform validation\n",
    "    if default_args['num_retrieved_documents'] is None:\n",
    "        raise ValueError(\"'num_retrieved_documents' must be specified.\")\n",
    "    if default_args['num_retrieved_documents'] <= 0:\n",
    "        raise ValueError(\"'num_retrieved_documents' must be a positive integer.\")\n",
    "    if default_args['gold_position'] is not None:\n",
    "        if (default_args['gold_position'] < 0 or \n",
    "            default_args['gold_position'] >= default_args['num_retrieved_documents']):\n",
    "            raise ValueError(\"'gold_position' must be within the range of 'num_retrieved_documents'.\")\n",
    "\n",
    "    return DotDict(**default_args)\n",
    "\n",
    "\n",
    "def load_corpus(\n",
    "    args: argparse.Namespace\n",
    ") -> Tuple[List[Dict], Optional[Dict[int, int]]]:\n",
    "    \n",
    "    # Corpus with documents from Contriever\n",
    "    corpus, full_to_subset_idx_map = read_corpus_with_contriever()\n",
    "\n",
    "    return corpus, full_to_subset_idx_map\n",
    "\n",
    "def load_search_results(args: argparse.Namespace) -> List[Tuple[List[int], List[float]]]:\n",
    "\n",
    "    search_results_path = info[args.dataset][args.split]['contriever_search_results_path']\n",
    "    retriever_search_results = read_pickle(search_results_path)\n",
    "\n",
    "    return retriever_search_results\n",
    "\n",
    "\n",
    "def get_prompt_template(args: argparse.Namespace):\n",
    "    prompt_configuration = args.dataset\n",
    "    if args.use_model_chat_template:\n",
    "        chat_task_template_str = chat_task_templates[args.llm_id]['template']\n",
    "        \n",
    "        task_instruction = task_instructions[prompt_configuration]\n",
    "\n",
    "        prompt_template = apply_chat_task_template(chat_task_template_str, task_instruction)\n",
    "    else:\n",
    "        task_template = task_templates[prompt_configuration]\n",
    "\n",
    "        prompt_template = task_template.create_prompt_template()\n",
    "\n",
    "    return prompt_template\n",
    "\n",
    "\n",
    "def initialize_dataset_and_loader(\n",
    "    args: argparse.Namespace, \n",
    "    corpus: List[Dict], \n",
    "    full_to_subset_idx_map: Optional[Dict[int, int]], \n",
    "    retriever_search_results: List[Tuple[List[int], List[float]]], \n",
    "    tokenizer: PreTrainedTokenizer\n",
    ") -> DataLoader:\n",
    "    \n",
    "    prompt_template = get_prompt_template(args)\n",
    "    \n",
    "    prompt_ds = PromptDataset(\n",
    "        corpus=corpus, data_path=info[args.dataset][args.split]['data_path'], \n",
    "        tokenizer=tokenizer, \n",
    "        max_tokenized_length=args.model_max_length - 2, \n",
    "        search_results=retriever_search_results,\n",
    "        prompt_template=prompt_template,\n",
    "        full_to_subset_idx_map=full_to_subset_idx_map,\n",
    "        do_normalize_query=True, \n",
    "        num_documents_in_context=args.num_retrieved_documents,\n",
    "        gold_position=args.gold_position, # None in these experiments\n",
    "    )\n",
    "        \n",
    "    prompt_dataloader = DataLoader(\n",
    "        prompt_ds,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=8,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    return prompt_dataloader\n",
    "\n",
    "\n",
    "def print_info(args: argparse.Namespace):\n",
    "    print(\"INFO:\")    \n",
    "    print(f\"DATA: {info[args.dataset][args.split]['data_path']}\")\n",
    "    print(f\"USE TEST: {args.use_test}\")\n",
    "    print(f\"MODEL: {args.llm_id}\")\n",
    "    print(f\"MODEL MAX LENGTH: {args.model_max_length}\")\n",
    "    print(f'MAX NEW TOKENS: {args.max_new_tokens}')\n",
    "    print(f\"USE MODEL CHAT TEMPLATE: {args.use_model_chat_template}\")\n",
    "    print(f\"TASK WITH PROOF:\", args.use_task_with_proof)\n",
    "    print(f\"GOLD POSITION: {args.gold_position}\")\n",
    "    print(f\"NUM DOCUMENTS IN CONTEXT: {args.num_retrieved_documents}\")\n",
    "    print(f\"BATCH SIZE: {args.batch_size}\")\n",
    "    print(f\"SAVE EVERY: {args.save_every}\")\n",
    "\n",
    "\n",
    "def extract_generate_answers(\n",
    "    args: argparse.Namespace, \n",
    "    generated_output: List[str]\n",
    ") -> List[str]:\n",
    "    answer_prefix = \"Answer:\"\n",
    "    if args.use_model_chat_template:\n",
    "        answer_prefix = re.escape(chat_task_templates[args.llm_id]['answer_prefix'])\n",
    "\n",
    "    generated_answers = []\n",
    "    for output in generated_output:\n",
    "        matches = list(re.finditer(answer_prefix, output))\n",
    "        match_idx = 0\n",
    "\n",
    "        # When using the proof there is a one-shot example that already \n",
    "        # contains the string \"Answer:\". Thus, we should get the second (match_idx=1) match.\n",
    "        if args.use_model_chat_template and answer_prefix != \"Answer:\":\n",
    "            match_idx = 0\n",
    " \n",
    "        answer_end = matches[match_idx].end()\n",
    "        response = output[answer_end:].strip()\n",
    "        generated_answers.append(response)\n",
    "    \n",
    "    return generated_answers\n",
    "\n",
    "\n",
    "def BGMTraining(\n",
    "    args: argparse.Namespace, \n",
    "    prompt_ds: PromptDataset,\n",
    "    llm: BGM, \n",
    "    prompt_dataloader: DataLoader\n",
    "):\n",
    "    # Info from arguments\n",
    "    llm_id = args.llm_id\n",
    "    num_doc = args.num_retrieved_documents\n",
    "    save_every = args.save_every\n",
    "    retriever_str = \"contriever\" \n",
    "    chat_template_str = \"_template\" if args.use_model_chat_template else \"\"\n",
    "    prompt_type = \"retrieved_proof\" if args.use_task_with_proof else \"retrieved\"\n",
    "\n",
    "    # Create the saving directory\n",
    "    llm_folder = llm_id.split(\"/\")[1] if '/' in llm_id else llm_id\n",
    "    saving_dir = f\"{args.output_dir}/{args.dataset}/{llm_folder}/{args.split}/{prompt_type}/{retriever_str}/{num_doc}_doc\"\n",
    "    os.makedirs(saving_dir, exist_ok=True)\n",
    "\n",
    "    all_info = []  \n",
    "    for idx, prompt_batch in enumerate(tqdm(prompt_dataloader)):\n",
    "        prompts = prompt_batch['prompt']\n",
    "        example_id = prompt_batch['example_id']\n",
    "        prompts = prompt_batch['prompt']\n",
    "        query = prompt_batch['query']\n",
    "        document_indices=prompt_batch['document_indices']\n",
    "        \n",
    "        for doc_idx in document_indices:\n",
    "            \n",
    "            candidate_docs += doc_idx\n",
    "\n",
    "            formatted_docs, _ = prompt_ds._get_documents_from_indices(candidate_docs)\n",
    "\n",
    "            if '\\nAnswer:' not in candidate_prompt:\n",
    "                candidate_prompt += '\\nAnswer:'\n",
    "\n",
    "            \n",
    "\n",
    "        generated_output = llm.generate(\n",
    "            prompts, \n",
    "            max_new_tokens=args.max_new_tokens\n",
    "        )\n",
    "\n",
    "        generated_answers = extract_generate_answers(args, generated_output)\n",
    "        prompt_batch['generated_answer'] = generated_answers\n",
    "        \n",
    "        all_info.append(prompt_batch)\n",
    "        '''\n",
    "        if (idx + 1) % save_every == 0 or (idx + 1) == len(prompt_dataloader):\n",
    "            print(f\"Saving at {idx + 1}...\")\n",
    "            file_name = f\"{saving_dir}/numdoc{num_doc}_retr{args.num_retrieved_documents}{chat_template_str}_info_{idx+1}.pkl\"\n",
    "            write_pickle(all_info, file_name)\n",
    "            all_info = []\n",
    "        '''\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = parse_arguments()\n",
    "\n",
    "    args.split = \"test\" if args.use_test else \"train\"\n",
    "\n",
    "    print(\"Loading LLM...\")\n",
    "    llm_id = args.llm_id\n",
    "    bgm = BGM(\n",
    "        llm_id, device,  \n",
    "        model_max_length=args.model_max_length\n",
    "    )\n",
    "    tokenizer = bgm.tokenizer\n",
    "    print(\"LLM loaded\")\n",
    "\n",
    "\n",
    "    print(\"Loading corpus and search results...\")\n",
    "    corpus, full_to_subset_idx_map = load_corpus(args)\n",
    "    retriever_search_results = load_search_results(args)\n",
    "    print(\"Corpus and search results loaded\")\n",
    "\n",
    "\n",
    "    print(\"Loading prompt dataset...\")\n",
    "    prompt_ds, prompt_dataloader = initialize_dataset_and_loader(\n",
    "        args, corpus, full_to_subset_idx_map, \n",
    "        retriever_search_results, tokenizer\n",
    "    )\n",
    "    print(\"Prompt dataset loaded\")\n",
    "\n",
    "    print_info(args)\n",
    "\n",
    "    #output_json_path = r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\dataloader_contents.json'\n",
    "    #save_dataloader_to_json(prompt_dataloader, output_json_path, num_examples=15)\n",
    "        \n",
    "    BGMTraining(args, prompt_ds, bgm, prompt_dataloader)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    seed_everything(SEED)\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File aggiornato salvato in: C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\gen_ids_document_training_set_bgm\\nq_training\\gemma-2-2b-it\\train\\retrieved\\contriever\\5_doc\\numdoc5_retr5_template_info_all_extended_updated.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def match_example_ids(file1_path, file2_path, output_path):\n",
    "    \"\"\"\n",
    "    Modifica il file1 aggiungendo l'example_id da file2 quando query e question corrispondono.\n",
    "\n",
    "    Args:\n",
    "        file1_path (str): Percorso al file JSON di input 1.\n",
    "        file2_path (str): Percorso al file JSON di input 2.\n",
    "        output_path (str): Percorso al file JSON di output aggiornato.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Caricamento dei file JSON\n",
    "        with open(file1_path, 'r') as f1:\n",
    "            file1 = json.load(f1)\n",
    "\n",
    "        with open(file2_path, 'r') as f2:\n",
    "            file2 = json.load(f2)\n",
    "\n",
    "        # Creazione di un dizionario per mappare le domande agli example_id\n",
    "        question_to_example_id = {item['question']: item['example_id'] for item in file2}\n",
    "\n",
    "        # Modifica del primo file\n",
    "        for entry in file1:\n",
    "            query = entry.get('query')\n",
    "            if query in question_to_example_id:\n",
    "                entry['example_id'] = question_to_example_id[query]\n",
    "\n",
    "        # Salvataggio del file aggiornato\n",
    "        with open(output_path, 'w') as f1_updated:\n",
    "            json.dump(file1, f1_updated, indent=4)\n",
    "\n",
    "        print(f\"File aggiornato salvato in: {output_path}\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Errore: {e}\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Errore nel parsing del file JSON: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Errore imprevisto: {e}\")\n",
    "\n",
    "def update_queries_with_document_indices(file1_path, file2_path, output_path):\n",
    "    # Carica i dati dai file JSON\n",
    "    with open(file1_path, 'r', encoding='utf-8') as f1, open(file2_path, 'r', encoding='utf-8') as f2:\n",
    "        file1_data = json.load(f1)\n",
    "        file2_data = json.load(f2)\n",
    "\n",
    "    # Crea un dizionario per mappare le query ai document_indices di File 2\n",
    "    query_to_indices = {\n",
    "        entry['query']: entry.get('document_indices', [])\n",
    "        for entry in file2_data\n",
    "    }\n",
    "\n",
    "    # Aggiorna File 1 aggiungendo i document_indices associati alle query\n",
    "    for entry in file1_data:\n",
    "        query = entry['query']\n",
    "        if query in query_to_indices:\n",
    "            entry['document_indices'] = query_to_indices[query]\n",
    "\n",
    "    # Salva il risultato in un nuovo file JSON\n",
    "    with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "        json.dump(file1_data, output_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "\n",
    "path_output=r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\gen_ids_document_training_set_bgm\\nq_training\\gemma-2-2b-it\\train\\retrieved\\contriever\\5_doc\\numdoc5_retr5_template_info_all_extended_updated.json'\n",
    "file_da_modificare = r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\gen_ids_document_training_set_bgm\\nq_training\\gemma-2-2b-it\\train\\retrieved\\contriever\\5_doc\\numdoc5_retr5_template_info_all_extended.json'\n",
    "file_di_confronto = r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\10k_train_dataset.json'\n",
    "\n",
    "match_example_ids(file_da_modificare, file_di_confronto, path_output)\n",
    "\n",
    "update_queries_with_document_indices(r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\gen_ids_document_training_set_bgm\\nq_training\\gemma-2-2b-it\\train\\retrieved\\contriever\\5_doc\\numdoc5_retr5_template_info_all_extended_updated.json', r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\gen_ids_document_training_set_bgm\\nq_training\\gemma-2-2b-it\\train\\retrieved\\contriever\\5_doc\\numdoc5_retr5_template_info_all_extended_updated.json', r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\gen_ids_document_training_set_bgm\\nq_training\\gemma-2-2b-it\\train\\retrieved\\contriever\\5_doc\\numdoc5_retr5_template_info_all_extended_updated_last.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "# Percentuali per ogni caso\n",
    "percentages = {\n",
    "    \"case_1_single_doc\": 0.1,\n",
    "    \"case_2_multiple_docs\": 0.2,\n",
    "    \"case_3_no_docs\": 0.1,\n",
    "    \"case_4_less_docs\": 0.4,\n",
    "    \"case_5_reranking\": 0.2,\n",
    "}\n",
    "\n",
    "# Task instruction da aggiungere a ogni query\n",
    "task_instruction = \"Output only the document IDs relevant to the query. Use this format: [ID1, ID2, ...].\"\n",
    "\n",
    "def process_data(input_file, output_file):\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        examples = json.load(f)\n",
    "\n",
    "    dataset = []\n",
    "    \n",
    "    # Shuffle examples to ensure random sampling\n",
    "    random.shuffle(examples)\n",
    "\n",
    "    # Total examples to be processed for each case\n",
    "    total_examples = len(examples)\n",
    "    case_limits = {case: int(total_examples * perc) for case, perc in percentages.items()}\n",
    "    case_counters = {case: 0 for case in percentages}\n",
    "\n",
    "    for example in examples:\n",
    "        if all(count >= case_limits[case] for case, count in case_counters.items()):\n",
    "            break  # Stop if all case limits are met\n",
    "        \n",
    "        query = f\"Task Instruction: {task_instruction}\\nQuestion:{example['query']}\"  # Aggiunge la task instruction\n",
    "        retrieved_docs = example[\"document_indices\"]\n",
    "        selected_docs = example[\"selected_documents\"]\n",
    "        are_answer = example[\"are_answer\"]\n",
    "\n",
    "        # Case 1: Single document correct answer\n",
    "        if are_answer and len(selected_docs) == 1 and case_counters[\"case_1_single_doc\"] < case_limits[\"case_1_single_doc\"]:\n",
    "            dataset.append({\n",
    "                \"input\": {\n",
    "                    \"query\": query,\n",
    "                    \"retrieved_docs\": retrieved_docs,\n",
    "                },\n",
    "                \"output\": selected_docs,\n",
    "            })\n",
    "            case_counters[\"case_1_single_doc\"] += 1\n",
    "\n",
    "        # Case 2: Multiple documents correct answer\n",
    "        elif are_answer and len(selected_docs) > 1 and case_counters[\"case_2_multiple_docs\"] < case_limits[\"case_2_multiple_docs\"]:\n",
    "            dataset.append({\n",
    "                \"input\": {\n",
    "                    \"query\": query,\n",
    "                    \"retrieved_docs\": retrieved_docs,\n",
    "                },\n",
    "                \"output\": selected_docs,\n",
    "            })\n",
    "            case_counters[\"case_2_multiple_docs\"] += 1\n",
    "\n",
    "        # Case 3: No documents correct answer\n",
    "        elif are_answer and len(selected_docs) == 0 and case_counters[\"case_3_no_docs\"] < case_limits[\"case_3_no_docs\"]:\n",
    "            dataset.append({\n",
    "                \"input\": {\n",
    "                    \"query\": query,\n",
    "                    \"retrieved_docs\": [],\n",
    "                },\n",
    "                \"output\": [],\n",
    "            })\n",
    "            case_counters[\"case_3_no_docs\"] += 1\n",
    "\n",
    "        # Case 4: Input and output unchanged\n",
    "        elif are_answer and len(selected_docs) > 2 and case_counters[\"case_4_less_docs\"] < case_limits[\"case_4_less_docs\"]:\n",
    "            dataset.append({\n",
    "                \"input\": {\n",
    "                    \"query\": query,\n",
    "                    \"retrieved_docs\": selected_docs,\n",
    "                },\n",
    "                \"output\": selected_docs,\n",
    "            })\n",
    "            case_counters[\"case_4_less_docs\"] += 1\n",
    "\n",
    "        # Case 5: Reranking\n",
    "        elif are_answer and len(selected_docs) > 2 and case_counters[\"case_5_reranking\"] < case_limits[\"case_5_reranking\"]:\n",
    "            reranked_docs = random.sample(selected_docs, len(selected_docs))  # Randomize order\n",
    "            dataset.append({\n",
    "                \"input\": {\n",
    "                    \"query\": query,\n",
    "                    \"retrieved_docs\": reranked_docs,\n",
    "                },\n",
    "                \"output\": selected_docs,\n",
    "            })\n",
    "            case_counters[\"case_5_reranking\"] += 1\n",
    "\n",
    "    # Save the dataset to a file\n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(dataset, f, indent=4)\n",
    "\n",
    "# Path to input and output files\n",
    "input_file = r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\gen_ids_document_training_set_bgm\\nq_training\\gemma-2-2b-it\\train\\retrieved\\contriever\\5_doc\\numdoc5_retr5_info_all_extended_training_set.json'\n",
    "output_file = r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\training_dataset.json'\n",
    "\n",
    "process_data(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Totale esempi nel file di input: 3000\n",
      "Esempi con 'are_answer=True': 1233\n",
      "Esempi con 'selected_documents == 0': 366\n",
      "Esempi con 'selected_documents == 1': 736\n",
      "Esempi con 'selected_documents > 1': 131\n",
      "Distribuzione pianificata degli esempi nel dataset creato:\n",
      "case_1_single_doc: 51\n",
      "case_2_multiple_docs: 52\n",
      "case_3_no_docs: 36\n",
      "case_4_multi_doc_unchanged: 45\n",
      "case_5_reranking: 65\n",
      "case_6_single_doc_unchanged: 36\n",
      "Esempi effettivamente inclusi nel dataset creato:\n",
      "case_1_single_doc: 51\n",
      "case_2_multiple_docs: 52\n",
      "case_3_no_docs: 36\n",
      "case_4_multi_doc_unchanged: 45\n",
      "case_5_reranking: 34\n",
      "case_6_single_doc_unchanged: 36\n",
      "Totale degli Esempi inclusi nel training dataset creato: 254\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "# Percentuali per ogni caso\n",
    "percentages = {\n",
    "    \"case_1_single_doc\": 0.07,\n",
    "    \"case_2_multiple_docs\": 0.4,\n",
    "    \"case_3_no_docs\": 0.1,\n",
    "    \"case_4_multi_doc_unchanged\": 0.35,\n",
    "    \"case_5_reranking\": 0.5,\n",
    "    \"case_6_single_doc_unchanged\": 0.05,\n",
    "}\n",
    "\n",
    "# Task instruction da aggiungere a ogni query\n",
    "task_instruction = \"Output only the document IDs relevant to the query. Use this format: [ID1, ID2, ...].\"\n",
    "\n",
    "def process_data(input_file, output_file):\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        examples = json.load(f)\n",
    "\n",
    "    # Filtra gli esempi con are_answer = true\n",
    "    valid_examples = [ex for ex in examples if ex[\"are_answer\"] is True]\n",
    "\n",
    "    print(f\"Totale esempi nel file di input: {len(examples)}\")\n",
    "    print(f\"Esempi con 'are_answer=True': {len(valid_examples)}\")\n",
    "\n",
    "    # Raggruppa per numero di selected_documents\n",
    "    grouped_examples = {\n",
    "        \"len_0\": [ex for ex in valid_examples if len(ex[\"selected_documents\"]) == 0],\n",
    "        \"len_1\": [ex for ex in valid_examples if len(ex[\"selected_documents\"]) == 1],\n",
    "        \"len_gt_1\": [ex for ex in valid_examples if len(ex[\"selected_documents\"]) > 1],\n",
    "    }\n",
    "\n",
    "    print(f\"Esempi con 'selected_documents == 0': {len(grouped_examples['len_0'])}\")\n",
    "    print(f\"Esempi con 'selected_documents == 1': {len(grouped_examples['len_1'])}\")\n",
    "    print(f\"Esempi con 'selected_documents > 1': {len(grouped_examples['len_gt_1'])}\")\n",
    "\n",
    "    # Calcola le suddivisioni per ogni gruppo\n",
    "    group_case_limits = {\n",
    "        \"case_1_single_doc\": int(len(grouped_examples[\"len_1\"]) * percentages[\"case_1_single_doc\"]),\n",
    "        \"case_2_multiple_docs\": int(len(grouped_examples[\"len_gt_1\"]) * percentages[\"case_2_multiple_docs\"]),\n",
    "        \"case_3_no_docs\": int(len(grouped_examples[\"len_0\"]) * percentages[\"case_3_no_docs\"]),\n",
    "        \"case_4_multi_doc_unchanged\": int(len(grouped_examples[\"len_gt_1\"]) * percentages[\"case_4_multi_doc_unchanged\"]),\n",
    "        \"case_5_reranking\": int(len(grouped_examples[\"len_gt_1\"]) * percentages[\"case_5_reranking\"]),\n",
    "        \"case_6_single_doc_unchanged\": int(len(grouped_examples[\"len_1\"]) * percentages[\"case_6_single_doc_unchanged\"]),\n",
    "    }\n",
    "\n",
    "    print(\"Distribuzione pianificata degli esempi nel dataset creato:\")\n",
    "    for case, limit in group_case_limits.items():\n",
    "        print(f\"{case}: {limit}\")\n",
    "\n",
    "    dataset = []\n",
    "    case_counters = {case: 0 for case in group_case_limits}\n",
    "\n",
    "    # Processa gli esempi\n",
    "    for example in valid_examples:\n",
    "        query = f\"Task Instruction: {task_instruction}\\nQuestion:{example['query']}\"\n",
    "        retrieved_docs = example[\"document_indices\"]\n",
    "        selected_docs = example[\"selected_documents\"]\n",
    "\n",
    "        # Case 1: Single document correct answer\n",
    "        if len(selected_docs) == 1 and case_counters[\"case_1_single_doc\"] < group_case_limits[\"case_1_single_doc\"]:\n",
    "            dataset.append({\n",
    "                \"input\": {\n",
    "                    \"query\": query,\n",
    "                    \"retrieved_docs\": retrieved_docs,\n",
    "                },\n",
    "                \"output\": selected_docs,\n",
    "            })\n",
    "            case_counters[\"case_1_single_doc\"] += 1\n",
    "\n",
    "        # Case 2: Multiple documents correct answer\n",
    "        elif len(selected_docs) > 1 and case_counters[\"case_2_multiple_docs\"] < group_case_limits[\"case_2_multiple_docs\"]:\n",
    "            dataset.append({\n",
    "                \"input\": {\n",
    "                    \"query\": query,\n",
    "                    \"retrieved_docs\": retrieved_docs,\n",
    "                },\n",
    "                \"output\": selected_docs,\n",
    "            })\n",
    "            case_counters[\"case_2_multiple_docs\"] += 1\n",
    "\n",
    "        # Case 3: No documents correct answer\n",
    "        elif len(selected_docs) == 0 and case_counters[\"case_3_no_docs\"] < group_case_limits[\"case_3_no_docs\"]:\n",
    "            dataset.append({\n",
    "                \"input\": {\n",
    "                    \"query\": query,\n",
    "                    \"retrieved_docs\": [],\n",
    "                },\n",
    "                \"output\": [],\n",
    "            })\n",
    "            case_counters[\"case_3_no_docs\"] += 1\n",
    "\n",
    "        # Case 4: Input and output unchanged for multiple docs\n",
    "        elif len(selected_docs) > 1 and case_counters[\"case_4_multi_doc_unchanged\"] < group_case_limits[\"case_4_multi_doc_unchanged\"]:\n",
    "            dataset.append({\n",
    "                \"input\": {\n",
    "                    \"query\": query,\n",
    "                    \"retrieved_docs\": selected_docs,\n",
    "                },\n",
    "                \"output\": selected_docs,\n",
    "            })\n",
    "            case_counters[\"case_4_multi_doc_unchanged\"] += 1\n",
    "\n",
    "        # Case 6: Input and output unchanged for single doc\n",
    "        elif len(selected_docs) == 1 and case_counters[\"case_6_single_doc_unchanged\"] < group_case_limits[\"case_6_single_doc_unchanged\"]:\n",
    "            dataset.append({\n",
    "                \"input\": {\n",
    "                    \"query\": query,\n",
    "                    \"retrieved_docs\": selected_docs,\n",
    "                },\n",
    "                \"output\": selected_docs,\n",
    "            })\n",
    "            case_counters[\"case_6_single_doc_unchanged\"] += 1\n",
    "\n",
    "        # Case 5: Reranking\n",
    "        elif len(selected_docs) > 1 and case_counters[\"case_5_reranking\"] < group_case_limits[\"case_5_reranking\"]:\n",
    "            reranked_docs = selected_docs[:]\n",
    "            while reranked_docs == selected_docs:  # Garantisce che l'ordine sia diverso\n",
    "                reranked_docs = random.sample(selected_docs, len(selected_docs))\n",
    "            dataset.append({\n",
    "                \"input\": {\n",
    "                    \"query\": query,\n",
    "                    \"retrieved_docs\": reranked_docs,\n",
    "                },\n",
    "                \"output\": selected_docs,\n",
    "            })\n",
    "            case_counters[\"case_5_reranking\"] += 1\n",
    "\n",
    "    print(\"Esempi effettivamente inclusi nel dataset creato:\")\n",
    "    tot=0\n",
    "    for case, count in case_counters.items():\n",
    "        tot += count\n",
    "        print(f\"{case}: {count}\")\n",
    "\n",
    "    print(f\"Totale degli Esempi inclusi nel training dataset creato: {tot}\")\n",
    "    \n",
    "\n",
    "    # Save the dataset to a file\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(dataset, f, indent=4)\n",
    "\n",
    "# Path to input and output files\n",
    "input_file = r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\gen_best_ids_document_training_set_bgm\\nq_training\\gemma-2-2b-it\\train\\retrieved\\contriever\\5_doc\\numdoc5_retr5_info_all_extended_training_set.json'\n",
    "output_file = r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\training_dataset.json'\n",
    "\n",
    "process_data(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\Documents\\Bridge_the_GAP\\myenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BGM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BGM loaded\n",
      "Loading LLM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM loaded\n",
      "Loading corpus and search results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (778 > 15). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus and search results loaded\n",
      "Loading prompt dataset...\n",
      "Prompt dataset loaded\n",
      "INFO:\n",
      "DATA: C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\test_dataset.json\n",
      "TASK INSTRUCTION: Output only the document IDs relevant to the query. Use this format: [Id_1, Id_2, ...].\n",
      "USE TEST: True\n",
      "BGM MODEL: meta-llama/Llama-3.2-1B\n",
      "LLm MODEL: google/gemma-2-2b-it\n",
      "MODEL MAX LENGTH: 4096\n",
      "MAX NEW TOKENS: 50\n",
      "USE MODEL CHAT TEMPLATE: False\n",
      "TASK WITH PROOF: False\n",
      "GOLD POSITION: None\n",
      "NUM DOCUMENTS IN CONTEXT: 5\n",
      "BATCH SIZE: None\n",
      "SAVE EVERY: 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2889 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Example 1:\n",
      "\n",
      "ID disponibili nel prompt: ['628506', '3546609', '439756', '1860765', '2043329']\n",
      "ID validi generati dal modello: ['628506', '2043329', '1860765']\n",
      "You are given a question and you must respond based on the provided documents. You must always provide an answer.\n",
      "Question:who got the first nobel prize in physics\n",
      "Documents:\n",
      "Document [628506](Title: Nobel Prize in Physics) receive a diploma, a medal and a document confirming the prize amount. Nobel Prize in Physics The Nobel Prize in Physics () is a yearly award given by the Royal Swedish Academy of Sciences for those who have made the most outstanding contributions for mankind in the field of physics. It is one of the five Nobel Prizes established by the will of Alfred Nobel in 1895 and awarded since 1901; the others being the Nobel Prize in Chemistry, Nobel Prize in Literature, Nobel Peace Prize, and Nobel Prize in Physiology or Medicine. The first Nobel Prize in Physics was\n",
      "Document [2043329](Title: Universology) become the chief proponent of universology today. \"Everything in this universe is part of an uninterrupted sequence of events\" Mohri has said. In 1872 Andrews published \"The Basic Outline of Universology\" which was subtitled \"An introduction to the newly discovered science of the universe, its elementary principles, and the first stages of their development in the special sciences.\" Ilya Romanovich Prigogine (born on January 25, 1917) was a Belgian and American physicist and chemist who was born in Russia and became a Nobel Prize laureate in chemistry. In the book \"Order Out of Chaos: Man's New Dialogue With Nature\", which\n",
      "Document [1860765](Title: Tsung-Dao Lee) 1956, with her so-called Wu experiment. Lee was the youngest Nobel laureate after World War II until Malala Yousafzai was awarded the Nobel Peace Prize in 2014. He is the fourth youngest Nobel laureate in history after William L. Bragg (who won the prize at 25 with his father William H. Bragg in 1915), Werner Heisenberg (who won in 1932 also at 30) and Malala Yousafzai (awarded at just 17). Lee and Yang were the first Chinese laureates. Since he became a naturalized American citizen in 1962, Lee is also the youngest American ever to have won a Nobel Prize.\n",
      "Answer:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/2889 [01:57<94:29:59, 117.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La risposta generata dal modello e':\n",
      "['This document does not say who received the first Nobel Prize in Physics.']\n",
      "Processing Example 2:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/2889 [02:54<139:47:25, 174.25s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 447\u001b[0m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    446\u001b[0m     seed_everything(SEED)\n\u001b[1;32m--> 447\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 441\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    438\u001b[0m print_info(args)\n\u001b[0;32m    440\u001b[0m training_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mfranc\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDocuments\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mBridge_the_GAP\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mSFT_training_bgm\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmeta-llama-Llama-3.2-1B\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mcheckpoint-800\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m#best checkpoint 700-800\u001b[39;00m\n\u001b[1;32m--> 441\u001b[0m \u001b[43mgenerate_and_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 364\u001b[0m, in \u001b[0;36mgenerate_and_save\u001b[1;34m(args, model_weights_path, llm, tokenizer, dataset, num_examples, max_length)\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[38;5;66;03m# Generate the model's response\u001b[39;00m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 364\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;66;03m# Decode the generated response\u001b[39;00m\n\u001b[0;32m    367\u001b[0m generated_text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\franc\\Documents\\Bridge_the_GAP\\myenv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\franc\\Documents\\Bridge_the_GAP\\myenv\\Lib\\site-packages\\transformers\\generation\\utils.py:2246\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   2238\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   2239\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   2240\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   2241\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   2242\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2243\u001b[0m     )\n\u001b[0;32m   2245\u001b[0m     \u001b[38;5;66;03m# 13. run beam sample\u001b[39;00m\n\u001b[1;32m-> 2246\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_beam_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2247\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeam_scorer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2252\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2254\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2256\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGROUP_BEAM_SEARCH:\n\u001b[0;32m   2257\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[0;32m   2258\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[0;32m   2259\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   2260\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2266\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[0;32m   2267\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\franc\\Documents\\Bridge_the_GAP\\myenv\\Lib\\site-packages\\transformers\\generation\\utils.py:3455\u001b[0m, in \u001b[0;36mGenerationMixin._beam_search\u001b[1;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[0;32m   3452\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m stack_model_outputs(outputs_per_sub_batch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget_text_config())\n\u001b[0;32m   3454\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Unchanged original behavior\u001b[39;00m\n\u001b[1;32m-> 3455\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   3457\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[0;32m   3458\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[0;32m   3459\u001b[0m     outputs,\n\u001b[0;32m   3460\u001b[0m     model_kwargs,\n\u001b[0;32m   3461\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   3462\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\franc\\Documents\\Bridge_the_GAP\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\franc\\Documents\\Bridge_the_GAP\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\franc\\Documents\\Bridge_the_GAP\\myenv\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:1190\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **loss_kwargs)\u001b[0m\n\u001b[0;32m   1187\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m   1189\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m-> 1190\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1191\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1195\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1196\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1197\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1198\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1201\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1203\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\franc\\Documents\\Bridge_the_GAP\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\franc\\Documents\\Bridge_the_GAP\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\franc\\Documents\\Bridge_the_GAP\\myenv\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:945\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m    933\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    934\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    935\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    942\u001b[0m         position_embeddings,\n\u001b[0;32m    943\u001b[0m     )\n\u001b[0;32m    944\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 945\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    947\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    948\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    949\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    950\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    951\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    952\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    953\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    954\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    956\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    958\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\franc\\Documents\\Bridge_the_GAP\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\franc\\Documents\\Bridge_the_GAP\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\franc\\Documents\\Bridge_the_GAP\\myenv\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:692\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[0;32m    690\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m    691\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[1;32m--> 692\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    693\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[0;32m    695\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[1;32mc:\\Users\\franc\\Documents\\Bridge_the_GAP\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\franc\\Documents\\Bridge_the_GAP\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\franc\\Documents\\Bridge_the_GAP\\myenv\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:258\u001b[0m, in \u001b[0;36mLlamaMLP.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    256\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(down_proj)\n\u001b[0;32m    257\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 258\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_fn(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgate_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup_proj(x))\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "File \u001b[1;32mc:\\Users\\franc\\Documents\\Bridge_the_GAP\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\franc\\Documents\\Bridge_the_GAP\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\franc\\Documents\\Bridge_the_GAP\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re \n",
    "import argparse\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from typing import Tuple, Dict, Optional, Union, List\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import PreTrainedTokenizer, AutoModelForCausalLM\n",
    "from trl import setup_chat_format\n",
    "\n",
    "from utils import *\n",
    "from bgm import BGM\n",
    "from llm import LLM\n",
    "from default_prompts import *\n",
    "from prompt_dataset import PromptDataset\n",
    "from datasets import Dataset\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "device = torch.device(f\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "warnings.filterwarnings('ignore')\n",
    "SEED=10\n",
    "\n",
    "info = {\n",
    "    \"nq\": {\n",
    "        \"test\": {\n",
    "            \"data_path\": r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\test_dataset.json',\n",
    "            \"contriever_search_results_path\": r\"C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\processed\\contriever_test_search_results_at150.pkl\",\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "class DotDict:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "def parse_arguments(custom_args=None):\n",
    "    \"\"\"\n",
    "    Mimics argparse to parse arguments for LLM generation. Accepts custom arguments as a dictionary for notebooks.\n",
    "    \"\"\"\n",
    "    # Define default values\n",
    "    default_args = {\n",
    "        'output_dir': r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\gen_res_example_llm_with_bgm',\n",
    "        'bgm_id': 'meta-llama/Llama-3.2-1B',\n",
    "        'llm_id': 'google/gemma-2-2b-it',\n",
    "        'dataset': 'nq',\n",
    "        'task_instruction' : \"Output only the document IDs relevant to the query. Use this format: [Id_1, Id_2, ...].\",\n",
    "        'model_max_length': 4096,\n",
    "        'quantization_bits': 4,\n",
    "        'gold_position': None,\n",
    "        'use_model_chat_template': False, \n",
    "        'num_retrieved_documents': 5,\n",
    "        'use_test': True,\n",
    "        'padding_strategy': 'longest',\n",
    "        'max_new_tokens': 50,\n",
    "        'use_task_with_proof': False,\n",
    "        'batch_size': None,\n",
    "        'save_every': 250,\n",
    "    }\n",
    "\n",
    "    # If custom_args is provided, update defaults\n",
    "    if custom_args:\n",
    "        default_args.update(custom_args)\n",
    "\n",
    "    # Perform validation\n",
    "    if default_args['num_retrieved_documents'] is None:\n",
    "        raise ValueError(\"'num_retrieved_documents' must be specified.\")\n",
    "    if default_args['num_retrieved_documents'] <= 0:\n",
    "        raise ValueError(\"'num_retrieved_documents' must be a positive integer.\")\n",
    "    if default_args['gold_position'] is not None:\n",
    "        if (default_args['gold_position'] < 0 or \n",
    "            default_args['gold_position'] >= default_args['num_retrieved_documents']):\n",
    "            raise ValueError(\"'gold_position' must be within the range of 'num_retrieved_documents'.\")\n",
    "\n",
    "    return DotDict(**default_args)\n",
    "\n",
    "\n",
    "def load_corpus(\n",
    "    args: argparse.Namespace\n",
    ") -> Tuple[List[Dict], Optional[Dict[int, int]]]:\n",
    "    \n",
    "    # Corpus with documents from Contriever\n",
    "    corpus, full_to_subset_idx_map = read_test_corpus_with_random_and_contriever()\n",
    "\n",
    "    return corpus, full_to_subset_idx_map\n",
    "\n",
    "def load_search_results(args: argparse.Namespace) -> List[Tuple[List[int], List[float]]]:\n",
    "\n",
    "    search_results_path = info[args.dataset][args.split]['contriever_search_results_path']\n",
    "    retriever_search_results = read_pickle(search_results_path)\n",
    "\n",
    "    return retriever_search_results\n",
    "\n",
    "\n",
    "def get_prompt_template(args: argparse.Namespace):\n",
    "    prompt_configuration = args.dataset\n",
    "    if args.use_model_chat_template:\n",
    "        chat_task_template_str = chat_task_templates[args.llm_id]['template']\n",
    "        \n",
    "        task_instruction = task_instructions[prompt_configuration]\n",
    "\n",
    "        prompt_template = apply_chat_task_template(chat_task_template_str, task_instruction)\n",
    "    else:\n",
    "        task_template = task_templates[prompt_configuration]\n",
    "\n",
    "        prompt_template = task_template.create_prompt_template()\n",
    "\n",
    "    return prompt_template\n",
    "\n",
    "def process_dataset(dataset, task_instruction):\n",
    "    \"\"\"\n",
    "    Processes the dataset by applying the chat template transformation.\n",
    "\n",
    "    Args:\n",
    "        dataset (List[Dict]): The dataset to be processed.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict]: The processed dataset with formatted text.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the chat messages format\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": task_instruction},\n",
    "        {\"role\": \"user\", \"content\": dataset},\n",
    "    ]\n",
    "\n",
    "    return messages\n",
    "\n",
    "def initialize_dataset_and_loader(\n",
    "    args: argparse.Namespace, \n",
    "    corpus: List[Dict], \n",
    "    full_to_subset_idx_map: Optional[Dict[int, int]], \n",
    "    retriever_search_results: List[Tuple[List[int], List[float]]], \n",
    "    tokenizer: PreTrainedTokenizer,\n",
    ") -> DataLoader:\n",
    "    \n",
    "    prompt_template = get_prompt_template(args)\n",
    "    \n",
    "    prompt_ds = PromptDataset(\n",
    "        corpus=corpus, data_path=info[args.dataset][args.split]['data_path'], \n",
    "        tokenizer=tokenizer, \n",
    "        max_tokenized_length=args.model_max_length - 2, \n",
    "        search_results=retriever_search_results,\n",
    "        prompt_template=prompt_template,\n",
    "        full_to_subset_idx_map=full_to_subset_idx_map,\n",
    "        do_normalize_query=True, \n",
    "        num_documents_in_context=args.num_retrieved_documents,\n",
    "        gold_position=args.gold_position, # None in these experiments\n",
    "    )\n",
    "        \n",
    "    prompt_dataloader = DataLoader(\n",
    "        prompt_ds,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=8,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    return prompt_dataloader\n",
    "\n",
    "    \n",
    "def map_document_indices(prompt: str, document_indices: list) -> tuple:\n",
    "    \"\"\"\n",
    "    Modifica il prompt mappando i document ID originali con ID sequenziali.\n",
    "    \n",
    "    Args:\n",
    "        prompt (str): Il prompt originale contenente i document ID.\n",
    "        document_indices (list): Lista degli indici dei documenti nel prompt.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Il prompt modificato e la mappatura (original_id -> new_id).\n",
    "    \"\"\"\n",
    "    id_mapping = {}\n",
    "    \n",
    "    # Mappa i document ID ai nuovi ID sequenziali (Id_1, Id_2, ...)\n",
    "    for idx, doc_id in enumerate(document_indices, start=1):\n",
    "        new_id = f\"Id_{idx}\"\n",
    "        id_mapping[str(doc_id)] = new_id\n",
    "    \n",
    "    # Sostituisce i document ID nel prompt\n",
    "    modified_prompt = prompt\n",
    "    for original_id, new_id in id_mapping.items():\n",
    "        modified_prompt = re.sub(rf'\\[{original_id}\\]', f'[{new_id}]', modified_prompt)\n",
    "    \n",
    "    return modified_prompt, id_mapping\n",
    "\n",
    "def extract_and_convert_answer_indices(generated_output: str, id_mapping: dict) -> str:\n",
    "    \"\"\"\n",
    "    Estrae e converte gli ID generati dal modello dopo '<|im_start|>assistant'.\n",
    "    Converte gli ID nel formato originale. Se non trova un ID nella mappatura, restituisce 'Unknown(ID)'.\n",
    "\n",
    "    Args:\n",
    "        generated_output (str): Testo con la risposta generata dal modello.\n",
    "        id_mapping (dict): Mappatura {original_id: Id_n}.\n",
    "\n",
    "    Returns:\n",
    "        str: Stringa con gli ID originali separati da virgola.\n",
    "    \"\"\"\n",
    "    # Invertire la mappatura per ottenere {Id_n: original_id}\n",
    "    inverse_mapping = {v: k for k, v in id_mapping.items()}\n",
    "\n",
    "    # Estrae la risposta dopo '<|im_start|>assistant'\n",
    "    match = re.search(r'<\\|im_start\\|>assistant\\s*(.*)', generated_output, re.DOTALL)\n",
    "    if not match:\n",
    "        return \"Nessuna risposta trovata\"\n",
    "    #if match:\n",
    "        #print(\"Contenuto dopo assistant:\", repr(match.group(1)))\n",
    "\n",
    "    # Ottieni la stringa con gli ID dopo assistant\n",
    "    answer_string = match.group(1).strip().split(\"<|im_end|>\")[0].strip()\n",
    "\n",
    "    # Dividi e converte gli ID\n",
    "    generated_ids = [id_.strip() for id_ in answer_string.split(\",\") if id_.strip()]\n",
    "    original_ids = [inverse_mapping.get(id_, f\"Unknown({id_})\") for id_ in generated_ids]\n",
    "\n",
    "    # Restituisci gli ID originali come stringa separata da virgole\n",
    "    return \",\".join(original_ids)\n",
    "\n",
    "def reconstruct_prompt_from_ids(\n",
    "    original_ids: str,\n",
    "    prompt: str,\n",
    "    task_instruction: str = \"You are given a question and you must respond based on the provided documents. You must always provide an answer.\"\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Ricostruisce il prompt originale utilizzando solo i documenti selezionati dal modello,\n",
    "    ignorando gli ID non presenti nella lista generale.\n",
    "\n",
    "    Args:\n",
    "        original_ids (str): Stringa contenente gli ID separati da virgola (es. \"628506,3546609\").\n",
    "        prompt (str): Testo completo del prompt originale.\n",
    "\n",
    "    Returns:\n",
    "        str: Il prompt ricostruito contenente solo i documenti selezionati.\n",
    "    \"\"\"\n",
    "    query_match = re.search(r\"Question:\\s*(.*?)(?=Document|\\Z)\", prompt, re.DOTALL)\n",
    "    query = query_match.group(1).strip() if query_match else \"Query non trovata\"\n",
    "\n",
    "    #print(f\"Query originale trovata: {query}\")\n",
    "    \n",
    "    documents = re.findall(\n",
    "        r\"(Document \\[(\\d+)\\]\\(.*?\\)\\s.*?)(?=Document \\[\\d+\\]|$)\", \n",
    "        prompt, \n",
    "        re.DOTALL\n",
    "    )\n",
    "    documents_dict = {doc_id.strip(): full_text.strip() for full_text, doc_id in documents}\n",
    "\n",
    "    print(f\"ID disponibili nel prompt: {list(documents_dict.keys())}\")\n",
    "\n",
    "    selected_ids = [id_.strip() for id_ in original_ids.split(\",\") if id_.strip()]\n",
    "    \n",
    "    valid_ids = [id_ for id_ in selected_ids if id_ in documents_dict]\n",
    "\n",
    "    print(f\"ID validi generati dal modello: {valid_ids}\")\n",
    "\n",
    "    selected_documents = [\n",
    "        re.sub(r'Answer:\\s*\\n?', '', documents_dict[id_]) \n",
    "        for id_ in valid_ids\n",
    "    ]\n",
    "\n",
    "    if not selected_documents:\n",
    "        return #nesun documento trovato\n",
    "\n",
    "    reconstructed_prompt = \"\\n\".join(selected_documents)\n",
    "\n",
    "    final_prompt = (f\"{task_instruction}\\nQuestion: {query}\\nDocuments:\\n{reconstructed_prompt}\\nAnswer:\")\n",
    "\n",
    "    final_prompt = re.sub(r'\\n+', '\\n', final_prompt).strip()\n",
    "\n",
    "    return final_prompt\n",
    "\n",
    "def extract_generate_answers(\n",
    "    args: argparse.Namespace, \n",
    "    generated_output: List[str]\n",
    ") -> List[str]:\n",
    "    answer_prefix = \"Answer:\"\n",
    "    if args.use_model_chat_template:\n",
    "        answer_prefix = re.escape(chat_task_templates[args.llm_id]['answer_prefix'])\n",
    "\n",
    "    generated_answers = []\n",
    "    for output in generated_output:\n",
    "        matches = list(re.finditer(answer_prefix, output))\n",
    "        match_idx = 0\n",
    "\n",
    "        # When using the proof there is a one-shot example that already \n",
    "        # contains the string \"Answer:\". Thus, we should get the second (match_idx=1) match.\n",
    "        if args.use_task_with_proof:\n",
    "            match_idx = 1\n",
    "            if args.use_model_chat_template and answer_prefix != \"Answer:\":\n",
    "                match_idx = 0\n",
    " \n",
    "        answer_end = matches[match_idx].end()\n",
    "        response = output[answer_end:].strip()\n",
    "        generated_answers.append(response)\n",
    "    \n",
    "    return generated_answers\n",
    "\n",
    "def print_info(args: argparse.Namespace):\n",
    "    print(\"INFO:\")    \n",
    "    print(f\"DATA: {info[args.dataset]['test']['data_path']}\")\n",
    "    print(f\"TASK INSTRUCTION: {args.task_instruction}\")\n",
    "    print(f\"USE TEST: {args.use_test}\")\n",
    "    print(f\"BGM MODEL: {args.bgm_id}\")\n",
    "    print(f\"LLm MODEL: {args.llm_id}\")\n",
    "    print(f\"MODEL MAX LENGTH: {args.model_max_length}\")\n",
    "    print(f'MAX NEW TOKENS: {args.max_new_tokens}')\n",
    "    print(f\"USE MODEL CHAT TEMPLATE: {args.use_model_chat_template}\")\n",
    "    print(f\"TASK WITH PROOF:\", args.use_task_with_proof)\n",
    "    print(f\"GOLD POSITION: {args.gold_position}\")\n",
    "    print(f\"NUM DOCUMENTS IN CONTEXT: {args.num_retrieved_documents}\")\n",
    "    print(f\"BATCH SIZE: {args.batch_size}\")\n",
    "    print(f\"SAVE EVERY: {args.save_every}\")\n",
    "\n",
    "\n",
    "def generate_and_save(\n",
    "    args: argparse.Namespace, \n",
    "    model_weights_path,\n",
    "    llm: LLM, \n",
    "    tokenizer,\n",
    "    dataset,\n",
    "    num_examples=10,\n",
    "    max_length=50\n",
    "):\n",
    "    \n",
    "    llm_id = args.llm_id\n",
    "    num_doc = args.num_retrieved_documents\n",
    "    save_every = args.save_every\n",
    "    retriever_str = \"contriever\"\n",
    "    padding_str = f\"_{args.padding_strategy}{args.model_max_length}\" if args.padding_strategy != \"longest\" else \"\" \n",
    "    chat_template_str = \"_template\" if args.use_model_chat_template else \"\"\n",
    "    prompt_type = \"retrieved_proof\" if args.use_task_with_proof else \"retrieved\"\n",
    "\n",
    "    # Create the saving directory\n",
    "    llm_folder = llm_id.split(\"/\")[1] if '/' in llm_id else llm_id\n",
    "    saving_dir = f\"{args.output_dir}/{args.dataset}/{llm_folder}/{args.split}/{prompt_type}/{retriever_str}/{num_doc}_doc\"\n",
    "    #os.makedirs(saving_dir, exist_ok=True)\n",
    "\n",
    "    # Load the trained model\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_weights_path)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_info = []  \n",
    "    for idx, prompt_batch in enumerate(tqdm(dataset)):\n",
    "        if idx >= num_examples:\n",
    "            break\n",
    "\n",
    "        prompt = prompt_batch['prompt'].replace('You are given a question and you must respond based on the provided documents. You must always provide an answer.', \"\")\n",
    "        document_indices= prompt_batch['document_indices']\n",
    "\n",
    "        # Mappa i document ID nel prompt\n",
    "        modified_prompt, id_mapping = map_document_indices(prompt, document_indices)\n",
    "        print(f\"Processing Example {idx+1}:\\n\")\n",
    "\n",
    "        prompt_formatted = process_dataset(modified_prompt, args.task_instruction)\n",
    "        prompt_formatted = tokenizer.apply_chat_template(\n",
    "            prompt_formatted, tokenize=False, add_generation_prompt=False, add_special_tokens=False\n",
    "        )\n",
    "\n",
    "        # Tokenize the input\n",
    "        inputs = tokenizer(prompt_formatted, return_tensors=\"pt\", truncation=False).to(device)\n",
    "\n",
    "        # Generate the model's response\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(**inputs, max_new_tokens=max_length, num_beams=5)\n",
    "\n",
    "        # Decode the generated response\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "\n",
    "        # Convertire gli ID generati nei document ID originali\n",
    "        original_ids = extract_and_convert_answer_indices(generated_text, id_mapping)\n",
    "\n",
    "        filtered_prompt = reconstruct_prompt_from_ids(original_ids, prompt)\n",
    "        print(filtered_prompt)\n",
    "\n",
    "        generated_output = llm.generate(\n",
    "            filtered_prompt,\n",
    "            padding_strategy=args.padding_strategy, \n",
    "            max_new_tokens=args.max_new_tokens\n",
    "        )\n",
    "\n",
    "        generated_answers = extract_generate_answers(args, generated_output)\n",
    "        print(f\"La risposta generata dal modello e':\\n{generated_answers}\")\n",
    "        prompt_batch['generated_answer'] = generated_answers\n",
    "        all_info.append(prompt_batch)\n",
    "\n",
    "        '''\n",
    "        if (idx + 1) % save_every == 0 or (idx + 1) == len(dataset):\n",
    "            print(f\"Saving at {idx + 1}...\")\n",
    "            file_name = f\"{saving_dir}/numdoc{num_doc}_retr{args.num_retrieved_documents}{padding_str}{chat_template_str}_info_{idx+1}.pkl\"\n",
    "            write_pickle(all_info, file_name)\n",
    "            all_info = []\n",
    "        '''\n",
    "        \n",
    "\n",
    "def main():\n",
    "    args = parse_arguments()\n",
    "\n",
    "    args.split = \"test\" if args.use_test else \"train\"\n",
    "\n",
    "    print(\"Loading BGM...\")\n",
    "    bgm_id = args.bgm_id\n",
    "    \n",
    "    bgm = BGM(\n",
    "        bgm_id, device, \n",
    "        quantization_bits=args.quantization_bits, \n",
    "        model_max_length=15,\n",
    "    )\n",
    "    bgm_model= bgm.model\n",
    "\n",
    "    tokenizer = bgm.tokenizer\n",
    "    model, tokenizer = setup_chat_format(bgm_model, tokenizer)\n",
    "    print(\"BGM loaded\")\n",
    "    \n",
    "    print(\"Loading LLM...\")\n",
    "    llm_id = args.llm_id\n",
    "    llm = LLM(\n",
    "        llm_id, device, \n",
    "        quantization_bits=args.quantization_bits, \n",
    "        model_max_length=args.model_max_length\n",
    "    )\n",
    "    print(\"LLM loaded\")\n",
    "\n",
    "    task_instruction = args.task_instruction\n",
    "\n",
    "    print(\"Loading corpus and search results...\")\n",
    "    corpus, full_to_subset_idx_map = load_corpus(args)\n",
    "    retriever_search_results = load_search_results(args)\n",
    "    print(\"Corpus and search results loaded\")\n",
    "\n",
    "\n",
    "    print(\"Loading prompt dataset...\")\n",
    "    prompt_dataloader = initialize_dataset_and_loader(\n",
    "        args, corpus, full_to_subset_idx_map, \n",
    "        retriever_search_results, tokenizer\n",
    "    )\n",
    "    print(\"Prompt dataset loaded\")\n",
    "\n",
    "    print_info(args)\n",
    "\n",
    "    training_path=r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\SFT_training_bgm\\meta-llama-Llama-3.2-1B\\checkpoint-800'  #best checkpoint 700-800\n",
    "    generate_and_save(args, training_path, llm, tokenizer, prompt_dataloader, num_examples=10, max_length=15)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    seed_everything(SEED)\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LLM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM loaded\n",
      "INFO:\n",
      "DATA: C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\test_dataset.json\n",
      "TASK INSTRUCTION: Output only the IDs of the documents relevant to the query. Use this format: [Id_1, Id_2, ...]. If there is no relevant document, output NO_DOCS.\n",
      "USE TEST: True\n",
      "BGM MODEL: meta-llama/Llama-3.2-1B\n",
      "LLM MODEL: google/gemma-2-2b-it\n",
      "MODEL MAX LENGTH: 4096\n",
      "MAX NEW TOKENS: 50\n",
      "USE MODEL CHAT TEMPLATE: True\n",
      "TASK WITH PROOF: False\n",
      "GOLD POSITION: None\n",
      "NUM DOCUMENTS IN CONTEXT: 5\n",
      "BATCH SIZE: None\n",
      "SAVE EVERY: 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 250/2889 [03:36<31:35,  1.39it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at 250...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 500/2889 [07:07<27:59,  1.42it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at 500...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 750/2889 [10:25<24:56,  1.43it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at 750...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 1000/2889 [13:56<25:17,  1.24it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at 1000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 1250/2889 [17:35<26:11,  1.04it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at 1250...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 1500/2889 [21:05<42:57,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at 1500...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 1750/2889 [24:49<14:08,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at 1750...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 2000/2889 [28:12<09:41,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at 2000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 2250/2889 [31:28<08:39,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at 2250...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 2500/2889 [34:57<06:13,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at 2500...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 2750/2889 [38:16<01:48,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at 2750...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2889/2889 [40:13<00:00,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at 2889...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re \n",
    "import argparse\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from typing import Tuple, Dict, Optional, Union, List\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import PreTrainedTokenizer, AutoModelForCausalLM\n",
    "from trl import setup_chat_format\n",
    "\n",
    "from utils import *\n",
    "from bgm import BGM\n",
    "from llm import LLM\n",
    "from default_prompts import *\n",
    "from prompt_dataset import PromptDataset\n",
    "from datasets import Dataset\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "device = torch.device(f\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "warnings.filterwarnings('ignore')\n",
    "SEED=10\n",
    "\n",
    "info = {\n",
    "    \"nq\": {\n",
    "        \"test\": {\n",
    "            \"data_path\": r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\test_dataset.json',\n",
    "            \"contriever_search_results_path\": r\"C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\processed\\contriever_test_search_results_at150.pkl\",\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "class DotDict:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "def parse_arguments(custom_args=None):\n",
    "    \"\"\"\n",
    "    Mimics argparse to parse arguments for LLM generation. Accepts custom arguments as a dictionary for notebooks.\n",
    "    \"\"\"\n",
    "    # Define default values\n",
    "    default_args = {\n",
    "        'output_dir': r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\gen_res_example_llm_with_bgm',\n",
    "        'bgm_id': 'meta-llama/Llama-3.2-1B',\n",
    "        'llm_id': 'google/gemma-2-2b-it',\n",
    "        'dataset': 'nq',\n",
    "        'task_instruction' : \"Output only the IDs of the documents relevant to the query. Use this format: [Id_1, Id_2, ...]. If there is no relevant document, output NO_DOCS.\",\n",
    "        'model_max_length': 4096,\n",
    "        'quantization_bits': 4,\n",
    "        'gold_position': None,\n",
    "        'use_model_chat_template': True, \n",
    "        'num_retrieved_documents': 5,\n",
    "        'use_test': True,\n",
    "        'padding_strategy': 'longest',\n",
    "        'max_new_tokens': 50,\n",
    "        'use_task_with_proof': False,\n",
    "        'batch_size': None,\n",
    "        'save_every': 250,\n",
    "    }\n",
    "\n",
    "    # If custom_args is provided, update defaults\n",
    "    if custom_args:\n",
    "        default_args.update(custom_args)\n",
    "\n",
    "    # Perform validation\n",
    "    if default_args['num_retrieved_documents'] is None:\n",
    "        raise ValueError(\"'num_retrieved_documents' must be specified.\")\n",
    "    if default_args['num_retrieved_documents'] <= 0:\n",
    "        raise ValueError(\"'num_retrieved_documents' must be a positive integer.\")\n",
    "    if default_args['gold_position'] is not None:\n",
    "        if (default_args['gold_position'] < 0 or \n",
    "            default_args['gold_position'] >= default_args['num_retrieved_documents']):\n",
    "            raise ValueError(\"'gold_position' must be within the range of 'num_retrieved_documents'.\")\n",
    "\n",
    "    return DotDict(**default_args)\n",
    "\n",
    "\n",
    "def get_prompt_template(args: argparse.Namespace):\n",
    "    prompt_configuration = args.dataset\n",
    "    if args.use_model_chat_template:\n",
    "        chat_task_template_str = chat_task_templates[args.llm_id]['template']\n",
    "        \n",
    "        task_instruction = task_instructions[prompt_configuration]\n",
    "\n",
    "        prompt_template = apply_chat_task_template(chat_task_template_str, task_instruction)\n",
    "    else:\n",
    "        task_template = task_templates[prompt_configuration]\n",
    "\n",
    "        prompt_template = task_template.create_prompt_template()\n",
    "\n",
    "    return prompt_template\n",
    "\n",
    "\n",
    "def extract_generate_answers(\n",
    "    args: argparse.Namespace, \n",
    "    generated_output: List[str]\n",
    ") -> List[str]:\n",
    "    answer_prefix = \"Answer:\"\n",
    "    if args.use_model_chat_template:\n",
    "        answer_prefix = '\\n\\nmodel' #re.escape(chat_task_templates[args.llm_id]['answer_prefix'])\n",
    "\n",
    "    generated_answers = []\n",
    "    for output in generated_output:\n",
    "        matches = list(re.finditer(answer_prefix, output))\n",
    "        match_idx = 0\n",
    "\n",
    "        # When using the proof there is a one-shot example that already \n",
    "        # contains the string \"Answer:\". Thus, we should get the second (match_idx=1) match.\n",
    "        if args.use_task_with_proof:\n",
    "            match_idx = 1\n",
    "            if args.use_model_chat_template and answer_prefix != \"Answer:\":\n",
    "                match_idx = 0\n",
    " \n",
    "        answer_end = matches[match_idx].end()\n",
    "        response = output[answer_end:].strip()\n",
    "        generated_answers.append(response)\n",
    "    \n",
    "    return generated_answers\n",
    "\n",
    "def print_info(args: argparse.Namespace):\n",
    "    print(\"INFO:\")    \n",
    "    print(f\"DATA: {info[args.dataset]['test']['data_path']}\")\n",
    "    print(f\"TASK INSTRUCTION: {args.task_instruction}\")\n",
    "    print(f\"USE TEST: {args.use_test}\")\n",
    "    print(f\"BGM MODEL: {args.bgm_id}\")\n",
    "    print(f\"LLM MODEL: {args.llm_id}\")\n",
    "    print(f\"MODEL MAX LENGTH: {args.model_max_length}\")\n",
    "    print(f'MAX NEW TOKENS: {args.max_new_tokens}')\n",
    "    print(f\"USE MODEL CHAT TEMPLATE: {args.use_model_chat_template}\")\n",
    "    print(f\"TASK WITH PROOF:\", args.use_task_with_proof)\n",
    "    print(f\"GOLD POSITION: {args.gold_position}\")\n",
    "    print(f\"NUM DOCUMENTS IN CONTEXT: {args.num_retrieved_documents}\")\n",
    "    print(f\"BATCH SIZE: {args.batch_size}\")\n",
    "    print(f\"SAVE EVERY: {args.save_every}\")\n",
    "\n",
    "\n",
    "def generate_and_save(\n",
    "    args: argparse.Namespace, \n",
    "    steps_training: int,\n",
    "    llm: LLM, \n",
    "    dataset_path,\n",
    "    max_length=50\n",
    "):\n",
    "    \n",
    "    dataset = read_json(dataset_path)\n",
    "    \n",
    "    llm_id = args.llm_id\n",
    "    num_doc = args.num_retrieved_documents\n",
    "    save_every = args.save_every\n",
    "    retriever_str = \"contriever\"\n",
    "    padding_str = f\"_{args.padding_strategy}{args.model_max_length}\" if args.padding_strategy != \"longest\" else \"\" \n",
    "    chat_template_str = \"_template\" if args.use_model_chat_template else \"\"\n",
    "    prompt_type = \"retrieved_proof\" if args.use_task_with_proof else \"retrieved\"\n",
    "\n",
    "    # Create the saving directory\n",
    "    llm_folder = llm_id.split(\"/\")[1] if '/' in llm_id else llm_id\n",
    "    saving_dir = f\"{args.output_dir}/steps_{steps_training}/{args.dataset}/{llm_folder}/{args.split}/{prompt_type}/{retriever_str}/{num_doc}_doc\"\n",
    "    os.makedirs(saving_dir, exist_ok=True)\n",
    "\n",
    "    all_info = []\n",
    "    for idx, prompt_batch in enumerate(tqdm(dataset)):\n",
    "            \n",
    "        prompt = prompt_batch['prompt'].replace('You are given a question and you must respond based on the provided documents. You must always provide an answer.', 'You are given a question and you must respond based on the provided documents. Respond directly without providing any premise or explanation. If none of the documents contain the answer, please respond with NO-RESPONSE. Do not try to respond based on your own knowledge.')\n",
    "\n",
    "        generated_output = llm.generate(\n",
    "            prompt,\n",
    "            padding_strategy=args.padding_strategy, \n",
    "            max_new_tokens=args.max_new_tokens\n",
    "        )\n",
    "        \n",
    "        generated_answers = extract_generate_answers(args, generated_output)\n",
    "\n",
    "        prompt_batch['generated_answer'] = generated_answers\n",
    "        prompt_batch['prompt'] = prompt\n",
    "\n",
    "        all_info.append(prompt_batch)\n",
    "\n",
    "        if (idx + 1) % save_every == 0 or (idx + 1) == len(dataset):\n",
    "            print(f\"Saving at {idx + 1}...\")\n",
    "            file_name = f\"{saving_dir}/numdoc{num_doc}_retr{args.num_retrieved_documents}{padding_str}{chat_template_str}_info_{idx+1}.pkl\"\n",
    "            write_pickle(all_info, file_name)\n",
    "            all_info = []\n",
    "        \n",
    "\n",
    "def main():\n",
    "    args = parse_arguments()\n",
    "\n",
    "    args.split = \"test\" if args.use_test else \"train\"\n",
    "    \n",
    "    print(\"Loading LLM...\")\n",
    "    llm_id = args.llm_id\n",
    "    llm = LLM(\n",
    "        llm_id, device, \n",
    "        quantization_bits=args.quantization_bits, \n",
    "        model_max_length=args.model_max_length\n",
    "    )\n",
    "    print(\"LLM loaded\")\n",
    "\n",
    "    print_info(args)\n",
    "\n",
    "    dataset_path = r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\gen_res_example_llm_with_bgm\\old_task_instruction\\steps_1800\\nq\\gemma-2-2b-it\\test\\retrieved\\contriever\\5_doc\\numdoc5_retr5_template_info_all_extended.json'\n",
    "    generate_and_save(args, 1800, llm, dataset_path, max_length=15)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    seed_everything(SEED)\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Risultati da inserire nella tesi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trovati 92 esempi con risposta corretta solo in RAG + BGM. I risultati sono stati salvati in C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\results_for_the_thesis\\risultati_rag_bgm_only.json.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "## CONSIDERA SOLO I CASI DI RISPOSTA VALIDA , NON CONSIDERANDO I CASI NO-RESPONSE\n",
    "def trova_risposte_corrette_bgm_only(rag_bgm_file, llm_file, output_file):\n",
    "    # Carica i dati dai due file JSON\n",
    "    with open(rag_bgm_file, 'r') as f:\n",
    "        rag_bgm_data = json.load(f)\n",
    "    \n",
    "    with open(llm_file, 'r') as f:\n",
    "        llm_data = json.load(f)\n",
    "    \n",
    "    # Crea un dizionario per trovare facilmente gli ID esempio nel file LLM\n",
    "    llm_examples = {example['example_id']: example for example in llm_data}\n",
    "    \n",
    "    # Lista per memorizzare gli ID degli esempi con risposta corretta solo in RAG + BGM\n",
    "    correct_only_in_rag_bgm = []\n",
    "    \n",
    "    for example in rag_bgm_data:\n",
    "        example_id = example['example_id']\n",
    "        generated_answer_bgm = example['generated_answer']\n",
    "        correct_bgm = example['is_correct']\n",
    "        \n",
    "        # Verifica che la risposta in RAG + BGM sia corretta, non sia \"NO-RESPONSE\" e sia presente nei documenti\n",
    "        if correct_bgm and generated_answer_bgm != \"NO-RESPONSE\" and example['ans_in_documents']:\n",
    "            # Controlla se esiste lo stesso esempio nel file LLM\n",
    "            if example_id in llm_examples:\n",
    "                # Ottieni l'esempio corrispondente nel file LLM\n",
    "                llm_example = llm_examples[example_id]\n",
    "                generated_answer_llm = llm_example['generated_answer']\n",
    "                correct_llm = llm_example['is_correct']\n",
    "                \n",
    "                # Verifica che la risposta nel file LLM non sia corretta\n",
    "                if not correct_llm:\n",
    "                    correct_only_in_rag_bgm.append(example)\n",
    "    \n",
    "    # Salva i risultati in un nuovo file JSON\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(correct_only_in_rag_bgm, f, indent=4)\n",
    "    \n",
    "    return correct_only_in_rag_bgm\n",
    "\n",
    "# Esempio di utilizzo\n",
    "rag_bgm_file = r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\gen_res_example_llm_with_bgm\\steps_1800\\nq\\gemma-2-2b-it\\test\\retrieved\\contriever\\5_doc\\numdoc5_retr5_template_info_all_extended.json'\n",
    "llm_file = r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\gen_res_example_llm\\nq\\gemma-2-2b-it\\test\\retrieved\\contriever\\5_doc\\numdoc5_retr5_template_info_all_extended.json' \n",
    "output_file = r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\results_for_the_thesis\\risultati_rag+bgm_only_with_answer_in_context.json'\n",
    "\n",
    "risultati = trova_risposte_corrette_bgm_only(rag_bgm_file, llm_file, output_file)\n",
    "print(f\"Trovati {len(risultati)} esempi con risposta corretta solo in RAG + BGM. I risultati sono stati salvati in {output_file}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trovati 92 esempi con indici BGM in ordine diverso rispetto agli indici dei documenti (ignorando 'Unknown'). I risultati sono stati salvati in C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\results_for_the_thesis\\risultati_rag+bgm_only_with_different_id_document_order.json.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def trova_indici_in_ordine_diverso(input_file, output_file):\n",
    "    # Carica i dati dal file di input (che è stato generato dalla funzione precedente)\n",
    "    with open(input_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Lista per memorizzare gli esempi che soddisfano la condizione\n",
    "    examples_with_different_order = []\n",
    "    \n",
    "    for example in data:\n",
    "        # Filtra gli indici \"Unknown\" sia da bgm_indices che da document_indices\n",
    "        bgm_indices = [index for index in example['bgm_indices'].split(',') if not index.lower().startswith(\"unknown\")]\n",
    "        document_indices = [str(idx) for idx in example['document_indices']]  # Gli indici nei documenti sono già una lista\n",
    "        \n",
    "        # Verifica se l'ordine degli indici è diverso tra bgm_indices e document_indices\n",
    "        if bgm_indices != document_indices:\n",
    "            examples_with_different_order.append(example)\n",
    "    \n",
    "    # Salva i risultati in un nuovo file JSON\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(examples_with_different_order, f, indent=4)\n",
    "    \n",
    "    return examples_with_different_order\n",
    "\n",
    "# Esempio di utilizzo\n",
    "input_file = r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\results_for_the_thesis\\risultati_rag+bgm_only_with_answer_in_context.json'\n",
    "output_file = r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\results_for_the_thesis\\risultati_rag+bgm_only_with_different_id_document_order.json'  \n",
    "\n",
    "risultati = trova_indici_in_ordine_diverso(input_file, output_file)\n",
    "print(f\"Trovati {len(risultati)} esempi con indici BGM in ordine diverso rispetto agli indici dei documenti (ignorando 'Unknown'). I risultati sono stati salvati in {output_file}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trovati 475 esempi con risposta corretta solo in RAG + BGM. I risultati sono stati salvati in C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\results_for_the_thesis\\risultati_rag+bgm_only_with_NO-RESPONSE_answer_in_context.json.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def trova_risposte_NO_RESPONSE_utili(rag_bgm_file, llm_file, output_file):\n",
    "    # Carica i dati dai due file JSON\n",
    "    with open(rag_bgm_file, 'r') as f:\n",
    "        rag_bgm_data = json.load(f)\n",
    "    \n",
    "    with open(llm_file, 'r') as f:\n",
    "        llm_data = json.load(f)\n",
    "    \n",
    "    # Crea un dizionario per trovare facilmente gli ID esempio nel file LLM\n",
    "    llm_examples = {example['example_id']: example for example in llm_data}\n",
    "    \n",
    "    # Lista per memorizzare gli ID degli esempi che soddisfano tutte le condizioni\n",
    "    results = []\n",
    "    \n",
    "    for example in rag_bgm_data:\n",
    "        example_id = example['example_id']\n",
    "        bgm_indices = example['bgm_indices'].split(',')  # Indici generati dal BGM\n",
    "        document_indices = example['document_indices']  # Indici nei documenti\n",
    "\n",
    "        # Verifica che bgm_indices non contenga documenti che sono in document_indices\n",
    "        if not any(index in document_indices for index in bgm_indices if \"Unknown\" not in index):\n",
    "            # Verifica che i criteri della configurazione RAG + BGM siano soddisfatti\n",
    "            if example['is_correct'] and example['generated_answer'] == \"NO-RESPONSE\" and example['is_no_res']:\n",
    "                # Controlla se esiste lo stesso esempio nel file LLM\n",
    "                if example_id in llm_examples:\n",
    "                    llm_example = llm_examples[example_id]\n",
    "                    # Verifica che la configurazione LLM abbia una risposta diversa da \"NO-RESPONSE\"\n",
    "                    if not llm_example['is_correct'] and llm_example['generated_answer'] != \"NO-RESPONSE\":\n",
    "                        results.append(example)\n",
    "    \n",
    "    # Salva i risultati in un nuovo file JSON\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Esempio di utilizzo\n",
    "rag_bgm_file = r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\gen_res_example_llm_with_bgm\\steps_1800\\nq\\gemma-2-2b-it\\test\\retrieved\\contriever\\5_doc\\numdoc5_retr5_template_info_all_extended.json'\n",
    "llm_file = r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\gen_res_example_llm\\nq\\gemma-2-2b-it\\test\\retrieved\\contriever\\5_doc\\numdoc5_retr5_template_info_all_extended.json' \n",
    "output_file = r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\results_for_the_thesis\\risultati_rag+bgm_only_with_NO-RESPONSE_answer_in_context.json'\n",
    "\n",
    "risultati = trova_risposte_NO_RESPONSE_utili(rag_bgm_file, llm_file, output_file)\n",
    "print(f\"Trovati {len(risultati)} esempi con risposta corretta solo in RAG + BGM. I risultati sono stati salvati in {output_file}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grafico della loss del training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10: Loss = 2.3321\n",
      "Step 20: Loss = 2.2809\n",
      "Step 30: Loss = 2.3231\n",
      "Step 40: Loss = 2.2703\n",
      "Step 50: Loss = 2.2742\n",
      "Step 60: Loss = 2.2158\n",
      "Step 70: Loss = 2.3227\n",
      "Step 80: Loss = 2.2122\n",
      "Step 90: Loss = 2.1946\n",
      "Step 100: Loss = 2.2613\n",
      "Step 110: Loss = 2.2402\n",
      "Step 120: Loss = 2.1603\n",
      "Step 130: Loss = 2.179\n",
      "Step 140: Loss = 2.1962\n",
      "Step 150: Loss = 2.1491\n",
      "Step 160: Loss = 2.2642\n",
      "Step 170: Loss = 2.217\n",
      "Step 180: Loss = 2.2434\n",
      "Step 190: Loss = 2.1269\n",
      "Step 200: Loss = 2.1478\n",
      "Step 210: Loss = 2.1651\n",
      "Step 220: Loss = 2.1845\n",
      "Step 230: Loss = 2.1812\n",
      "Step 240: Loss = 2.1512\n",
      "Step 250: Loss = 2.1937\n",
      "Step 260: Loss = 2.1822\n",
      "Step 270: Loss = 2.1816\n",
      "Step 280: Loss = 2.2095\n",
      "Step 290: Loss = 2.1517\n",
      "Step 300: Loss = 2.0591\n",
      "Step 310: Loss = 2.2018\n",
      "Step 320: Loss = 1.5321\n",
      "Step 330: Loss = 1.4512\n",
      "Step 340: Loss = 1.4981\n",
      "Step 350: Loss = 1.4301\n",
      "Step 360: Loss = 1.4632\n",
      "Step 370: Loss = 1.5216\n",
      "Step 380: Loss = 1.4515\n",
      "Step 390: Loss = 1.5059\n",
      "Step 400: Loss = 1.4267\n",
      "Step 410: Loss = 1.4797\n",
      "Step 420: Loss = 1.4702\n",
      "Step 430: Loss = 1.4521\n",
      "Step 440: Loss = 1.5064\n",
      "Step 450: Loss = 1.4271\n",
      "Step 460: Loss = 1.422\n",
      "Step 470: Loss = 1.5168\n",
      "Step 480: Loss = 1.4127\n",
      "Step 490: Loss = 1.4295\n",
      "Step 500: Loss = 1.4922\n",
      "Step 510: Loss = 1.4324\n",
      "Step 520: Loss = 1.4251\n",
      "Step 530: Loss = 1.4896\n",
      "Step 540: Loss = 1.448\n",
      "Step 550: Loss = 1.4434\n",
      "Step 560: Loss = 1.4995\n",
      "Step 570: Loss = 1.4814\n",
      "Step 580: Loss = 1.4475\n",
      "Step 590: Loss = 1.4862\n",
      "Step 600: Loss = 1.5386\n",
      "Step 610: Loss = 1.522\n",
      "Step 620: Loss = 1.3348\n",
      "Step 630: Loss = 0.8696\n",
      "Step 640: Loss = 0.8857\n",
      "Step 650: Loss = 0.8832\n",
      "Step 660: Loss = 0.8711\n",
      "Step 670: Loss = 0.8605\n",
      "Step 680: Loss = 0.8417\n",
      "Step 690: Loss = 0.8457\n",
      "Step 700: Loss = 0.8188\n",
      "Step 710: Loss = 0.8527\n",
      "Step 720: Loss = 0.8447\n",
      "Step 730: Loss = 0.8998\n",
      "Step 740: Loss = 0.8224\n",
      "Step 750: Loss = 0.8675\n",
      "Step 760: Loss = 0.8811\n",
      "Step 770: Loss = 0.8745\n",
      "Step 780: Loss = 0.8323\n",
      "Step 790: Loss = 0.9258\n",
      "Step 800: Loss = 0.8466\n",
      "Step 810: Loss = 0.8749\n",
      "Step 820: Loss = 0.8686\n",
      "Step 830: Loss = 0.8699\n",
      "Step 840: Loss = 0.915\n",
      "Step 850: Loss = 0.8847\n",
      "Step 860: Loss = 0.8519\n",
      "Step 870: Loss = 0.882\n",
      "Step 880: Loss = 0.8301\n",
      "Step 890: Loss = 0.9012\n",
      "Step 900: Loss = 0.8897\n",
      "Step 910: Loss = 0.8321\n",
      "Step 920: Loss = 0.928\n",
      "Step 930: Loss = 0.6936\n",
      "Step 940: Loss = 0.4504\n",
      "Step 950: Loss = 0.443\n",
      "Step 960: Loss = 0.4272\n",
      "Step 970: Loss = 0.4393\n",
      "Step 980: Loss = 0.3928\n",
      "Step 990: Loss = 0.4184\n",
      "Step 1000: Loss = 0.3776\n",
      "Step 1010: Loss = 0.4259\n",
      "Step 1020: Loss = 0.4697\n",
      "Step 1030: Loss = 0.4417\n",
      "Step 1040: Loss = 0.4076\n",
      "Step 1050: Loss = 0.4421\n",
      "Step 1060: Loss = 0.4251\n",
      "Step 1070: Loss = 0.4295\n",
      "Step 1080: Loss = 0.4166\n",
      "Step 1090: Loss = 0.4174\n",
      "Step 1100: Loss = 0.4561\n",
      "Step 1110: Loss = 0.4173\n",
      "Step 1120: Loss = 0.4367\n",
      "Step 1130: Loss = 0.4184\n",
      "Step 1140: Loss = 0.4389\n",
      "Step 1150: Loss = 0.4271\n",
      "Step 1160: Loss = 0.424\n",
      "Step 1170: Loss = 0.419\n",
      "Step 1180: Loss = 0.4218\n",
      "Step 1190: Loss = 0.4297\n",
      "Step 1200: Loss = 0.446\n",
      "Step 1210: Loss = 0.4771\n",
      "Step 1220: Loss = 0.4212\n",
      "Step 1230: Loss = 0.4422\n",
      "Step 1240: Loss = 0.3591\n",
      "Step 1250: Loss = 0.1913\n",
      "Step 1260: Loss = 0.1955\n",
      "Step 1270: Loss = 0.1968\n",
      "Step 1280: Loss = 0.2048\n",
      "Step 1290: Loss = 0.1909\n",
      "Step 1300: Loss = 0.1954\n",
      "Step 1310: Loss = 0.1957\n",
      "Step 1320: Loss = 0.2017\n",
      "Step 1330: Loss = 0.1888\n",
      "Step 1340: Loss = 0.1777\n",
      "Step 1350: Loss = 0.1837\n",
      "Step 1360: Loss = 0.1816\n",
      "Step 1370: Loss = 0.1891\n",
      "Step 1380: Loss = 0.1917\n",
      "Step 1390: Loss = 0.2011\n",
      "Step 1400: Loss = 0.2051\n",
      "Step 1410: Loss = 0.1867\n",
      "Step 1420: Loss = 0.1909\n",
      "Step 1430: Loss = 0.1949\n",
      "Step 1440: Loss = 0.1859\n",
      "Step 1450: Loss = 0.2013\n",
      "Step 1460: Loss = 0.1899\n",
      "Step 1470: Loss = 0.202\n",
      "Step 1480: Loss = 0.1908\n",
      "Step 1490: Loss = 0.22\n",
      "Step 1500: Loss = 0.189\n",
      "Step 1510: Loss = 0.2\n",
      "Step 1520: Loss = 0.1806\n",
      "Step 1530: Loss = 0.1849\n",
      "Step 1540: Loss = 0.2078\n",
      "Step 1550: Loss = 0.161\n",
      "Step 1560: Loss = 0.0918\n",
      "Step 1570: Loss = 0.0882\n",
      "Step 1580: Loss = 0.0952\n",
      "Step 1590: Loss = 0.0856\n",
      "Step 1600: Loss = 0.0874\n",
      "Step 1610: Loss = 0.0921\n",
      "Step 1620: Loss = 0.095\n",
      "Step 1630: Loss = 0.0912\n",
      "Step 1640: Loss = 0.0836\n",
      "Step 1650: Loss = 0.0913\n",
      "Step 1660: Loss = 0.0916\n",
      "Step 1670: Loss = 0.0924\n",
      "Step 1680: Loss = 0.0877\n",
      "Step 1690: Loss = 0.0979\n",
      "Step 1700: Loss = 0.0995\n",
      "Step 1710: Loss = 0.092\n",
      "Step 1720: Loss = 0.0875\n",
      "Step 1730: Loss = 0.0877\n",
      "Step 1740: Loss = 0.095\n",
      "Step 1750: Loss = 0.0934\n",
      "Step 1760: Loss = 0.0916\n",
      "Step 1770: Loss = 0.0902\n",
      "Step 1780: Loss = 0.0935\n",
      "Step 1790: Loss = 0.0957\n",
      "Step 1800: Loss = 0.0957\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "log_training_file = r'C:\\Users\\franc\\Documents\\Bridge_the_GAP\\data\\SFT_training_bgm\\meta-llama-Llama-3.2-1B\\checkpoint-1800\\trainer_state.json'\n",
    "\n",
    "# Carica il file JSON\n",
    "with open(log_training_file, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Estrai i log\n",
    "log_history = data[\"log_history\"]\n",
    "\n",
    "# Filtra solo le entries con \"loss\"\n",
    "steps = []\n",
    "losses = []\n",
    "\n",
    "for entry in log_history:\n",
    "    if \"loss\" in entry:\n",
    "        steps.append(entry[\"step\"])\n",
    "        losses.append(entry[\"loss\"])\n",
    "\n",
    "# Stampa i valori (opzionale)\n",
    "for s, l in zip(steps, losses):\n",
    "    print(f\"Step {s}: Loss = {l}\")\n",
    "\n",
    "# Plot della curva della loss\n",
    "plt.plot(steps, losses, marker='o', linestyle='-')\n",
    "plt.xlabel(\"Training Step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss Curve\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
